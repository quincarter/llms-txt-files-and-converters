# Supabase JavaScript Reference & Guides
Generated: 2026-02-15



# Auth
Source: https://supabase.com/docs/guides/auth

Auth


### Use Supabase to authenticate and authorize your users.
Use Supabase to authenticate and authorize your users.

Supabase Auth makes it easy to implement authentication and authorization in your app. We provide client SDKs and API endpoints to help you create and manage users.

Your users can use many popular Auth methods, including password, magic link, one-time password (OTP), social login, and single sign-on (SSO).


### About authentication and authorization#
Authentication and authorization are the core responsibilities of any Auth system.

- Authenticationmeans checking that a user is who they say they are.
- Authorizationmeans checking what resources a user is allowed to access.

Supabase Auth usesJSON Web Tokens (JWTs)for authentication. For a complete reference of all JWT fields, see theJWT Fields Reference. Auth integrates with Supabase's database features, making it easy to useRow Level Security (RLS)for authorization.


### The Supabase ecosystem#
You can use Supabase Auth as a standalone product, but it's also built to integrate with the Supabase ecosystem.

Auth uses your project's Postgres database under the hood, storing user data and other Auth information in a special schema. You can connect this data to your own tables using triggers and foreign key references.

Auth also enables access control to your database's automatically generatedREST API. When using Supabase SDKs, your data requests are automatically sent with the user's Auth Token. The Auth Token scopes database access on a row-by-row level when used along withRLS policies.


### Providers#
Supabase Auth works with many popular Auth methods, including Social and Phone Auth using third-party providers. See the following sections for a list of supported third-party providers.


### Social Auth#

### Phone Auth#

### Pricing#
Charges apply to Monthly Active Users (MAU), Monthly Active Third-Party Users (Third-Party MAU), and Monthly Active SSO Users (SSO MAU) and Advanced MFA Add-ons. For a detailed breakdown of how these charges are calculated, refer to the following pages:

- Pricing MAU
- Pricing Third-Party MAU
- Pricing SSO MAU
- Advanced MFA - Phone


================================================================================


# Auth architecture
Source: https://supabase.com/docs/guides/auth/architecture

Auth architecture


### The architecture behind Supabase Auth.
The architecture behind Supabase Auth.

There are four major layers to Supabase Auth:


### Client layer#
The client layer runs in your app. This could be running in many places, including:

- Your frontend browser code
- Your backend server code
- Your native application

The client layer provides the functions that you use to sign in and manage users. We recommend using the Supabase client SDKs, which handle:

- Configuration and authentication of HTTP calls to the Supabase Auth backend
- Persistence, refresh, and removal of Auth Tokens in your app's storage medium
- Integration with other Supabase products

But at its core, this layer manages the making of HTTP calls, so you could write your own client layer if you wanted to.

See the Client SDKs for more information:

- JavaScript
- Flutter
- Swift
- Python
- C#
- Kotlin


### Auth service#
TheAuth serviceis an Auth API server written and maintained by Supabase. It is a fork of the GoTrue project, originally created by Netlify.

When you deploy a new Supabase project, we deploy an instance of this server alongside your database, and inject your database with the required Auth schema.

The Auth service is responsible for:

- Validating, issuing, and refreshing JWTs
- Serving as the intermediary between your app and Auth information in the database
- Communicating with external providers for Social Login and SSO


### Postgres#
Supabase Auth uses theauthschema in your Postgres database to store user tables and other information. For security, this schema is not exposed on the auto-generated API.

You can connect Auth information to your own objects usingdatabase triggersandforeign keys. Make sure that any views you create for Auth data are adequately protected byenabling RLSorrevoking grants.

Make sure any views you create for Auth data are protected.

Starting in Postgres version 15, views inherit the RLS policies of the underlying tables if created withsecurity_invoker. Views in earlier versions, or those created withoutsecurity_invoker, inherit the permissions of the owner, who can bypass RLS policies.


================================================================================


# Auth Audit Logs
Source: https://supabase.com/docs/guides/auth/audit-logs

Auth Audit Logs


### Monitor and track authentication events with audit logging.
Monitor and track authentication events with audit logging.

Auth audit logs provide comprehensive tracking of authentication events in your Supabase project. Audit logs are automatically captured for all authentication events and help you monitor user authentication activities, detect suspicious behavior, and maintain compliance with security requirements.


### What gets logged#
Supabase auth audit logs automatically capture all authentication events including:

- User signups and logins
- Password changes and resets
- Email verification events
- Token refresh and logout events


### Storage options#
By default, audit logs are stored in two places:

You can disable Postgres storage to reduce database storage costs while keeping the external log storage.


### Configuring audit log storage#
Disabling Postgres storage reduces your database storage costs. Audit logs will still be available through the dashboard.


### Log format#
Audit logs contain detailed information about each authentication event:

```javascript
1{2  "timestamp": "2025-08-01T10:30:00Z",3  "user_id": "uuid",4  "action": "user_signedup",5  "ip_address": "192.168.1.1",6  "user_agent": "Mozilla/5.0...",7  "metadata": {8    "provider": "email"9  }10}
```


### Log actions reference#

### Limitations#
- There may be a short delay before logs appear
- Query capabilities are limited to the dashboard interface


================================================================================


# Anonymous Sign-Ins
Source: https://supabase.com/docs/guides/auth/auth-anonymous

Anonymous Sign-Ins


### Create and use anonymous users to authenticate with Supabase
Create and use anonymous users to authenticate with Supabase

Enable Anonymous Sign-Insto build apps which provide users an authenticated experience without requiring users to enter an email address, password, use an OAuth provider or provide any other PII (Personally Identifiable Information). Later, when ready, the user can link an authentication method to their account.

CallingsignInAnonymously()creates an anonymous user. It's just like a permanent user, except the user can't access their account if they sign out, clear browsing data, or use another device.

Like permanent users, theauthenticatedPostgres role will be used when using the Data APIs to access your project. JWTs for these users will have anis_anonymousclaim which you can use to distinguish in RLS policies.

This is different from theanonAPI key which does not create a user and can be used to implement public access to your database as it uses theanonymousPostgres role.

Anonymous sign-ins can be used to build:

- E-commerce applications, such as shopping carts before check-out
- Full-feature demos without collecting personal information
- Temporary or throw-away accounts

Review your existing RLS policies before enabling anonymous sign-ins. Anonymous users use theauthenticatedrole. To distinguish between anonymous users and permanent users, your policies need to check theis_anonymousfield of the user's JWT.

See theAccess control sectionfor more details.

The Supabase team has received reports of user metadata being cached across unique anonymous users as a result of Next.js static page rendering. For the best user experience, utilize dynamic page rendering.

For self-hosting, you can update your project configuration using the files and environment variables provided. See thelocal development docsfor more details.


### Sign in anonymously#
Call thesignInAnonymously()method:

```javascript
1const { ,  } = await ..()
```


### Convert an anonymous user to a permanent user#
Converting an anonymous user to a permanent user requireslinking an identityto the user. This requires you toenable manual linkingin your Supabase project.


### Link an email / phone identity#
You can use theupdateUser()method to link an email or phone identity to the anonymous user. To add a password for the anonymous user, the user's email or phone number needs to be verified first.

```javascript
1const { : , :  } = await ..({2  : 'valid.email@supabase.io',3})45// verify the user's email by clicking on the email change link6// or entering the 6-digit OTP sent to the email address78// once the user has been verified, update the password9const { : , :  } = await ..({10  : 'password',11})
```


### Link an OAuth identity#
You can use thelinkIdentity()method to link an OAuth identity to the anonymous user.

```javascript
1const { ,  } = await ..({ : 'google' })
```


### Access control#
An anonymous user assumes theauthenticatedrole just like a permanent user. You can use row-level security (RLS) policies to differentiate between an anonymous user and a permanent user by checking for theis_anonymousclaim in the JWT returned byauth.jwt():

```javascript
1create policy "Only permanent users can post to the news feed"2on news_feed as restrictive for insert3to authenticated4with check ((select (auth.jwt()->>'is_anonymous')::boolean) is false );56create policy "Anonymous and permanent users can view the news feed"7on news_feed for select8to authenticated9using ( true );
```

RLS policies are permissive by default, which means that they are combined using an "OR" operator when multiple policies are applied. It is important to construct restrictive policies to ensure that the checks for an anonymous user are always enforced when combined with other policies.
Be aware that a single 'restrictive' RLS policy alone will fail unless combined with another policy that returns true, ensuring the combined condition is met.


### Resolving identity conflicts#
Depending on your application requirements, data conflicts can arise when an anonymous user is converted to a permanent user. For example, in the context of an e-commerce application, an anonymous user would be allowed to add items to the shopping cart without signing up / signing in. When they decide to sign-in to an existing account, you will need to decide how you want to resolve data conflicts in the shopping cart:


### Linking an anonymous user to an existing account#
In some cases, you may need to link an anonymous user to an existing account rather than creating a new permanent account. This process requires manual handling of potential conflicts. Here's a general approach:

```javascript
1// 1. Sign in anonymously (assuming the user is already signed in anonymously)2const { data: anonData, error: anonError } = await supabase.auth.getSession()34// 2. Attempt to update the user with the existing email5const { data: updateData, error: updateError } = await supabase.auth.updateUser({6  email: 'valid.email@supabase.io',7})89// 3. Handle the error (since the email belongs to an existing user)10if (updateError) {11  console.log('This email belongs to an existing user. Please sign in to that account.')1213  // 4. Sign in to the existing account14  const {15    data: { user: existingUser },16    error: signInError,17  } = await supabase.auth.signInWithPassword({18    email: 'valid.email@supabase.io',19    password: 'user_password',20  })2122  if (existingUser) {23    // 5. Reassign entities tied to the anonymous user24    // This step will vary based on your specific use case and data model25    const { data: reassignData, error: reassignError } = await supabase26      .from('your_table')27      .update({ user_id: existingUser.id })28      .eq('user_id', anonData.session.user.id)2930    // 6. Implement your chosen conflict resolution strategy31    // This could involve merging data, overwriting, or other custom logic32    await resolveDataConflicts(anonData.session.user.id, existingUser.id)33  }34}3536// Helper function to resolve data conflicts (implement based on your strategy)37async function resolveDataConflicts(anonymousUserId, existingUserId) {38  // Implement your conflict resolution logic here39  // This could involve ignoring the anonymous user's metadata, overwriting the existing user's metadata, or merging the data of both the anonymous and existing user.40}
```


### Abuse prevention and rate limits#
Since anonymous users are stored in your database, bad actors can abuse the endpoint to increase your database size drastically. It is strongly recommended toenable invisible CAPTCHA or Cloudflare Turnstileto prevent abuse for anonymous sign-ins. An IP-based rate limit is enforced at 30 requests per hour which can be modified in yourdashboard. You can refer to the full list of rate limitshere.


### Automatic cleanup#
Automatic cleanup of anonymous users is currently not available. Instead, you can delete anonymous users from your project by running the following SQL:

```javascript
1-- deletes anonymous users created more than 30 days ago2delete from auth.users3where is_anonymous is true and created_at < now() - interval '30 days';
```


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- Supabase Flutter Client
- Supabase Kotlin Client


================================================================================


# Enable CAPTCHA Protection
Source: https://supabase.com/docs/guides/auth/auth-captcha

Enable CAPTCHA Protection

Supabase provides you with the option of adding CAPTCHA to your sign-in, sign-up, and password reset forms. This keeps your website safe from bots and malicious scripts. Supabase authentication has support forhCaptchaandCloudflare Turnstile.


### Sign up for CAPTCHA#
Go to thehCaptchawebsite and sign up for an account. On the Welcome page, copy theSitekeyandSecret key.

If you have already signed up and didn't copy this information from the Welcome page, you can get theSecret keyfrom the Settings page.



TheSitekeycan be found in theSettingsof the active site you created.



In the Settings page, look for theSitekeysection and copy the key.




### Enable CAPTCHA protection for your Supabase project#
Navigate to theAuthsection of your Project Settings in the Supabase Dashboard and find theEnable CAPTCHA protectiontoggle under Settings > Authentication > Bot and Abuse Protection > Enable CAPTCHA protection.

Select your CAPTCHA provider from the dropdown, enter your CAPTCHASecret key, and clickSave.


### Add the CAPTCHA frontend component#
The frontend requires some changes to provide the CAPTCHA on-screen for the user. This example uses React and the corresponding CAPTCHA React component, but both CAPTCHA providers can be used with any JavaScript framework.

Install@hcaptcha/react-hcaptchain your project as a dependency.

```javascript
1npm install @hcaptcha/react-hcaptcha
```

Now import theHCaptchacomponent from the@hcaptcha/react-hcaptchalibrary.

```javascript
1import HCaptcha from '@hcaptcha/react-hcaptcha'
```

Let's create a empty state to store ourcaptchaToken

```javascript
1const [captchaToken, setCaptchaToken] = useState()
```

Now lets add theHCaptchacomponent to the JSX section of our code

```javascript
1<HCaptcha />
```

We will pass it the sitekey we copied from the hCaptcha website as a property along with aonVerifyproperty which takes a callback function. This callback function will have a token as one of its properties. Let's set the token in the state usingsetCaptchaToken

```javascript
1<HCaptcha2  sitekey="your-sitekey"3  onVerify={(token) => {4    setCaptchaToken(token)5  }}6/>
```

Now lets use the CAPTCHA token we receive in our Supabase signUp function.

```javascript
1await supabase.auth.signUp({2  email,3  password,4  options: { captchaToken },5})
```

We will also need to reset the CAPTCHA challenge after we have made a call to the function above.

Create a ref to use on ourHCaptchacomponent.

```javascript
1const captcha = useRef()
```

Let's add a ref attribute on theHCaptchacomponent and assign thecaptchaconstant to it.

```javascript
1<HCaptcha2  ref={captcha}3  sitekey="your-sitekey"4  onVerify={(token) => {5    setCaptchaToken(token)6  }}7/>
```

Reset thecaptchaafter the signUp function is called using the following code:

```javascript
1captcha.current.resetCaptcha()
```

In order to test that this works locally we will need to use something likengrokor add an entry to your hosts file. You can read more about this in thehCaptcha docs.

Run the application and you should now be provided with a CAPTCHA challenge.


================================================================================


# Passwordless email logins
Source: https://supabase.com/docs/guides/auth/auth-email-passwordless

Passwordless email logins


### Email logins using Magic Links or One-Time Passwords (OTPs)
Email logins using Magic Links or One-Time Passwords (OTPs)

Supabase Auth provides several passwordless login methods. Passwordless logins allow users to sign in without a password, by clicking a confirmation link or entering a verification code.

Passwordless login can:

- Improve the user experience by not requiring users to create and remember a password
- Increase security by reducing the risk of password-related security breaches
- Reduce support burden of dealing with password resets and other password-related flows

Supabase Auth offers two passwordless login methods that use the user's email address:

- Magic Link
- OTP


### With Magic Link#
Magic Links are a form of passwordless login where users click on a link sent to their email address to log in to their accounts. Magic Links only work with email addresses and are one-time use only.


### Enabling Magic Link#
Email authentication methods, including Magic Links, are enabled by default.

Configure the Site URL and any additional redirect URLs. These are the only URLs that are allowed as redirect destinations after the user clicks a Magic Link. You can change the URLs on theURL Configuration pagefor hosted projects, or in theconfiguration filefor self-hosted projects.

By default, a user can only request a magic link once every60 secondsand they expire after1 hour.


### Signing in with Magic Link#
Call the "sign in with OTP" method from the client library.

Though the method is labelled "OTP", it sends a Magic Link by default. The two methods differ only in the content of the confirmation email sent to the user.

If the user hasn't signed up yet, they are automatically signed up by default. To prevent this, set theshouldCreateUseroption tofalse.

```javascript
1async function () {2  const { ,  } = await ..({3    : 'valid.email@supabase.io',4    : {5      // set this to false if you do not want the user to be automatically signed up6      : false,7      : 'https://example.com/welcome',8    },9  })10}
```

That's it for the implicit flow.

If you're using PKCE flow, edit the Magic Linkemail templateto send a token hash:

```javascript
1<h2>Magic Link</h2>23<p>Follow this link to login:</p>4<p><a href="{{ .SiteURL }}/auth/confirm?token_hash={{ .TokenHash }}&type=email">Log In</a></p>
```

At the/auth/confirmendpoint, exchange the hash for the session:

```javascript
1const {  } = await ..({2  : 'hash',3  : 'email',4})
```


### With OTP#
Email one-time passwords (OTP) are a form of passwordless login where users key in a six digit code sent to their email address to log in to their accounts.


### Enabling email OTP#
Email authentication methods, including Email OTPs, are enabled by default.

Email OTPs share an implementation with Magic Links. To send an OTP instead of a Magic Link, alter theMagic Linkemail template. For a hosted Supabase project, go toEmail Templatesin the Dashboard. For a self-hosted project or local development, see theEmail Templates guide.

Modify the template to include the{{ .Token }}variable, for example:

```javascript
1<h2>One time login code</h2>23<p>Please enter this code: {{ .Token }}</p>
```

By default, a user can only request an OTP once every60 secondsand they expire after1 hour. This is configurable viaAuth > Providers > Email > Email OTP Expiration. An expiry duration of more than 86400 seconds (one day) is disallowed to guard against brute force attacks. The longer an OTP remains valid, the more time an attacker has to attempt brute force attacks. If the OTP is valid for several days, an attacker might have more opportunities to guess the correct OTP through repeated attempts.


### Signing in with email OTP#

### Step 1: Send the user an OTP code#
Get the user's email and call the "sign in with OTP" method from your client library.

If the user hasn't signed up yet, they are automatically signed up by default. To prevent this, set theshouldCreateUseroption tofalse.

```javascript
1const { ,  } = await ..({2  : 'valid.email@supabase.io',3  : {4    // set this to false if you do not want the user to be automatically signed up5    : false,6  },7})
```

If the request is successful, you receive a response witherror: nulland adataobject where bothuserandsessionare null. Let the user know to check their email inbox.

```javascript
1{2  "data": {3    "user": null,4    "session": null5  },6  "error": null7}
```


### Step 2: Verify the OTP to create a session#
Provide an input field for the user to enter their one-time code.

Call the "verify OTP" method from your client library with the user's email address, the code, and a type ofemail:

```javascript
1const {2  : {  },3  ,4} = await ..({5  : 'email@example.com',6  : '123456',7  : 'email',8})
```

If successful, the user is now logged in, and you receive a valid session that looks like:

```javascript
1{2  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJhdXRoZW50aWNhdGVkIiwiZXhwIjoxNjI3MjkxNTc3LCJzdWIiOiJmYTA2NTQ1Zi1kYmI1LTQxY2EtYjk1NC1kOGUyOTg4YzcxOTEiLCJlbWFpbCI6IiIsInBob25lIjoiNjU4NzUyMjAyOSIsImFwcF9tZXRhZGF0YSI6eyJwcm92aWRlciI6InBob25lIn0sInVzZXJfbWV0YWRhdGEiOnt9LCJyb2xlIjoiYXV0aGVudGljYXRlZCJ9.1BqRi0NbS_yr1f6hnr4q3s1ylMR3c1vkiJ4e_N55dhM",3  "token_type": "bearer",4  "expires_in": 3600,5  "refresh_token": "LSp8LglPPvf0DxGMSj-vaQ",6  "user": {...}7}
```


================================================================================


# Email Templates
Source: https://supabase.com/docs/guides/auth/auth-email-templates

Email Templates


### Learn how to manage the email templates in Supabase.
Learn how to manage the email templates in Supabase.

Email templates in Supabase fall into two categories: authentication and security notifications.

Authentication emails:

- Confirm sign up
- Invite user
- Magic link
- Change email address
- Reset password
- Reauthentication

Security notification emails:

- Password changed
- Email address changed
- Phone number changed
- Identity linked
- Identity unlinked
- Multi-factor authentication (MFA) method added
- Multi-factor authentication (MFA) method removed

Security emails are only sent to users if the respective security notifications have been enabled at a project-level.


### Terminology#
The templating system provides the following variables for use:


### Editing email templates#
On hosted Supabase projects, edit your email templates on theEmail Templatespage. On self-hosted projects or in local development, edit yourconfiguration files.

You can also manage email templates using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Get current email templates6curl -X GET "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  | jq 'to_entries | map(select(.key | startswith("mailer_templates"))) | from_entries'910# Update email templates11curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \12  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \13  -H "Content-Type: application/json" \14  -d '{15      "mailer_subjects_confirmation": "Confirm your signup",16      "mailer_templates_confirmation_content": "<h2>Confirm your signup</h2><p>Follow this link to confirm your user:</p><p><a href=\"{{ .ConfirmationURL }}\">Confirm your email</a></p>",17      "mailer_subjects_magic_link": "Your Magic Link",18      "mailer_templates_magic_link_content": "<h2>Magic Link</h2><p>Follow this link to login:</p><p><a href=\"{{ .ConfirmationURL }}\">Log In</a></p>",19      "mailer_subjects_recovery": "Rest Your Password",20      "mailer_templates_recovery_content": "<h2>Reset Password</h2><p>Follow this link to reset the password for your user:</p><p><a href=\"{{ .ConfirmationURL }}\">Reset Password</a></p>",21      "mailer_subjects_invite": "You have been invited",22      "mailer_templates_invite_content": "<h2>You have been invited</h2><p>You have been invited to create a user on {{ .SiteURL }}. Follow this link to accept the invite:</p><p><a href=\"{{ .ConfirmationURL }}\">Accept the invite</a></p>",23      "mailer_subjects_reauthentication": "Confirm reauthentication",24      "mailer_templates_reauthentication_content": "<h2>Confirm reauthentication</h2><p>Enter the code: {{token}}</p>",25      "mailer_subjects_email_change": "Confirm email change",26      "mailer_templates_email_change_content": "<h2>Confirm email change</h2><p>Follow this link to confirm the update of your email:</p><p><a href=\"{{ .ConfirmationURL }}\">Change email</a></p>",27      "mailer_notifications_password_changed_enabled": true,28      "mailer_subjects_password_changed_notification": "Your password has been changed",29      "mailer_templates_password_changed_notification_content": "<h2>Your password has been changed</h2>\n\n<p>This is a confirmation that the password for your account {{ .Email }} has just been changed.</p>\n<p>If you did not make this change, please contact support.</p>",30      "mailer_notifications_email_changed_enabled": true,31      "mailer_subjects_email_changed_notification": "Your email address has been changed",32      "mailer_templates_email_changed_notification_content": "<h2>Your email address has been changed</h2>\n\n<p>The email address for your account has been changed from {{ .OldEmail }} to {{ .Email }}.</p>\n<p>If you did not make this change, please contact support.</p>",33      "mailer_notifications_phone_changed_enabled": true,34      "mailer_subjects_phone_changed_notification": "Your phone number has been changed",35      "mailer_templates_phone_changed_notification_content": "<h2>Your phone number has been changed</h2>\n\n<p>The phone number for your account {{ .Email }} has been changed from {{ .OldPhone }} to {{ .Phone }}.</p>\n<p>If you did not make this change, please contact support immediately.</p>",36      "mailer_notifications_mfa_factor_enrolled_enabled": true,37      "mailer_subjects_mfa_factor_enrolled_notification": "A new MFA factor has been enrolled",38      "mailer_templates_mfa_factor_enrolled_notification_content": "<h2>A new MFA factor has been enrolled</h2>\n\n<p>A new factor ({{ .FactorType }}) has been enrolled for your account {{ .Email }}.</p>\n<p>If you did not make this change, please contact support immediately.</p>",39      "mailer_notifications_mfa_factor_unenrolled_enabled": true,40      "mailer_subjects_mfa_factor_unenrolled_notification": "An MFA factor has been unenrolled",41      "mailer_templates_mfa_factor_unenrolled_notification_content": "<h2>An MFA factor has been unenrolled</h2>\n\n<p>A factor ({{ .FactorType }}) has been unenrolled for your account {{ .Email }}.</p>\n<p>If you did not make this change, please contact support immediately.</p>",42      "mailer_notifications_identity_linked_enabled": true,43      "mailer_subjects_identity_linked_notification": "A new identity has been linked",44      "mailer_templates_identity_linked_notification_content": "<h2>A new identity has been linked</h2>\n\n<p>A new identity ({{ .Provider }}) has been linked to your account {{ .Email }}.</p>\n<p>If you did not make this change, please contact support immediately.</p>",45      "mailer_notifications_identity_unlinked_enabled": true,46      "mailer_subjects_identity_unlinked_notification": "An identity has been unlinked",47      "mailer_templates_identity_unlinked_notification_content": "<h2>An identity has been unlinked</h2>\n\n<p>An identity ({{ .Provider }}) has been unlinked from your account {{ .Email }}.</p>\n<p>If you did not make this change, please contact support immediately.</p>"48  }'
```


### Mobile deep linking#
For mobile applications, you might need to link or redirect to a specific page within your app. See theMobile Deep Linking guideto set this up.


### Limitations#

### Email prefetching#
Certain email providers may have spam detection or other security features that prefetch URL links from incoming emails (e.g.Safe Links in Microsoft Defender for Office 365).
In this scenario, the{{ .ConfirmationURL }}sent will be consumed instantly which leads to a "Token has expired or is invalid" error.
To guard against this there are the options below:

Option 1

- Use an email OTP instead by including{{ .Token }}in the email template
- Create your own custom email link to redirect the user to a page where they can enter with their email and token to login

```javascript
1<a href="{{ .SiteURL }}/confirm-signup">Confirm your signup</a>
```

- Log them in by verifying the OTP token value with their email e.g. withsupabase.auth.verifyOtpshow below

```javascript
1const { data, error } = await supabase.auth.verifyOtp({ email, token, type: 'email' })
```

Option 2

- Create your own custom email link to redirect the user to a page where they can click on a button to confirm the action

```javascript
1<a href="{{ .SiteURL }}/confirm-signup?confirmation_url={{ .ConfirmationURL }}">2  Confirm your signup3</a>
```

- The button should contain the actual confirmation link which can be obtained from parsing theconfirmation_url={{ .ConfirmationURL }}query parameter in the URL.


### Email tracking#
If you are using an external email provider that enables "email tracking", the links inside the Supabase email templates will be overwritten and won't perform as expected. We recommend disabling email tracking to ensure email links are not overwritten.


### Redirecting the user to a server-side endpoint#
If you intend to useServer-side rendering, you might want the email link to redirect the user to a server-side endpoint to check if they are authenticated before returning the page. However, the default email link will redirect the user after verification to the redirect URL with the session in the query fragments. Since the session is returned in the query fragments by default, you won't be able to access it on the server-side.

You can customize the email link in the email template to redirect the user to a server-side endpoint successfully. For example:

```javascript
1<a2  href="https://api.example.com/v1/authenticate?token_hash={{ .TokenHash }}&type=invite&redirect_to={{ .RedirectTo }}"3>4  Accept the invite5</a>
```

When the user clicks on the link, the request will hithttps://api.example.com/v1/authenticateand you can grab thetoken_hash,typeandredirect_toquery parameters from the URL. Then, you can call theverifyOtpmethod to get back an authenticated session before redirecting the user back to the client. Since theverifyOtpmethod makes aPOSTrequest to Supabase Auth to verify the user, the session will be returned in the response body, which can be read by the server. For example:

```javascript
1const { ,  } = .(new (..))2const {3  : {  },4  ,5} = await ..({ , :  as  })67// subsequently redirect the user back to the client using the redirect_to param8// ...
```


### Customization#
Supabase Auth makes use ofGo Templates. This means it is possible to conditionally render information based on template properties.


### Send different email to early access users#
Send a different email to users who signed up via an early access domain (https://www.earlyaccess.trial.com).

```javascript
1{{ if eq .Data.Domain "https://www.example.com" }}2<h1>Welcome to Our Database Service!</h1>3  <p>Dear Developer,</p>4  <p>Welcome to Billy, the scalable developer platform!</p>5  <p>Best Regards,<br>6Billy Team</p>7{{ else if eq .Data.Domain "https://www.earlyaccess.trial.com" }}8<h1>Welcome to Our Database Service!</h1>9  <p>Dear Developer,</p>10  <p>Welcome Billy, the scalable developer platform!</p>11  <p> As an early access member, you have access to select features like Point To Space Restoration.</p>12  <p>Best Regards,<br>13Billy Team</p>14{{ end }}
```


================================================================================


# Auth Hooks
Source: https://supabase.com/docs/guides/auth/auth-hooks

Auth Hooks


### Use HTTP or Postgres Functions to customize your authentication flow
Use HTTP or Postgres Functions to customize your authentication flow


### What is a hook#
A hook is an endpoint that allows you to alter the default Supabase Auth flow at specific execution points. Developers can use hooks to add custom behavior that's not supported natively.

Hooks help you:

- Track the origin of user signups by adding metadata
- Improve security by adding additional checks to password and multi-factor authentication
- Support legacy systems by integrating with identity credentials from external authentication systems
- Add additional custom claims to your JWT
- Send authentication emails or SMS messages through a custom provider

The following hooks are available:

Supabase supports 2 ways toconfigure a hookin your project:

APostgres functioncan be configured as a hook. The function should take in a single argument -- the event of type JSONB -- and return a JSONB object. Since the Postgres function runs on your database, the request does not leave your project's instance.


### Security model#
Sign the payload and grant permissions selectively in order to guard the integrity of the payload.

When you configure a Postgres function as a hook, Supabase will automatically apply the following grants to the function for these reasons:

- Allow thesupabase_auth_adminrole to execute the function. Thesupabase_auth_adminrole is the Postgres role that is used by Supabase Auth to make requests to your database.
- Revoke permissions from other roles (e.g.anon,authenticated,public) to ensure the function is not accessible by Supabase Data APIs.

```javascript
1-- Grant access to function to supabase_auth_admin2grant execute3  on function public.custom_access_token_hook4  to supabase_auth_admin;56-- Grant access to schema to supabase_auth_admin7grant usage on schema public to supabase_auth_admin;89-- Revoke function permissions from authenticated, anon and public10revoke execute11  on function public.custom_access_token_hook12  from authenticated, anon, public;
```

You will need to alter your row-level security (RLS) policies to allow thesupabase_auth_adminrole to access tables that you have RLS policies on. You can read more about RLS policieshere.

Alternatively, you can create your Postgres function via the dashboard with thesecurity definertag. Thesecurity definertag specifies that the function is to be executed with the privileges of the user that owns it.

Currently, functions created via the dashboard take on thepostgresrole. Read more about thesecurity definertagin our database guide


### Using Hooks#

### Developing#
Let us develop a Hook locally and then deploy it to the cloud. As a recap, here’s a list of available Hooks

Editconfig.tomlto set up the Auth Hook locally.

Modify theauth.hook.<hook_name>field and seturito a value ofpg-functions://postgres/<schema>/<function_name>

```javascript
1[auth.hook.<hook_name>]2enabled = true3uri = "pg-functions://...."
```

You need to assign additional permissions so that Supabase Auth can access the hook as well as the tables it interacts with.

Thesupabase_auth_adminrole does not have permissions to thepublicschema. You need to grant the role permission to execute your hook:

```javascript
1grant execute2  on function public.custom_access_token_hook3  to supabase_auth_admin;
```

You also need to grant usage tosupabase_auth_admin:

```javascript
1grant usage on schema public to supabase_auth_admin;
```

Also revoke permissions from theauthenticatedandanonroles to ensure the function is not accessible by Supabase Serverless APIs.

```javascript
1revoke execute2  on function public.custom_access_token_hook3  from authenticated, anon;
```

For security, we recommend against the use thesecurity definertag. Thesecurity definertag specifies that the function is to be executed with the privileges of the user that owns it. When a function is created via the Supabase dashboard with the tag, it will have the extensive permissions of thepostgresrole which make it easier for undesirable actions to occur.

We recommend that you do not use any tag and explicitly grant permissions tosupabase_auth_adminas described above.

Read more aboutsecurity definertagin our database guide.

Once done, save your Auth Hook as a migration in order to version the Auth Hook and share it with other team members. Runsupabase migration newto create a migration.

If you're using the Supabase SQL Editor, there's an issue when using the?(Does the string exist as a top-level key within the JSON value?) operator. Use a direct connection to the database if you need to use it when defining a function.

Here is an example hook signature:

```javascript
1create or replace function public.custom_access_token_hook(event jsonb)2returns jsonb3language plpgsql4as $$5declare6  -- Insert variables here7begin8  -- Insert logic here9  return event;10end;11$$;
```

You can visitSQL Editor > Templatesfor hook templates.


### Deploying#
In the dashboard, navigate toAuthentication > Hooksand select the appropriate function type (SQL or HTTP) from the dropdown menu.


### Error handling#
You should return an error when facing a runtime error. Runtime errors are specific to your application and arise from specific business rules rather than programmer errors.

Runtime errors could happen when:

- The user does not have appropriate permissions
- The event payload received does not have required claims.
- The user has performed an action which violates a business rule.
- The email or phone provider used in the webhook returned an error.

The error is a JSON object and has the following properties:

- errorAn object that contains information about the error.http_codeA number indicating the HTTP code to be returned. If not set, the code is HTTP 500 Internal Server Error.messageA message to be returned in the HTTP response. Required.

- http_codeA number indicating the HTTP code to be returned. If not set, the code is HTTP 500 Internal Server Error.
- messageA message to be returned in the HTTP response. Required.

Here's an example:

```javascript
1{2  "error": {3    "http_code": 429,4    "message": "You can only verify a factor once every 10 seconds."5  }6}
```

Errors returned from a Postgres Hook are not retry-able. When an error is returned, the error is propagated from the hook to Supabase Auth and translated into a HTTP error which is returned to your application. Supabase Auth will only take into account the error and disregard the rest of the payload.

Outside of runtime errors, both HTTP Hooks and Postgres Hooks return timeout errors. Postgres Hooks have2seconds to complete processing while HTTP Hooks should complete in5seconds. Both HTTP Hooks and Postgres Hooks are run in a transaction do limit the duration of execution to avoid delays in authentication process.


### Available Hooks#
Each Hook description contains an example JSON Schema which you can use in conjunction withJSON Schema Fakerin order to generate a mock payload. For HTTP Hooks, you can also usethe Standard Webhooks Testing Toolto simulate a request.

Custom Access Token

Send SMS

Send Email

MFA Verification

Password verification


================================================================================


# Before User Created Hook
Source: https://supabase.com/docs/guides/auth/auth-hooks/before-user-created-hook

Before User Created Hook


### Prevent unwanted signups by inspecting and rejecting user creation requests
Prevent unwanted signups by inspecting and rejecting user creation requests

This hook runs before a new user is created. It allows developers to inspect the incoming user object and optionally reject the request. Use this to enforce custom signup policies that Supabase Auth does not handle natively - such as blocking disposable email domains, restricting access by region or IP, or requiring that users belong to a specific email domain.

You can implement this hook using an HTTP endpoint or a Postgres function. If the hook returns an error object, the signup is denied and the user is not created. If the hook responds successfully (HTTP 200 or 204 with no error), the request proceeds as usual. This gives you full control over which users are allowed to register — and the flexibility to apply that logic server-side.


### Inputs#
Supabase Auth will send a payload containing these fields to your hook:

Because the hook is ran just before the insertion into the database, this user will not be found in Postgres at the time the hook is called.

```javascript
1{2  "metadata": {3    "uuid": "8b34dcdd-9df1-4c10-850a-b3277c653040",4    "time": "2025-04-29T13:13:24.755552-07:00",5    "name": "before-user-created",6    "ip_address": "127.0.0.1"7  },8  "user": {9    "id": "ff7fc9ae-3b1b-4642-9241-64adb9848a03",10    "aud": "authenticated",11    "role": "",12    "email": "valid.email@supabase.com",13    "phone": "",14    "app_metadata": {15      "provider": "email",16      "providers": ["email"]17    },18    "user_metadata": {},19    "identities": [],20    "created_at": "0001-01-01T00:00:00Z",21    "updated_at": "0001-01-01T00:00:00Z",22    "is_anonymous": false23  }24}
```


### Outputs#
Your hook must return a response that either allows or blocks the signup request.

Returning an empty object with a200or204status code allows the request to proceed. Returning a JSON response with anerrorobject and a4xxstatus code blocks the request and propagates the error message to the client. See theerror handling documentationfor more details.


### Allow the signup#
```javascript
1{}
```

or with a204 No Contentresponse:

```javascript
1HTTP/1.1 204 No Content
```


### Reject the signup with an error#
```javascript
1{2  "error": {3    "http_code": 400,4    "message": "Only company emails are allowed to sign up."5  }6}
```

This response will block the user creation and return the error message to the client that attempted signup.


### Examples#
Each of the following examples shows how to use thebefore-user-createdhook to control signup behavior. Each use case includes both a HTTP implementation (e.g. using an Edge Function) and a SQL implementation (Postgres function).

Allow signups only from specific domains like supabase.com or example.test. Reject all others. This is useful for private/internal apps, enterprise gating, or invite-only beta access.

Thebefore-user-createdhook solves this by:

- Detecting that a user is about to be created
- Providing the email address in theuser.emailfield

Run the following snippet in your project'sSQL Editor. This will create asignup_email_domainstable with some sample data and ahook_restrict_signup_by_email_domainfunction to be called by thebefore-user-createdauth hook.

```javascript
1-- Create ENUM type for domain rule classification2do $$ begin3  create type signup_email_domain_type as enum ('allow', 'deny');4exception5  when duplicate_object then null;6end $$;78-- Create the signup_email_domains table9create table if not exists public.signup_email_domains (10  id serial primary key,11  domain text not null,12  type signup_email_domain_type not null,13  reason text default null,14  created_at timestamptz not null default now(),15  updated_at timestamptz not null default now()16);1718-- Create a trigger to maintain updated_at19create or replace function update_signup_email_domains_updated_at()20returns trigger as $$21begin22  new.updated_at = now();23  return new;24end;25$$ language plpgsql;2627drop trigger if exists trg_signup_email_domains_set_updated_at on public.signup_email_domains;2829create trigger trg_signup_email_domains_set_updated_at30before update on public.signup_email_domains31for each row32execute procedure update_signup_email_domains_updated_at();3334-- Seed example data35insert into public.signup_email_domains (domain, type, reason) values36  ('supabase.com', 'allow', 'Internal signups'),37  ('gmail.com', 'deny', 'Public email provider'),38  ('yahoo.com', 'deny', 'Public email provider');3940-- Create the function41create or replace function public.hook_restrict_signup_by_email_domain(event jsonb)42returns jsonb43language plpgsql44as $$45declare46  email text;47  domain text;48  is_allowed int;49  is_denied int;50begin51  email := event->'user'->>'email';52  domain := split_part(email, '@', 2);5354  -- Check for allow match55  select count(*) into is_allowed56  from public.signup_email_domains57  where type = 'allow' and lower(domain) = lower($1);5859  if is_allowed > 0 then60    return '{}'::jsonb;61  end if;6263  -- Check for deny match64  select count(*) into is_denied65  from public.signup_email_domains66  where type = 'deny' and lower(domain) = lower($1);6768  if is_denied > 0 then69    return jsonb_build_object(70      'error', jsonb_build_object(71        'message', 'Signups from this email domain are not allowed.',72        'http_code', 40373      )74    );75  end if;7677  -- No match, allow by default78  return '{}'::jsonb;79end;80$$;8182-- Permissions83grant execute84  on function public.hook_restrict_signup_by_email_domain85  to supabase_auth_admin;8687revoke execute88  on function public.hook_restrict_signup_by_email_domain89  from authenticated, anon, public;
```


================================================================================


# Custom Access Token Hook
Source: https://supabase.com/docs/guides/auth/auth-hooks/custom-access-token-hook

Custom Access Token Hook


### Customize the access token issued by Supabase Auth
Customize the access token issued by Supabase Auth

The custom access token hook runs before a token is issued and allows you to add additional claims based on the authentication method used.

Claims returned must conform to our specification. Supabase Auth will check for these claims after the hook is run and return an error if they are not present.

These are the fields currently available on an access token:

Required Claims:iss,aud,exp,iat,sub,role,aal,session_id,email,phone,is_anonymous

Optional Claims:jti,nbf,app_metadata,user_metadata,amr,

Inputs

```javascript
1{2  "user_id": "8ccaa7af-909f-44e7-84cb-67cdccb56be6",3  "claims": {4    "aud": "authenticated",5    "exp": 1715690221,6    "iat": 1715686621,7    "sub": "8ccaa7af-909f-44e7-84cb-67cdccb56be6",8    "email": "",9    "phone": "",10    "app_metadata": {},11    "user_metadata": {},12    "role": "authenticated",13    "aal": "aal1",14    "amr": [ { "method": "anonymous", "timestamp": 1715686621 } ],15    "session_id": "4b938a09-5372-4177-a314-cfa292099ea2",16    "is_anonymous": true,17    "client_id": "oauth-client-id-if-oauth-flow"18  },19  "authentication_method": "anonymous"20}
```

Outputs

Return these only if your hook processed the input without errors.

Sometimes the size of the JWT can be a problem especially if you're using aServer-Side Rendering framework. Common situations where the JWT can get too large include:

- The user has a particularly large name, email address or phone number
- The default JWT has too many claims coming from OAuth providers
- A large avatar URL is included

To lower the size of the JWT you can define a Custom Access Token hook like the one below which will instruct the Auth server to issue a JWT with only the listed claims. Check the documentation above on what JWT claims must be present and cannot be removed.

Refer to thePostgres JSON functionson how to manipulatejsonbobjects.

```javascript
1create or replace function public.custom_access_token_hook(event jsonb)2returns jsonb3language plpgsql4as $$5  declare6    original_claims jsonb;7    new_claims jsonb;8    claim text;9  begin10    original_claims = event->'claims';11    new_claims = '{}'::jsonb;1213    foreach claim in array array[14      -- add claims you want to keep here15      'iss',16      'aud',17      'exp',18      'iat',19      'sub',20      'role',21      'aal',22      'session_id',23      'email',24      'phone',25      'is_anonymous'26   ] loop27      if original_claims ? claim then28        -- original_claims contains one of the listed claims, set it on new_claims29        new_claims = jsonb_set(new_claims, array[claim], original_claims->claim);30      end if;31    end loop;3233    return jsonb_build_object('claims', new_claims);34  end35$$;
```


================================================================================


# MFA Verification Hook
Source: https://supabase.com/docs/guides/auth/auth-hooks/mfa-verification-hook

MFA Verification Hook

You can add additional checks to theSupabase MFA implementationwith hooks. For example, you can:

- Limit the number of verification attempts performed over a period of time.
- Sign out users who have too many invalid verification attempts.
- Count, rate limit, or ban sign-ins.

Inputs

Supabase Auth will send a payload containing these fields to your hook:

```javascript
1{2  "factor_id": "6eab6a69-7766-48bf-95d8-bd8f606894db",3  "user_id": "3919cb6e-4215-4478-a960-6d3454326cec",4  "valid": true5}
```

Outputs

Return this if your hook processed the input without errors.

```javascript
1{2  "decision": "reject",3  "message": "You have exceeded maximum number of MFA attempts."4}
```

Your company requires that a user can input an incorrect MFA Verification code no more than once every 2 seconds.

Create a table to record the last time a user had an incorrect MFA verification attempt for a factor.

```javascript
1create table public.mfa_failed_verification_attempts (2  user_id uuid not null,3  factor_id uuid not null,4  last_failed_at timestamp not null default now(),5  primary key (user_id, factor_id)6);
```

Create a hook to read and write information to this table. For example:

```javascript
1create function public.hook_mfa_verification_attempt(event jsonb)2  returns jsonb3  language plpgsql4as $$5  declare6    last_failed_at timestamp;7  begin8    if event->'valid' is true then9      -- code is valid, accept it10      return jsonb_build_object('decision', 'continue');11    end if;1213    select last_failed_at into last_failed_at14      from public.mfa_failed_verification_attempts15      where16        user_id = event->'user_id'17          and18        factor_id = event->'factor_id';1920    if last_failed_at is not null and now() - last_failed_at < interval '2 seconds' then21      -- last attempt was done too quickly22      return jsonb_build_object(23        'error', jsonb_build_object(24          'http_code', 429,25          'message',   'Please wait a moment before trying again.'26        )27      );28    end if;2930    -- record this failed attempt31    insert into public.mfa_failed_verification_attempts32      (33        user_id,34        factor_id,35        last_refreshed_at36      )37      values38      (39        event->'user_id',40        event->'factor_id',41        now()42      )43      on conflict do update44        set last_refreshed_at = now();4546    -- finally let Supabase Auth do the default behavior for a failed attempt47    return jsonb_build_object('decision', 'continue');48  end;49$$;5051-- Assign appropriate permissions and revoke access52grant all53  on table public.mfa_failed_verification_attempts54  to supabase_auth_admin;5556revoke all57  on table public.mfa_failed_verification_attempts58  from authenticated, anon, public;
```


================================================================================


# Password Verification Hook
Source: https://supabase.com/docs/guides/auth/auth-hooks/password-verification-hook

Password Verification Hook

Your company wishes to increase security beyond the requirements of the default password implementation in order to fulfill security or compliance requirements. You plan to track the status of a password sign-in attempt and take action via an email or a restriction on logins where necessary.

As this hook runs on unauthenticated requests, malicious users can abuse the hook by calling it multiple times. Pay extra care when using the hook as you can unintentionally block legitimate users from accessing your application.

Check if a password is valid prior to taking any additional action to ensure the user is legitimate. Where possible, send an email or notification instead of blocking the user.

Inputs

```javascript
1{2  "user_id": "3919cb6e-4215-4478-a960-6d3454326cec",3  "valid": true4}
```

Outputs

Return these only if your hook processed the input without errors.

```javascript
1{2  "decision": "reject",3  "message": "You have exceeded maximum number of password sign-in attempts.",4  "should_logout_user": "false"5}
```

As part of new security measures within the company, users can only input an incorrect password every 10 seconds and not more than that. You want to write a hook to enforce this.

Create a table to record each user's last incorrect password verification attempt.

```javascript
1create table public.password_failed_verification_attempts (2  user_id uuid not null,3  last_failed_at timestamp not null default now(),4  primary key (user_id)5);
```

Create a hook to read and write information to this table. For example:

```javascript
1create function public.hook_password_verification_attempt(event jsonb)2returns jsonb3language plpgsql4as $$5  declare6    last_failed_at timestamp;7  begin8    if event->'valid' is true then9      -- password is valid, accept it10      return jsonb_build_object('decision', 'continue');11    end if;1213    select last_failed_at into last_failed_at14      from public.password_failed_verification_attempts15      where16        user_id = event->'user_id';1718    if last_failed_at is not null and now() - last_failed_at < interval '10 seconds' then19      -- last attempt was done too quickly20      return jsonb_build_object(21        'error', jsonb_build_object(22          'http_code', 429,23          'message',   'Please wait a moment before trying again.'24        )25      );26    end if;2728    -- record this failed attempt29    insert into public.password_failed_verification_attempts30      (31        user_id,32        last_failed_at33      )34      values35      (36        event->'user_id',37        now()38      )39      on conflict do update40        set last_failed_at = now();4142    -- finally let Supabase Auth do the default behavior for a failed attempt43    return jsonb_build_object('decision', 'continue');44  end;45$$;4647-- Assign appropriate permissions48grant all49  on table public.password_failed_verification_attempts50  to supabase_auth_admin;5152revoke all53  on table public.password_failed_verification_attempts54  from authenticated, anon, public;
```


================================================================================


# Send Email Hook
Source: https://supabase.com/docs/guides/auth/auth-hooks/send-email-hook

Send Email Hook


### Use your own email service to send authentication emails.
Use your own email service to send authentication emails.

The Send Email Hook replaces Supabase's built-in email sending. You can use this hook to:

- Send emails using your own email provider
- Add internationalization or custom logic
- Fall back to another provider if your primary one fails

Inputs

```javascript
1{2  "user": {3    "id": "8484b834-f29e-4af2-bf42-80644d154f76",4    "aud": "authenticated",5    "role": "authenticated",6    "email": "valid.email@supabase.io",7    "phone": "",8    "app_metadata": {9      "provider": "email",10      "providers": ["email"]11    },12    "user_metadata": {13      "email": "valid.email@supabase.io",14      "email_verified": false,15      "phone_verified": false,16      "sub": "8484b834-f29e-4af2-bf42-80644d154f76"17    },18    "identities": [19      {20        "identity_id": "bc26d70b-517d-4826-bce4-413a5ff257e7",21        "id": "8484b834-f29e-4af2-bf42-80644d154f76",22        "user_id": "8484b834-f29e-4af2-bf42-80644d154f76",23        "identity_data": {24          "email": "valid.email@supabase.io",25          "email_verified": false,26          "phone_verified": false,27          "sub": "8484b834-f29e-4af2-bf42-80644d154f76"28        },29        "provider": "email",30        "last_sign_in_at": "2024-05-14T12:56:33.824231484Z",31        "created_at": "2024-05-14T12:56:33.824261Z",32        "updated_at": "2024-05-14T12:56:33.824261Z",33        "email": "valid.email@supabase.io"34      }35    ],36    "created_at": "2024-05-14T12:56:33.821567Z",37    "updated_at": "2024-05-14T12:56:33.825595Z",38    "is_anonymous": false39  },40  "email_data": {41    "token": "305805",42    "token_hash": "7d5b7b1964cf5d388340a7f04f1dbb5eeb6c7b52ef8270e1737a58d0",43    "redirect_to": "http://localhost:3000/",44    "email_action_type": "signup",45    "site_url": "http://localhost:9999",46    "token_new": "",47    "token_hash_new": "",48    "old_email": "",49    "old_phone": "",50    "provider": "",51    "factor_type": ""52  }53}
```

Outputs

- No outputs are required. An empty response with a status code of 200 is taken as a successful response.


### Email sending behavior#
Email sending depends on two settings: Email Provider and Auth Hook status.


### Email change behavior and token hash mapping#
Whenemail_action_typeisemail_change, the hook payload can include one or two OTPs and their hashes. This depends on yourSecure Email Changesetting.

- Secure Email Change enabled: two OTPs are generated, one for the current email (user.email) and one for the new email (user.email_new). You must send two emails.
- Secure Email Change disabled: only one OTP is generated for the new email. You send a single email.

The token hash field names are reversed due to backward compatibility. Pay careful attention to which token/hash pair goes with which email address:

- token_hash_new→ use with thecurrentemail address (user.email) andtoken
- token_hash→ use with thenewemail address (user.email_new) andtoken_new

Do not assume the_newsuffix refers to the new email address.


### What to send#
When Secure Email Change is enabled (both token/hash pairs present):

- Send tocurrentemail address (user.email): usetokenwithtoken_hash_new
- Send tonewemail address (user.email_new): usetoken_newwithtoken_hash

When Secure Email Change isdisabled(only one token/hash pair present):

- Send a single email to thenewemail address. Usetokenwithtoken_hashortoken_newwithtoken_hash, depending on which fields are present in the payload.

You can configureResendas the custom email provider through the "Send Email" hook. This allows you to take advantage of Resend's developer-friendly APIs to send emails and leverageReact Emailfor managing your email templates. For a more advanced React Email tutorial, refer tothis guide.

If you want to send emails through the Supabase Resend integration, which uses Resend's SMTP server, check outthis integrationinstead.

Create a.envfile with the following environment variables:

```javascript
1RESEND_API_KEY="your_resend_api_key"2SEND_EMAIL_HOOK_SECRET="v1,whsec_<base64_secret>"
```

You can generate the secret in theAuth Hookssection of the Supabase dashboard.

Set the secrets in your Supabase project:

```javascript
1supabase secrets set --env-file .env
```

Create a new edge function:

```javascript
1supabase functions new send-email
```

Add the following code to your edge function:

```javascript
1import { Webhook } from "https://esm.sh/standardwebhooks@1.0.0";2import { Resend } from "npm:resend";34const resend = new Resend(Deno.env.get("RESEND_API_KEY") as string);5const hookSecret = (Deno.env.get("SEND_EMAIL_HOOK_SECRET") as string).replace("v1,whsec_", "");67Deno.serve(async (req) => {8  if (req.method !== "POST") {9    return new Response("not allowed", { status: 400 });10  }1112  const payload = await req.text();13  const headers = Object.fromEntries(req.headers);14  const wh = new Webhook(hookSecret);15  try {16    const { user, email_data } = wh.verify(payload, headers) as {17      user: {18        email: string;19      };20      email_data: {21        token: string;22        token_hash: string;23        redirect_to: string;24        email_action_type: string;25        site_url: string;26        token_new: string;27        token_hash_new: string;28      };29    };3031    const { error } = await resend.emails.send({32      from: "welcome <onboarding@example.com>",33      to: [user.email],34      subject: "Welcome to my site!",35      text: `Confirm you signup with this code: ${email_data.token}`,36    });37    if (error) {38      throw error;39    }40  } catch (error) {41    return new Response(42      JSON.stringify({43        error: {44          http_code: error.code,45          message: error.message,46        },47      }),48      {49        status: 401,50        headers: { "Content-Type": "application/json" },51      },52    );53  }5455  const responseHeaders = new Headers();56  responseHeaders.set("Content-Type", "application/json");57  return new Response(JSON.stringify({}), {58    status: 200,59    headers: responseHeaders,60  });61});
```

Deploy your edge function andconfigure it as a hook:

```javascript
1supabase functions deploy send-email --no-verify-jwt
```


================================================================================


# Send SMS Hook
Source: https://supabase.com/docs/guides/auth/auth-hooks/send-sms-hook

Send SMS Hook


### Use your own SMS service to send authentication messages.
Use your own SMS service to send authentication messages.

The Send SMS Hook replaces Supabase's built-in SMS sending. You can use this hook to:

- Use a regional SMS Provider
- Use alternate messaging channels such as WhatsApp
- Fall back to another provider if your primary one fails
- Adjust the message body to include platform specific fields such as theAppHash

Inputs

```javascript
1{2  "user": {3    "id": "6481a5c1-3d37-4a56-9f6a-bee08c554965",4    "aud": "authenticated",5    "role": "authenticated",6    "email": "",7    "phone": "+1333363128",8    "phone_confirmed_at": "2024-05-13T11:52:48.157306Z",9    "confirmation_sent_at": "2024-05-14T12:31:52.824573Z",10    "confirmed_at": "2024-05-13T11:52:48.157306Z",11    "phone_change_sent_at": "2024-05-13T11:47:02.183064Z",12    "last_sign_in_at": "2024-05-13T11:52:48.162518Z",13    "app_metadata": {14      "provider": "phone",15      "providers": ["phone"]16    },17    "user_metadata": {},18    "identities": [19      {20        "identity_id": "3be5e552-65aa-41d9-9db9-2a502f845459",21        "id": "6481a5c1-3d37-4a56-9f6a-bee08c554965",22        "user_id": "6481a5c1-3d37-4a56-9f6a-bee08c554965",23        "identity_data": {24          "email_verified": false,25          "phone": "+1612341244428",26          "phone_verified": true,27          "sub": "6481a5c1-3d37-4a56-9f6a-bee08c554965"28        },29        "provider": "phone",30        "last_sign_in_at": "2024-05-13T11:52:48.155562Z",31        "created_at": "2024-05-13T11:52:48.155599Z",32        "updated_at": "2024-05-13T11:52:48.159391Z"33      }34    ],35    "created_at": "2024-05-13T11:45:33.7738Z",36    "updated_at": "2024-05-14T12:31:52.82475Z",37    "is_anonymous": false38  },39  "sms": {40    "otp": "561166"41  }42}
```

Outputs

- No outputs are required. An empty response with a status code of 200 is taken as a successful response.

Your company uses a worker to manage all messaging related jobs. For performance reasons, the messaging system sends messages in intervals via a job queue. Instead of sending a message immediately, messages are queued and sent in periodic intervals viapg_cron.

Create a table to store jobs

```javascript
1create table job_queue (2  job_id uuid primary key default gen_random_uuid(),3  job_data jsonb not null,4  created_at timestamp default now(),5  status text default 'pending',6  priority int default 0,7  retry_count int default 0,8  max_retries int default 2,9  scheduled_at timestamp default now()10);
```

Create the hook:

```javascript
1create or replace function send_sms(event jsonb) returns void as $$2declare3    job_data jsonb;4    scheduled_time timestamp;5    priority int;6begin7    -- extract phone and otp from the event json8    job_data := jsonb_build_object(9        'phone', event->'user'->>'phone',10        'otp', event->'sms'->>'otp'11    );1213    -- calculate the nearest 5-minute window for scheduled_time14    scheduled_time := date_trunc('minute', now()) + interval '5 minute' * floor(extract('epoch' from (now() - date_trunc('minute', now())) / 60) / 5);1516    -- assign priority dynamically (example logic: higher priority for earlier scheduled time)17    priority := extract('epoch' from (scheduled_time - now()))::int;1819    -- insert the job into the job_queue table20    insert into job_queue (job_data, priority, scheduled_at, max_retries)21    values (job_data, priority, scheduled_time, 2);22end;23$$ language plpgsql;2425grant all26  on table public.job_queue27  to supabase_auth_admin;2829revoke all30  on table public.job_queue31  from authenticated, anon;
```

Create a function to periodically run and dequeue all jobs

```javascript
1create or replace function dequeue_and_run_jobs() returns void as $$2declare3    job record;4begin5    for job in6        select * from job_queue7        where status = 'pending'8          and scheduled_at <= now()9        order by priority desc, created_at10        for update skip locked11    loop12        begin13            -- add job processing logic here.14            -- for demonstration, we'll just update the job status to 'completed'.15            update job_queue16            set status = 'completed'17            where job_id = job.job_id;1819        exception when others then20            -- handle job failure and retry logic21            if job.retry_count < job.max_retries then22                update job_queue23                set retry_count = retry_count + 1,24                    scheduled_at = now() + interval '1 minute'  -- delay retry by 1 minute25                where job_id = job.job_id;26            else27                update job_queue28                set status = 'failed'29                where job_id = job.job_id;30            end if;31        end;32    end loop;33end;34$$ language plpgsql;3536grant execute37  on function public.dequeue_and_run_jobs38  to supabase_auth_admin;3940revoke execute41  on function public.dequeue_and_run_jobs42  from authenticated, anon;
```

Configurepg_cronto run the job on an interval. You can use a tool likecrontab.guruto check that your job is running on an appropriate schedule. Ensure thatpg_cronis enabled underDatabase > Extensions

```javascript
1select2  cron.schedule(3    '* * * * *', -- this cron expression means every minute.4    'select dequeue_and_run_jobs();'5  );
```


================================================================================


# Identity Linking
Source: https://supabase.com/docs/guides/auth/auth-identity-linking

Identity Linking


### Manage the identities associated with your user
Manage the identities associated with your user


### Identity linking strategies#
Currently, Supabase Auth supports 2 strategies to link an identity to a user:


### Automatic linking#
Supabase Auth automatically links identities with the same email address to a single user. This helps to improve the user experience when multiple OAuth login options are presented since the user does not need to remember which OAuth account they used to sign up with. When a new user signs in with OAuth, Supabase Auth will attempt to look for an existing user that uses the same email address. If a match is found, the new identity is linked to the user.

In order for automatic linking to correctly identify the user for linking, Supabase Auth needs to ensure that all user emails are unique. It would also be an insecure practice to automatically link an identity to a user with an unverified email address since that could lead to pre-account takeover attacks. To prevent this from happening, when a new identity can be linked to an existing user, Supabase Auth will remove any other unconfirmed identities linked to an existing user.

Users that signed up withSAML SSOwill not be considered as targets for automatic linking.


### Manual linking (beta)#
Supabase Auth allows a user to initiate identity linking with a different email address when they are logged in. To link an OAuth identity to the user, calllinkIdentity():

```javascript
1const { ,  } = await ..({ : 'google' })
```

In the example above, the user will be redirected to Google to complete the OAuth2.0 flow. Once the OAuth2.0 flow has completed successfully, the user will be redirected back to the application and the Google identity will be linked to the user. You can enable manual linking from your project's authenticationconfiguration optionsor by setting the environment variableGOTRUE_SECURITY_MANUAL_LINKING_ENABLED: truewhen self-hosting.


### Link identity with native OAuth (ID token)#
For native mobile applications, you can link an identity using an ID token obtained from a third-party OAuth provider. This is useful when you want to use native OAuth flows (like Google Sign-In or Sign in with Apple) rather than web-based OAuth redirects.

```javascript
1// Example with Google Sign-In (using a native Google Sign-In library)2const idToken = 'ID_TOKEN_FROM_GOOGLE'3const accessToken = 'ACCESS_TOKEN_FROM_GOOGLE'45const { data, error } = await supabase.auth.linkIdentity({6  provider: 'google',7  token: idToken,8  access_token: accessToken,9})
```


### Unlink an identity#
You can usegetUserIdentities()to fetch all the identities linked to a user. Then, callunlinkIdentity()to unlink the identity. The user needs to be logged in and have at least 2 linked identities in order to unlink an existing identity.

```javascript
1// retrieve all identities linked to a user2const { : , :  } = await ..()34if (!) {5  // find the google identity linked to the user6  const  = ..(() => . === 'google')78  if () {9    // unlink the google identity from the user10    const { ,  } = await ..()11  }12}
```


### Frequently asked questions#

### How to add email/password login to an OAuth account?#
Call theupdateUser({ password: 'validpassword'})to add email with password authentication to an account created with an OAuth provider (Google, GitHub, etc.).


### Can you sign up with email if already using OAuth?#
If you try to create an email account after previously signing up with OAuth using the same email, you'll receive an obfuscated user response with no verification email sent. This prevents user enumeration attacks.


================================================================================


# Multi-Factor Authentication
Source: https://supabase.com/docs/guides/auth/auth-mfa

Multi-Factor Authentication

Multi-factor authentication (MFA), sometimes called two-factor authentication (2FA), adds an additional layer of security to your application by verifying their identity through additional verification steps.

It is considered a best practice to use MFA for your applications.

Users with weak passwords or compromised social login accounts are prone to malicious account takeovers. These can be prevented with MFA because they require the user to provide proof of both of these:

- Something they know.
Password, or access to a social-login account.
- Something they have.
Access to an authenticator app (a.k.a. TOTP) or a mobile phone.


### Overview#
Supabase Auth implements MFA via two methods: App Authenticator, which makes use of a Time based-one Time Password, and phone messaging, which makes use of a code generated by Supabase Auth.

Applications using MFA require two important flows:

Supabase Auth provides:

- Enrollment API- build rich user interfaces for adding and removing factors.
- Challenge and Verify APIs- securely verify that the user has access to a factor.
- List Factors API- build rich user interfaces for signing in with additional factors.

You can control access to the Enrollment API as well as the Challenge and Verify APIs via the Supabase Dashboard. A setting ofVerification Disabledwill disable both the challenge API and the verification API.

These sets of APIs let you control the MFA experience that works for you. You can create flows where MFA is optional, mandatory for all, or only specific groups of users.

Once users have enrolled or signed-in with a factor, Supabase Auth adds additional metadata to the user's access token (JWT) that your application can use to allow or deny access.

This information is represented by anAuthenticator Assurance Level, a standard measure about the assurance of the user's identity Supabase Auth has for that particular session. There are two levels recognized today:

This assurance level is encoded in theaalclaim in the JWT associated with the user. By decoding this value you can create custom authorization rules in your frontend, backend, and database that will enforce the MFA policy that works for your application. JWTs without anaalclaim are at theaal1level.


### Adding to your app#
Adding MFA to your app involves these four steps:

The enrollment flow and the challenge steps differ by factor and are covered on a separate page. Visit thePhoneorApp Authenticatorpages to see how to add the flows for the respective factors. You can combine both flows and allow for use of both Phone and App Authenticator Factors.


### Add unenroll flow#
The unenroll process is the same for both Phone and TOTP factors.

An unenroll flow provides a UI for users to manage and unenroll factors linked to their accounts. Most applications do so via a factor management page where users can view and unlink selected factors.

When a user unenrolls a factor, callsupabase.auth.mfa.unenroll()with the ID of the factor. For example, call:

```javascript
1...({ : 'd30fd651-184e-4748-a928-0a4b9be1d429' })
```

to unenroll a factor with IDd30fd651-184e-4748-a928-0a4b9be1d429.


### Enforce rules for MFA logins#
Adding MFA to your app's UI does not in-and-of-itself offer a higher level of security to your users. You also need to enforce the MFA rules in your application's database, APIs, and server-side rendering.

Depending on your application's needs, there are three ways you can choose to enforce MFA.


### Example: React#
Below is an example that creates a newUnenrollMFAcomponent that illustrates the important pieces of the MFA enrollment flow. Note that users can only unenroll a factor after completing the enrollment flow and obtaining anaal2JWT claim. Here are some points of note:

- When the component appears on screen, thesupabase.auth.mfa.listFactors()endpoint
fetches all existing factors together with their details.
- The existing factors for a user are displayed in a table.
- Once the user has selected a factor to unenroll, they can type in thefactorIdand clickUnenrollwhich creates a confirmation modal.

Unenrolling a factor will downgrade the assurance level fromaal2toaal1only after the refresh interval has lapsed. For an immediate downgrade fromaal2toaal1after enrolling one will need to manually callrefreshSession()

```javascript
1/**2 * UnenrollMFA shows a simple table with the list of factors together with a button to unenroll.3 * When a user types in the factorId of the factor that they wish to unenroll and clicks unenroll4 * the corresponding factor will be unenrolled.5 */6export function UnenrollMFA() {7  const [factorId, setFactorId] = useState('')8  const [factors, setFactors] = useState([])9  const [error, setError] = useState('') // holds an error message1011  useEffect(() => {12    ;(async () => {13      const { data, error } = await supabase.auth.mfa.listFactors()14      if (error) {15        throw error16      }1718      setFactors([...data.totp, ...data.phone])19    })()20  }, [])2122  return (23    <>24      {error && <div className="error">{error}</div>}25      <tbody>26        <tr>27          <td>Factor ID</td>28          <td>Friendly Name</td>29          <td>Factor Status</td>30          <td>Phone Number</td>31        </tr>32        {factors.map((factor) => (33          <tr>34            <td>{factor.id}</td>35            <td>{factor.friendly_name}</td>36            <td>{factor.factor_type}</td>37            <td>{factor.status}</td>38            <td>{factor.phone}</td>39          </tr>40        ))}41      </tbody>42      <input type="text" value={verifyCode} onChange={(e) => setFactorId(e.target.value.trim())} />43      <button onClick={() => supabase.auth.mfa.unenroll({ factorId })}>Unenroll</button>44    </>45  )46}
```


### Database#
Your app should sufficiently deny or allow access to tables or rows based on the user's current and possible authenticator levels.

Postgres has two types of policies: permissive and restrictive. This guide uses restrictive policies. Make sure you don't omit theas restrictiveclause.

If your app falls under this case, this is a template Row Level Security policy you can apply to all your tables:

```javascript
1create policy "Policy name."2  on table_name3  as restrictive4  to authenticated5  using ((select auth.jwt()->>'aal') = 'aal2');
```

- Here the policy will not accept any JWTs with anaalclaim other thanaal2, which is the highest authenticator assurance level.
- Usingas restrictiveensures this policy will restrict all commands on the
table regardless of other policies!

If your app falls under this case, the rules get more complex. User accounts created past a certain timestamp must have aaal2level to access the database.

```javascript
1create policy "Policy name."2  on table_name3  as restrictive -- very important!4  to authenticated5  using6    (array[(select auth.jwt()->>'aal')] <@ (7       select8         case9           when created_at >= '2022-12-12T00:00:00Z' then array['aal2']10           else array['aal1', 'aal2']11         end as aal12       from auth.users13       where (select auth.uid()) = id));
```

- The policy will accept bothaal1andaal2for users with acreated_attimestamp prior to 12th December 2022 at 00:00 UTC, but will only acceptaal2for all other timestamps.
- The<@operator is PostgreSQL's"contained in"
operator.
- Usingas restrictiveensures this policy will restrict all commands on the
table regardless of other policies!

Users that have enrolled MFA on their account are expecting that your
application only works for them if they've gone through MFA.

```javascript
1create policy "Policy name."2  on table_name3  as restrictive -- very important!4  to authenticated5  using (6    array[(select auth.jwt()->>'aal')] <@ (7      select8          case9            when count(id) > 0 then array['aal2']10            else array['aal1', 'aal2']11          end as aal12        from auth.mfa_factors13        where ((select auth.uid()) = user_id) and status = 'verified'14    ));
```

- The policy will only accept onlyaal2when the user has at least one MFA
factor verified.
- Otherwise, it will accept bothaal1andaal2.
- The<@operator is PostgreSQL's"contained in"
operator.
- Usingas restrictiveensures this policy will restrict all commands on the
table regardless of other policies!


### Server-Side Rendering#
When using the Supabase JavaScript library in a server-side rendering context, make sure you always create a new object for each request! This will prevent you from accidentally rendering and serving content belonging to different users.

It is possible to enforce MFA on the Server-Side Rendering level. However, this can be tricky do to well.

You can use thesupabase.auth.mfa.getAuthenticatorAssuranceLevel()andsupabase.auth.mfa.listFactors()APIs to identify the AAL level of the session and any factors that are enabled for a user, similar to how you would use these on the browser.

However, encountering a different AAL level on the server may not actually be a security problem. Consider these likely scenarios:

We thus recommend you redirect users to a page where they can authenticate using their additional factor, instead of rendering a HTTP 401 Unauthorized or HTTP 403 Forbidden content.


### APIs#
If your application uses the Supabase Database, Storage or Edge Functions, just using Row Level Security policies will give you sufficient protection. In the event that you have other APIs that you wish to protect, follow these general guidelines:


### Frequently asked questions#

================================================================================


# Multi-Factor Authentication (Phone)
Source: https://supabase.com/docs/guides/auth/auth-mfa/phone

Multi-Factor Authentication (Phone)


### How does phone multi-factor-authentication work?#
Phone multi-factor authentication involves a shared code generated by Supabase Auth and the end user. The code is delivered via a messaging channel, such as SMS or WhatsApp, and the user uses the code to authenticate to Supabase Auth.

The phone messaging configuration for MFA is shared withphone auth login. The same provider configuration that is used for phone login is used for MFA. You can also use theSend SMS Hookif you need to use an MFA (Phone) messaging provider different from what is supported natively.

Below is a flow chart illustrating how the Enrollment and Verify APIs work in the context of MFA (Phone).


### Add enrollment flow#
An enrollment flow provides a UI for users to set up additional authentication factors. Most applications add the enrollment flow in two places within their app:

As far as possible, maintain a generic flow that you can reuse in both cases with minor modifications.

Enrolling a factor for use with MFA takes three steps for phone MFA:


### Example: React#
Below is an example that creates a newEnrollMFAcomponent that illustrates the important pieces of the MFA enrollment flow.

- When the component appears on screen, thesupabase.auth.mfa.enroll()API is
called once to start the process of enrolling a new factor for the current
user.
- A challenge is created using thesupabase.auth.mfa.challenge()API and the
code from the user is submitted for verification using thesupabase.auth.mfa.verify()challenge.
- onEnabledis a callback that notifies the other components that enrollment
has completed.
- onCancelledis a callback that notifies the other components that the user
has clicked theCancelbutton.

```javascript
1export function EnrollMFA({2  onEnrolled,3  onCancelled,4}: {5  onEnrolled: () => void6  onCancelled: () => void7}) {8  const [phoneNumber, setPhoneNumber] = useState('')9  const [factorId, setFactorId] = useState('')10  const [verifyCode, setVerifyCode] = useState('')11  const [error, setError] = useState('')12  const [challengeId, setChallengeId] = useState('')1314  const onEnableClicked = () => {15    setError('')16    ;(async () => {17      const verify = await auth.mfa.verify({18        factorId,19        challengeId,20        code: verifyCode,21      })22      if (verify.error) {23        setError(verify.error.message)24        throw verify.error25      }2627      onEnrolled()28    })()29  }30  const onEnrollClicked = async () => {31    setError('')32    try {33      const factor = await auth.mfa.enroll({34        phone: phoneNumber,35        factorType: 'phone',36      })37      if (factor.error) {38        setError(factor.error.message)39        throw factor.error40      }4142      setFactorId(factor.data.id)43    } catch (error) {44      setError('Failed to Enroll the Factor.')45    }46  }4748  const onSendOTPClicked = async () => {49    setError('')50    try {51      const challenge = await auth.mfa.challenge({ factorId })52      if (challenge.error) {53        setError(challenge.error.message)54        throw challenge.error55      }5657      setChallengeId(challenge.data.id)58    } catch (error) {59      setError('Failed to resend the code.')60    }61  }6263  return (64    <>65      {error && <div className="error">{error}</div>}66      <input67        type="text"68        placeholder="Phone Number"69        value={phoneNumber}70        onChange={(e) => setPhoneNumber(e.target.value.trim())}71      />72      <input73        type="text"74        placeholder="Verification Code"75        value={verifyCode}76        onChange={(e) => setVerifyCode(e.target.value.trim())}77      />78      <input type="button" value="Enroll" onClick={onEnrollClicked} />79      <input type="button" value="Submit Code" onClick={onEnableClicked} />80      <input type="button" value="Send OTP Code" onClick={onSendOTPClicked} />81      <input type="button" value="Cancel" onClick={onCancelled} />82    </>83  )84}
```


### Add a challenge step to login#
Once a user has logged in via their first factor (email+password, magic link, one time password, social login etc.) you need to perform a check if any additional factors need to be verified.

This can be done by using thesupabase.auth.mfa.getAuthenticatorAssuranceLevel()API. When the user signs in and is redirected back to your app, you should call this method to extract the user's current and next authenticator assurance level (AAL).

Therefore if you receive acurrentLevelwhich isaal1but anextLevelofaal2, the user should be given the option to go through MFA.

Below is a table that explains the combined meaning.


### Example: React#
Adding the challenge step to login depends heavily on the architecture of your app. However, a fairly common way to structure React apps is to have a large component (often namedApp) which contains most of the authenticated application logic.

This example will wrap this component with logic that will show an MFA challenge screen if necessary, before showing the full application. This is illustrated in theAppWithMFAexample below.

```javascript
1function AppWithMFA() {2  const [readyToShow, setReadyToShow] = useState(false)3  const [showMFAScreen, setShowMFAScreen] = useState(false)45  useEffect(() => {6    ;(async () => {7      try {8        const { data, error } = await supabase.auth.mfa.getAuthenticatorAssuranceLevel()9        if (error) {10          throw error11        }1213        console.log(data)1415        if (data.nextLevel === 'aal2' && data.nextLevel !== data.currentLevel) {16          setShowMFAScreen(true)17        }18      } finally {19        setReadyToShow(true)20      }21    })()22  }, [])2324  if (readyToShow) {25    if (showMFAScreen) {26      return <AuthMFA />27    }2829    return <App />30  }3132  return <></>33}
```

- supabase.auth.mfa.getAuthenticatorAssuranceLevel()does return a promise.
Don't worry, this is a very fast method (microseconds) as it rarely uses the
network.
- readyToShowonly makes sure the AAL check completes before showing any
application UI to the user.
- If the current level can be upgraded to the next one, the MFA screen is
shown.
- Once the challenge is successful, theAppcomponent is finally rendered on
screen.

Below is the component that implements the challenge and verify logic.

```javascript
1function AuthMFA() {2  const [verifyCode, setVerifyCode] = useState('')3  const [error, setError] = useState('')4  const [factorId, setFactorId] = useState('')5  const [challengeId, setChallengeId] = useState('')6  const [phoneNumber, setPhoneNumber] = useState('')78  const startChallenge = async () => {9    setError('')10    try {11      const factors = await supabase.auth.mfa.listFactors()12      if (factors.error) {13        throw factors.error14      }1516      const phoneFactor = factors.data.phone[0]1718      if (!phoneFactor) {19        throw new Error('No phone factors found!')20      }2122      const factorId = phoneFactor.id23      setFactorId(factorId)24      setPhoneNumber(phoneFactor.phone)2526      const challenge = await supabase.auth.mfa.challenge({ factorId })27      if (challenge.error) {28        setError(challenge.error.message)29        throw challenge.error30      }3132      setChallengeId(challenge.data.id)33    } catch (error) {34      setError(error.message)35    }36  }3738  const verifyCode = async () => {39    setError('')40    try {41      const verify = await supabase.auth.mfa.verify({42        factorId,43        challengeId,44        code: verifyCode,45      })46      if (verify.error) {47        setError(verify.error.message)48        throw verify.error49      }50    } catch (error) {51      setError(error.message)52    }53  }5455  return (56    <>57      <div>Please enter the code sent to your phone.</div>58      {phoneNumber && <div>Phone number: {phoneNumber}</div>}59      {error && <div className="error">{error}</div>}60      <input61        type="text"62        value={verifyCode}63        onChange={(e) => setVerifyCode(e.target.value.trim())}64      />65      {!challengeId ? (66        <input type="button" value="Start Challenge" onClick={startChallenge} />67      ) : (68        <input type="button" value="Verify Code" onClick={verifyCode} />69      )}70    </>71  )72}
```

- You can extract the available MFA factors for the user by callingsupabase.auth.mfa.listFactors(). Don't worry this method is also very quick
and rarely uses the network.
- IflistFactors()returns more than one factor (or of a different type) you
should present the user with a choice. For simplicity this is not shown in
the example.
- Phone numbers are unique per user. Users can only have one verified phone factor with a given phone number.
Attempting to enroll a new phone factor alongside an existing verified factor with the same number will result in an error.
- Each time the user presses the "Submit" button a new challenge is created for
the chosen factor (in this case the first one)
- On successful verification, the client library will refresh the session in
the background automatically and finally call theonSuccesscallback, which
will show the authenticatedAppcomponent on screen.


### Security configuration#
Each code is valid for up to 5 minutes, after which a new one can be sent. Successive codes remain valid until expiry. When possible choose the longest code length acceptable to your use case, at a minimum of 6. This can be configured in theAuthentication Settings.

Be aware that Phone MFA is vulnerable to SIM swap attacks where an attacker will call a mobile provider and ask to port the target's phone number to a new SIM card and then use the said SIM card to intercept an MFA code. Evaluate the your application's tolerance for such an attack. You can read more about SIM swapping attackshere


### Pricing#
$0.1027per hour ($75per month) for the first project.$0.0137per
hour ($10per month) for every additional project.

For a detailed breakdown of how charges are calculated, refer toManage Advanced MFA Phone usage.


================================================================================


# Multi-Factor Authentication (TOTP)
Source: https://supabase.com/docs/guides/auth/auth-mfa/totp

Multi-Factor Authentication (TOTP)


### How does app authenticator multi-factor authentication work?#
App Authenticator (TOTP) multi-factor authentication involves a timed one-time password generated from an authenticator app in the control of users. It uses a QR Code which to transmit a shared secret used to generate a One Time Password. A user can scan a QR code with their phone to capture a shared secret required for subsequent authentication.

The use of a QR code wasinitially introduced by Google Authenticatorbut is now universally accepted by all authenticator apps. The QR code has an alternate representation in URI form following theotpauthscheme such as:otpauth://totp/supabase:alice@supabase.com?secret=<secret>&issuer=supabasewhich a user can manually input in cases where there is difficulty rendering a QR Code.

Below is a flow chart illustrating how the Enrollment, Challenge, and Verify APIs work in the context of MFA (TOTP).

TOTP MFA APIis free to use and is enabled on all Supabase projects by default.


### Add enrollment flow#
An enrollment flow provides a UI for users to set up additional authentication factors. Most applications add the enrollment flow in two places within their app:

Enrolling a factor for use with MFA takes three steps:


### Example: React#
Below is an example that creates a newEnrollMFAcomponent that illustrates the important pieces of the MFA enrollment flow.

- When the component appears on screen, thesupabase.auth.mfa.enroll()API is
called once to start the process of enrolling a new factor for the current
user.
- This API returns a QR code in the SVG format, which is shown on screen using
a normal<img>tag by encoding the SVG as a data URL.
- Once the user has scanned the QR code with their authenticator app, they
should enter the verification code within theverifyCodeinput field and
click onEnable.
- A challenge is created using thesupabase.auth.mfa.challenge()API and the
code from the user is submitted for verification using thesupabase.auth.mfa.verify()challenge.
- onEnabledis a callback that notifies the other components that enrollment
has completed.
- onCancelledis a callback that notifies the other components that the user
has clicked theCancelbutton.

```javascript
1/**2 * EnrollMFA shows a simple enrollment dialog. When shown on screen it calls3 * the `enroll` API. Each time a user clicks the Enable button it calls the4 * `challenge` and `verify` APIs to check if the code provided by the user is5 * valid.6 * When enrollment is successful, it calls `onEnrolled`. When the user clicks7 * Cancel the `onCancelled` callback is called.8 */9export function EnrollMFA({10  onEnrolled,11  onCancelled,12}: {13  onEnrolled: () => void14  onCancelled: () => void15}) {16  const [factorId, setFactorId] = useState('')17  const [qr, setQR] = useState('') // holds the QR code image SVG18  const [verifyCode, setVerifyCode] = useState('') // contains the code entered by the user19  const [error, setError] = useState('') // holds an error message2021  const onEnableClicked = () => {22    setError('')23    ;(async () => {24      const challenge = await supabase.auth.mfa.challenge({ factorId })25      if (challenge.error) {26        setError(challenge.error.message)27        throw challenge.error28      }2930      const challengeId = challenge.data.id3132      const verify = await supabase.auth.mfa.verify({33        factorId,34        challengeId,35        code: verifyCode,36      })37      if (verify.error) {38        setError(verify.error.message)39        throw verify.error40      }4142      onEnrolled()43    })()44  }4546  useEffect(() => {47    ;(async () => {48      const { data, error } = await supabase.auth.mfa.enroll({49        factorType: 'totp',50      })51      if (error) {52        throw error53      }5455      setFactorId(data.id)5657      // Supabase Auth returns an SVG QR code which you can convert into a data58      // URL that you can place in an <img> tag.59      setQR(data.totp.qr_code)60    })()61  }, [])6263  return (64    <>65      {error && <div className="error">{error}</div>}66      <img src={qr} />67      <input68        type="text"69        value={verifyCode}70        onChange={(e) => setVerifyCode(e.target.value.trim())}71      />72      <input type="button" value="Enable" onClick={onEnableClicked} />73      <input type="button" value="Cancel" onClick={onCancelled} />74    </>75  )76}
```


### Add a challenge step to login#
Once a user has logged in via their first factor (email+password, magic link, one time password, social login etc.) you need to perform a check if any additional factors need to be verified.

This can be done by using thesupabase.auth.mfa.getAuthenticatorAssuranceLevel()API. When the user signs in and is redirected back to your app, you should call this method to extract the user's current and next authenticator assurance level (AAL).

Therefore if you receive acurrentLevelwhich isaal1but anextLevelofaal2, the user should be given the option to go through MFA.

Below is a table that explains the combined meaning.


### Example: React#
Adding the challenge step to login depends heavily on the architecture of your app. However, a fairly common way to structure React apps is to have a large component (often namedApp) which contains most of the authenticated application logic.

This example will wrap this component with logic that will show an MFA challenge screen if necessary, before showing the full application. This is illustrated in theAppWithMFAexample below.

```javascript
1function AppWithMFA() {2  const [readyToShow, setReadyToShow] = useState(false)3  const [showMFAScreen, setShowMFAScreen] = useState(false)45  useEffect(() => {6    ;(async () => {7      try {8        const { data, error } = await supabase.auth.mfa.getAuthenticatorAssuranceLevel()9        if (error) {10          throw error11        }1213        console.log(data)1415        if (data.nextLevel === 'aal2' && data.nextLevel !== data.currentLevel) {16          setShowMFAScreen(true)17        }18      } finally {19        setReadyToShow(true)20      }21    })()22  }, [])2324  if (readyToShow) {25    if (showMFAScreen) {26      return <AuthMFA />27    }2829    return <App />30  }3132  return <></>33}
```

- supabase.auth.mfa.getAuthenticatorAssuranceLevel()does return a promise.
Don't worry, this is a very fast method (microseconds) as it rarely uses the
network.
- readyToShowonly makes sure the AAL check completes before showing any
application UI to the user.
- If the current level can be upgraded to the next one, the MFA screen is
shown.
- Once the challenge is successful, theAppcomponent is finally rendered on
screen.

Below is the component that implements the challenge and verify logic.

```javascript
1function AuthMFA() {2  const [verifyCode, setVerifyCode] = useState('')3  const [error, setError] = useState('')45  const onSubmitClicked = () => {6    setError('')7    ;(async () => {8      const factors = await supabase.auth.mfa.listFactors()9      if (factors.error) {10        throw factors.error11      }1213      const totpFactor = factors.data.totp[0]1415      if (!totpFactor) {16        throw new Error('No TOTP factors found!')17      }1819      const factorId = totpFactor.id2021      const challenge = await supabase.auth.mfa.challenge({ factorId })22      if (challenge.error) {23        setError(challenge.error.message)24        throw challenge.error25      }2627      const challengeId = challenge.data.id2829      const verify = await supabase.auth.mfa.verify({30        factorId,31        challengeId,32        code: verifyCode,33      })34      if (verify.error) {35        setError(verify.error.message)36        throw verify.error37      }38    })()39  }4041  return (42    <>43      <div>Please enter the code from your authenticator app.</div>44      {error && <div className="error">{error}</div>}45      <input46        type="text"47        value={verifyCode}48        onChange={(e) => setVerifyCode(e.target.value.trim())}49      />50      <input type="button" value="Submit" onClick={onSubmitClicked} />51    </>52  )53}
```

- You can extract the available MFA factors for the user by callingsupabase.auth.mfa.listFactors(). Don't worry this method is also very quick
and rarely uses the network.
- IflistFactors()returns more than one factor (or of a different type) you
should present the user with a choice. For simplicity this is not shown in
the example.
- Each time the user presses the "Submit" button a new challenge is created for
the chosen factor (in this case the first one) and it is immediately
verified. Any errors are displayed to the user.
- On successful verification, the client library will refresh the session in
the background automatically and finally call theonSuccesscallback, which
will show the authenticatedAppcomponent on screen.


### Frequently asked questions#

================================================================================


# Send emails with custom SMTP
Source: https://supabase.com/docs/guides/auth/auth-smtp

Send emails with custom SMTP

If you're using Supabase Auth with the following configuration:

- Email and password accounts
- Passwordless accounts using one-time passwords or links sent over email (OTP, magic link, invites)
- Email-based user invitations from theUsers pageor from the Auth admin APIs
- Social login with email confirmation

You will need to set up a custom SMTP server to handle the delivery of messages to your users.

To get you started and let you explore and set up email message templates for your application, Supabase provides a simple SMTP server for all projects. This server imposes a few important restrictions and is not meant for production use.

Send messages only to pre-authorized addresses.

Unless you configure a custom SMTP server for your project, Supabase Auth will refuse to deliver messages to addresses that are not part of the project's team. You can manage this in theTeam tabof the organization's settings.

For example, if your project's organization has these member accountsperson-a@example.com,person-b@example.comandperson-c@example.comthen Supabase Auth will only send messages to these addresses. All other addresses will fail with the error messageEmail address not authorized.

Significant rate-limits that can change over time.

To maintain the health and reputation of the default SMTP sending service, the number of messages your project can send is limited and can change without notice. Currently this value is set to2messages per hour.

No SLA guarantee on message delivery or uptime for the default SMTP service.

The default SMTP service is provided as best-effort only and intended for the following non-production use cases:

- Exploring and getting started with Supabase Auth
- Setting up and testing email templates with the members of the project's team
- Building toy projects, demos or any non-mission-critical application

We urge all customers to set up custom SMTP server for all other use cases.


### How to set up a custom SMTP server?#
Supabase Auth works with any email sending service that supports the SMTP protocol. First you will need to choose a service, create an account (if you already do not have one) and obtain the SMTP server settings and credentials for your account. These include: the SMTP server host, port, user and password. You will also need to choose a default From address, usually something likeno-reply@example.com.

A non-exhaustive list of services that work with Supabase Auth is:

- Resend
- AWS SES
- Postmark
- Twilio SendGrid
- ZeptoMail
- Brevo

Once you've set up your account with an email sending service, head to theAuthentication settings pageto enable and configure custom SMTP.

You can also configure custom SMTP using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Configure custom SMTP6curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  -H "Content-Type: application/json" \9  -d '{10    "external_email_enabled": true,11    "mailer_secure_email_change_enabled": true,12    "mailer_autoconfirm": false,13    "smtp_admin_email": "no-reply@example.com",14    "smtp_host": "smtp.example.com",15    "smtp_port": 587,16    "smtp_user": "your-smtp-user",17    "smtp_pass": "your-smtp-password",18    "smtp_sender_name": "Your App Name"19  }'
```

Once you save these settings, your project's Auth server will send messages to all addresses. To protect the reputation of your newly set up service a low rate-limit of 30 messages per hour is imposed. To adjust this to an acceptable value for your use case head to theRate Limits configuration page.


### Dealing with abuse: How to maintain the sending reputation of your SMTP server?#
As you make your application known to the public and it grows in popularity, you can expect to see a few types of abuse that can negatively impact the reputation of your sending domain.

A common source of abuse is bots or attackers signing up users to your application.

They use lists of known email addresses to sign up users to your project with pre-determined passwords. These can vary in scale and intensity: sometimes the bots slowly send sign up requests over many months, or they send a lot of requests at once.

Usually the goal for this behavior is:

- To negatively affect your email sending reputation, after which they might ask for a ransom promising to stop the behavior.
- To cause a short-term or even long-term Denial of Service attack on your service, by preventing new account creation, signins with magic links or one-time passwords, or to severely impact important security flows in your application (such as reset password or forgot password).
- To force you to reduce the security posture of your project, such as by disabling email confirmations. At that point, they may target specific or a broad number of users by creating an account in their name. Then they can use social engineering techniques to trick them to use your application in such a way that both attacker and victim have access to the same account.

Mitigation strategies:

- Configure CAPTCHA protectionfor your project, which is the most effective way to control bots in this scenario. You can use CAPTCHA services which provide invisible challenges where real users won't be asked to solve puzzles most of the time.
- Prefer social login (OAuth) or SSO with SAML instead of email-based authentication flows in your apps.
- Prefer passwordless authentication (one-time password) as this limits the attacker's value to gain from this behavior.
- Do not disable email confirmations under pressure.


### Additional best practices#
Set up and maintain DKIM, DMARC and SPF configurations.

Work with your email sending service to configureDKIM, DMARC and SPFfor your sending domain. This will significantly increase the deliverability of your messages.

Set up a custom domain.

Authentication messages often contain links to your project's Auth server.Setting up a custom domainwill reduce the likelihood of your messages being picked up as spam due to another Supabase project's bad reputation.

Don't mix Auth emails with marketing emails.

Use separate services for Auth and marketing messages. If the reputation of one falls, it won't affect your whole application or operation.

This includes:

- Use a separate sending domain for authentication --auth.example.comand a separate domain for marketingmarketing.example.com.
- Use a separate From address --no-reply@auth.example.comvsno-reply@marketing.example.com.

Have another SMTP service set up on stand-by.

In case the primary SMTP service you're using is experiencing difficulty, or your account is under threat of being blocked due to spam, you have another service to quickly turn to.

Use consistent branding and focused content.

Make sure you've separated out authentication messages from marketing messages.

- Don't include promotional content as part of authentication messages.
- Avoid talking about what your application is inside authentication messages. This can be picked up by automated spam filters which will classify the message as marketing and increase its chances of being regarded as spam. This problem is especially apparent if your project is related to: Web3, Blockchain, AI, NFTs, Gambling, Pornography.
- Avoid taglines or other short-form marketing material in authentication messages.
- Reduce the number of links and call-to-actions in authentication messages.
- Change the authentication messages templates infrequently. Prefer a single big change over multiple smaller changes.
- Avoid A/B testing content in authentication messages.
- Use a separate base template (HTML) from your marketing messages.
- Avoid the use of email signatures in authentication messages. If you do, make sure the signatures are different in style and content from your marketing messages.
- Use short and to-the-point subject lines. Avoid or reduce the number of emojis in subjects.
- Reduce the number of images placed in authentication messages.
- Avoid including user-provided data such as names, usernames, email addresses or salutations in authentication messages. If you do, make sure they are sanitized.

Prepare for large surges ahead of time.

If you are planning on having a large surge of users coming at a specific time, work with your email sending service to adjust the rate limits and their expectations accordingly. Most email sending services dislike spikes in the number of messages being sent, and this may affect your sending reputation.

Consider implementing additional protections for such events:

- Build a queuing or waitlist system instead of allowing direct sign-up, which will help you control the number of messages being sent from the email sending service.
- Disable email-based sign ups for the event and use social login only. Alternatively you can deprioritize the email-based sign-up flows for the event by hiding them in the UI or making them harder to reach.

Use the Send Email Auth Hook for more control.

If you need more control over the sending process, instead of using a SMTP server you can use theSend Email Auth Hook. This can be useful in advanced scenarios such as:

- You want to use React or a different email templating engine.
- You want to use an email sending service that does not provide an SMTP service, or the non-SMTP API is more powerful.
- You want to queue up messages instead of sending them immediately, in an effort to smooth out spikes in email sending or do additional filtering (avoid repetitive messages).
- You want to use multiple email sending services to increase reliability (if primary service is unavailable, use backup service automatically).
- You want to use different email sending services based on the email address or user data (e.g. service A for users in the USA, service B for users in the EU, service C for users in China).
- You want to add or include additional email headers in messages, for tracking or other reasons.
- You want to add attachments to the messages (generally not recommended).
- You want to addS/MIME signaturesto messages.
- You want to use an email server not open to the Internet, such as some corporate or government mail servers.

Increase the duration of user sessions.

Having short liveduser sessionscan be problematic for email sending, as it forces active users to sign-in frequently, increasing the number of messages needed to be sent. Consider increasing the maximum duration of user sessions. If you do see an unnecessary increase in logins without a clear cause, check your frontend application for bugs.

If you are using aSSRframework on the frontend and are seeing an increased number of user logins without a clear cause, check your set up. Make sure to keep the@supabase/ssrpackage up to date and closely follow the guides we publish. Make sure that the middleware components of your SSR frontend works as intended and matches the guides we've published. Sometimes a misplacedreturnor conditional can cause early session termination.


================================================================================


# Sign in with Web3
Source: https://supabase.com/docs/guides/auth/auth-web3

Sign in with Web3


### Use your Web3 wallet to authenticate users with Supabase
Use your Web3 wallet to authenticate users with Supabase

Enable Sign In with Web3to allow users to sign in to your application using only their Web3 wallet.

Supported Web3 wallets:

- All Solana wallets
- All Ethereum wallets


### How does it work?#
Sign in with Web3 utilizes theEIP 4361standard to authenticate wallet addresses off-chain. This standard is widely supported by the Ethereum and Solana ecosystems, making it the best choice for verifying wallet ownership.

Authentication works by asking the Web3 wallet application to sign a predefined message with the user's wallet. This message is parsed both by the Web3 wallet application and Supabase Auth to verify its validity and purpose, before creating a user account or session.

An example of such a message is:

```javascript
1example.com wants you to sign in with your Ethereum account:20xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc234I accept the ExampleOrg Terms of Service: https://example.com/tos56URI: https://example.com/login7Version: 18Chain ID: 19Nonce: 3289175610Issued At: 2021-09-30T16:25:24Z11Resources:12- https://example.com/my-web2-claim.json
```

It defines the wallet address, timestamp, browser location where the sign-in occurred and includes a customizable statement (I accept...) which you can use to ask consent from the user.

Most Web3 wallets are able to recognize these messages and show a dedicated "Confirm Sign In" dialog validating and presenting the information in the message in a secure and responsible way to the user. Even if the wallet does not directly support these messages, it will use the message signature dialog instead.

Finally the Supabase Auth server validates both the message's contents and signature before issuing a validUser sessionto your application. Validation rules include:

- Message structure validation
- Cryptographic signature verification
- Timestamp validation, ensuring the signature was created within 10 minutes of the sign-in call
- URI and Domain validation, ensuring these match your server's definedRedirect URLs

The wallet address is used as the identity identifier, and in the identity data you can also find the statement and additional metadata.


### Enable the Web3 provider#
In the dashboard navigate to your project'sAuthentication Providerssection and enable the Web3 Wallet provider.

In the CLI add the following config to yoursupabase/config.tomlfile:

```javascript
1[auth.web3.solana]2enabled = true34[auth.web3.ethereum]5enabled = true
```


### Potential for abuse#
User accounts that sign in with their Web3 wallet will not have an email address or phone number associated with them. This can open your project to abuse as creating a Web3 wallet account is free and easy to automate and difficult to correlate with a real person's identity.

Control your project's exposure by configuring in the dashboard:

- Rate Limits for Web3
- Enable CAPTCHA protection

Or in the CLI:

```javascript
1[auth.rate_limit]2# Number of Web3 logins that can be made in a 5 minute interval per IP address.3web3 = 3045[auth.captcha]6enabled = true7provider = "hcaptcha" # or other supported providers8secret = "0x0000000000000000000000000000000000000000"
```

Many wallet applications will warn the user if the message sent for signing is not coming from the page they are currently visiting. To further prevent your Supabase project from receiving signed messages destined for other applications, you must register your application's URL using theRedirect URL settings.

For example if the user is signing in to the pagehttps://example.com/sign-inyou should add the following configurations in the Redirect URL settings:

- https://example.com/sign-in/(last slash is important)
- Alternatively set up a glob pattern such ashttps://example.com/**


### Sign in with Ethereum#
Ethereum defines thewindow.ethereumglobal scope objectthat your app uses to interact with Ethereum Wallets. Additionally there is awallet discovery mechanism (EIP-6963)that your app can use to discover all of the available wallets on the user's browser.

To sign in a user with their Ethereum wallet make sure that the user has installed a wallet application. There are two ways to do this:

Use the following code to sign in a user, implicitly relying on thewindow.ethereumglobal scope wallet API:

```javascript
1const { data, error } = await supabase.auth.signInWithWeb3({2  chain: 'ethereum',3  statement: 'I accept the Terms of Service at https://example.com/tos',4})
```


### Sign in with Solana#
Most Solana wallet applications expose their API via thewindow.solanaglobal scope object in your web application.

Supabase's JavaScript Client Library provides built-in support for this API.

To sign in a user make sure that:

Use the following code to authenticate a user:

```javascript
1const { data, error } = await supabase.auth.signInWithWeb3({2  chain: 'solana',3  statement: 'I accept the Terms of Service at https://example.com/tos',4})
```

Providing astatementis required for most Solana wallets and this message will be shown to the user on the consent dialog. It will also be added to the identity data for your users.

If you are using a non-standard Solana wallet that does not register thewindow.solanaobject, or your user has multiple Solana wallets attached to the page you can disambiguate by providing the wallet object like so:

- To useBrave Wallet with Solana:1const{data,error}=awaitsupabase.auth.signInWithWeb3({2chain:'solana',3statement:'I accept the Terms of Service at https://example.com/tos',4wallet:window.braveSolana,5})
- To usePhantom with Solana:1const{data,error}=awaitsupabase.auth.signInWithWeb3({2chain:'solana',3statement:'I accept the Terms of Service at https://example.com/tos',4wallet:window.phantom,5})

```javascript
1const { data, error } = await supabase.auth.signInWithWeb3({2  chain: 'solana',3  statement: 'I accept the Terms of Service at https://example.com/tos',4  wallet: window.braveSolana,5})
```

```javascript
1const { data, error } = await supabase.auth.signInWithWeb3({2  chain: 'solana',3  statement: 'I accept the Terms of Service at https://example.com/tos',4  wallet: window.phantom,5})
```


### Frequently asked questions#

### How to associate an email address, phone number or social login to a user signing in with Web3?#
Web3 wallets don't expose any identifying information about the user other than their wallet address (public key). This is why accounts that were created using Sign in with Web3 don't have any email address or phone number associated.

To associate an email address, phone number or other social login with their account you can use thesupabase.auth.updateUser()orsupabase.auth.linkIdentity()APIs.


================================================================================


# Error Codes
Source: https://supabase.com/docs/guides/auth/debugging/error-codes

Error Codes


### Learn about the Auth error codes and how to resolve them
Learn about the Auth error codes and how to resolve them


### Auth error codes#
Supabase Auth can return various errors when using its API. This guide explains how to handle these errors effectively across different programming languages.


### Error types#
Supabase Auth errors are generally categorized into two main types:

- API Errors: Originate from the Supabase Auth API.
- Client Errors: Originate from the client library's state.

Client errors differ by language so do refer to the appropriate section below:

All errors originating from thesupabase.authnamespace of the client library will be wrapped by theAuthErrorclass.

Error objects are split in a few classes:

- AuthApiError-- errors which originate from the Supabase Auth API.UseisAuthApiErrorinstead ofinstanceofchecks to see if an error you caught is of this type.
- CustomAuthError-- errors which generally originate from state in the client library.Use thenameproperty on the error to identify the class of error received.

- UseisAuthApiErrorinstead ofinstanceofchecks to see if an error you caught is of this type.

- Use thenameproperty on the error to identify the class of error received.

Errors originating from the server API classed asAuthApiErroralways have acodeproperty that can be used to identify the error returned by the server. Thestatusproperty is also present, encoding the HTTP status code received in the response.


### HTTP status codes#
Below are the most common HTTP status codes you might encounter, along with their meanings in the context of Supabase Auth:


### 403 Forbidden#
Sent out in rare situations where a certain Auth feature is not available for the user, and you as the developer are not checking a precondition whether that API is available for the user.


### 422 Unprocessable Entity#
Sent out when the API request is accepted, but cannot be processed because the user or Auth server is in a state where it cannot satisfy the request.


### 429 Too Many Requests#
Sent out when rate-limits are breached for an API. You should handle this status code often, especially in functions that authenticate a user.


### 500 Internal Server Error#
Indicate that the Auth server's service is degraded. Most often it points to issues in your database setup such as a misbehaving trigger on a schema, function, view or other database object.


### 501 Not Implemented#
Sent out when a feature is not enabled on the Auth server, and you are trying to use an API which requires it.


### Auth error codes table#
The following table provides a comprehensive list of error codes you may encounter when working with Supabase Auth. Each error code is associated with a specific issue and includes a description to help you understand and resolve the problem efficiently.

Anonymous sign-ins are disabled.

Returned from the PKCE flow where the provided code verifier does not match the expected one. Indicates a bug in the implementation of the client library.

Usually used when the HTTP body of the request is not valid JSON.

JWT sent in the Authorization header is not valid.

OAuth callback from provider to Auth does not have all the required attributes (state). Indicates an issue with the OAuth provider or client library implementation.

OAuth state (data echoed back by the OAuth provider to Supabase Auth) is not in the correct format. Indicates an issue with the OAuth provider integration.

CAPTCHA challenge could not be verified with the CAPTCHA provider. Check your CAPTCHA integration.

General database conflict, such as concurrent requests on resources that should not be modified concurrently. Can often occur when you have too many session refresh requests firing off at the same time for a user. Check your app for concurrency issues, and if detected, back off exponentially.

Example and test domains are currently not supported. Use a different email address.

Email sending is not allowed for this address as your project is using the default SMTP service. Emails can only be sent to members in your Supabase organization. If you want to send emails to others, set up a custom SMTP provider.

Learn more:

- Setting up a custom SMTP provider

Unlinking this identity causes the user's account to change to an email address which is already used by another user account. Indicates an issue where the user has two different accounts using different primary email addresses. You may need to migrate user data to one of their accounts in this case.

Email address already exists in the system.

Signing in is not allowed for this user as the email address is not confirmed.

Signups are disabled for email and password.

PKCE flow state to which the API request relates has expired. Ask the user to sign in again.

PKCE flow state to which the API request relates no longer exists. Flow states expire after a while and are progressively cleaned up, which can cause this error. Retried requests can cause this error, as the previous request likely destroyed the flow state. Ask the user to sign in again.

Payload from Auth does not have a valid Content-Type header.

Payload from Auth exceeds maximum size limit.

Unable to reach hook within maximum time allocated.

Unable to reach hook after maximum number of retries.

The identity to which the API relates is already linked to a user.

Identity to which the API call relates does not exist, such as when an identity is unlinked or deleted.

To call this API, the user must have a higher Authenticator Assurance Level. To resolve, ask the user to solve an MFA challenge.

Learn more:

- MFA

Login credentials or grant type not recognized.

Invite is expired or already used.

Calling the supabase.auth.linkUser() and related APIs is not enabled on the Auth server.

Responding to an MFA challenge should happen within a fixed time period. Request a new challenge when encountering this error.

MFA factors for a single user should not have the same friendly name.

MFA factor no longer exists.

The enrollment process for MFA factors must begin and end with the same IP address.

Enrollment of MFA Phone factors is disabled.

Login via Phone factors and verification of new Phone factors is disabled.

Enrollment of MFA TOTP factors is disabled.

Login via TOTP factors and verification of new TOTP factors is disabled.

MFA challenge could not be verified -- wrong TOTP code.

Further MFA verification is rejected. Only returned if the MFA verification attempt hook returns a reject decision.

Learn more:

- MFA verification hook

Verified phone factor already exists for a user. Unenroll existing verified phone factor to continue.

Enrollment of MFA Web Authn factors is disabled.

Login via WebAuthn factors and verification of new WebAuthn factors is disabled.

This HTTP request requires an Authorization header, which is not provided.

User accessing the API is not admin, i.e. the JWT does not contain a role claim that identifies them as an admin of the Auth server.

Using an OAuth provider which is disabled on the Auth server.

Sign in with OTPs (magic link, email OTP) is disabled. Check your server's configuration.

OTP code for this sign-in has expired. Ask the user to sign in again.

Too many emails have been sent to this email address. Ask the user to wait a while before trying again.

Too many requests have been sent by this client (IP address). Ask the user to try again in a few minutes. Sometimes can indicate a bug in your application that mistakenly sends out too many requests (such as a badly written useEffect React hook).

Learn more:

- React useEffect hook

Too many SMS messages have been sent to this phone number. Ask the user to wait a while before trying again.

Phone number already exists in the system.

Signing in is not allowed for this user as the phone number is not confirmed.

Signups are disabled for phone and password.

OAuth provider is disabled for use. Check your server's configuration.

Not all OAuth providers verify their user's email address. Supabase Auth requires emails to be verified, so this error is sent out when a verification email is sent after completing the OAuth flow.

A user needs to reauthenticate to change their password. Ask the user to reauthenticate by calling the supabase.auth.reauthenticate() API.

Verifying a reauthentication failed, the code is incorrect. Ask the user to enter a new code.

Refresh token has been revoked and falls outside the refresh token reuse interval. See the documentation on sessions for further information.

Learn more:

- Auth sessions

Session containing the refresh token not found.

Processing the request took too long. Retry the request.

A user that is updating their password must use a different password than the one currently used.

SAML assertion (user information) was received after sign in, but no email address was found in it, which is required. Check the provider's attribute mapping and/or configuration.

SAML assertion (user information) was received after sign in, but a user ID (called NameID) was not found in it, which is required. Check the SAML identity provider's configuration.

(Admin API.) Updating the SAML metadata for a SAML identity provider is not possible, as the entity ID in the update does not match the entity ID in the database. This is equivalent to creating a new identity provider, and you should do that instead.

(Admin API.) Adding a SAML identity provider that is already added.

SAML identity provider not found. Most often returned after IdP-initiated sign-in with an unregistered SAML identity provider in Supabase Auth.

(Admin API.) Adding or updating a SAML provider failed as its metadata could not be fetched from the provided URL.

Using Enterprise SSO with SAML 2.0 is not enabled on the Auth server.

Learn more:

- Enterprise SSO

SAML relay state is an object that tracks the progress of a supabase.auth.signInWithSSO() request. The SAML identity provider should respond after a fixed amount of time, after which this error is shown. Ask the user to sign in again.

SAML relay states are progressively cleaned up after they expire, which can cause this error. Ask the user to sign in again.

Session to which the API request relates has expired. This can occur if an inactivity timeout is configured, or the session entry has exceeded the configured timebox value. See the documentation on sessions for more information.

Learn more:

- Auth sessions

Session to which the API request relates no longer exists. This can occur if the user has signed out, or the session entry in the database was deleted in some other way.

Sign ups (new account creation) are disabled on the server.

Every user must have at least one identity attached to it, so deleting (unlinking) an identity is not allowed if it's the only one for the user.

Sending an SMS message failed. Check your SMS provider configuration.

(Admin API.) Only one SSO domain can be registered per SSO identity provider.

SSO provider not found. Check the arguments in supabase.auth.signInWithSSO().

A user can only have a fixed number of enrolled MFA factors.

(Deprecated feature not available via Supabase client libraries.) The request's X-JWT-AUD claim does not match the JWT's audience.

Auth service is degraded or a bug is present, without a specific reason.

User with this information (email address, phone number) cannot be created again as it already exists.

User to which the API request relates has a banned_until property which is still active. No further API requests should be attempted until this field is cleared.

User to which the API request relates no longer exists.

When a user comes from SSO, certain fields of the user cannot be updated (like email).

Provided parameters are not in the expected format.

User is signing up or changing their password without meeting the password strength criteria. Use the AuthWeakPasswordError class to access more information about what they need to do to make the password pass.


### Best practices for error handling#
- Always useerror.codeanderror.nameto identify errors, not string matching on error messages.
- Avoid relying solely on HTTP status codes, as they may change unexpectedly.


================================================================================


# Enterprise Single Sign-On
Source: https://supabase.com/docs/guides/auth/enterprise-sso

Enterprise Single Sign-On

Supabase Auth supports building enterprise applications that require Single Sign-On (SSO) authenticationwith SAML 2.0.


================================================================================


# Single Sign-On with SAML 2.0 for Projects
Source: https://supabase.com/docs/guides/auth/enterprise-sso/auth-sso-saml

Single Sign-On with SAML 2.0 for Projects

Looking for guides on how to use Single Sign-On with the Supabase dashboard? Head on over toEnable SSO for Your Organization.

Supabase Auth supports enterprise-level Single Sign-On (SSO) for any identity providers compatible with the SAML 2.0 protocol. This is a non-exclusive list of supported identity providers:

- Google Workspaces (formerly known as G Suite)
- Okta, Auth0
- Microsoft Active Directory, Azure Active Directory, Microsoft Entra
- PingIdentity
- OneLogin

If you're having issues with identity provider software not on this list,open a support ticket.


### Prerequisites#
This guide requires the use of theSupabase CLI. Make sure you're using version v1.46.4 or higher. You can usesupabase -vto see the currently installed version.
You can use thesupabase ssosubcommandsto manage your project's configuration.

SAML 2.0 support is disabled by default on Supabase projects. You can configure this on theAuth Providerspage on your project.

Note that SAML 2.0 support is offered on plans Pro and above. Check thePricingpage for more information.


### Terminology#
The number of SAML and SSO acronyms can often be overwhelming. Here's a glossary which you can refer back to at any time:

- Identity Provider,IdP, orIDPAn identity provider is a service that manages user accounts at a company or organization. It can verify the identity of a user and exchange that information with your Supabase project and other applications. It acts as a single source of truth for user identities and access rights. Commonly used identity providers are: Microsoft Active Directory (Azure AD, Microsoft Entra), Okta, Google Workspaces (G Suite), PingIdentity, OneLogin, and many others. There are also self-hosted and on-prem versions of identity providers, and sometimes they are accessible only by having access to a company VPN or being in a specific building.
- Service Provider,SPThis is the software that is asking for user information from an identity provider. In Supabase, this is your project's Auth server.
- AssertionAn assertion is a statement issued by an identity provider that contains information about a user.
- EntityIDA globally unique ID (usually a URL) that identifies an Identity Provider or Service Provider across the world.
- NameIDA unique ID (usually an email address) that identifies a user at an Identity Provider.
- MetadataAn XML document that describes the features and configuration of an Identity Provider or Service Provider. It can be as a standalone document or as a URL. Usually (but not always) theEntityIDis the URL at which you can access the Metadata.
- CertificateSupabase Auth (the Service Provider) trusts assertions from an Identity Provider based on the signature attached to the assertion. The signature is verified according to the certificate present in the Metadata.
- Assertion Consumer Service (ACS) URLThis is one of the most important SAML URLs. It is the URL where Supabase Auth will accept assertions from an identity provider. Basically, once the identity provider verifies the user's identity it will redirect to this URL and the redirect request will contain the assertion.
- Binding (Redirect, POST, or Artifact)This is a description of the way an identity provider communicates with Supabase Auth. When using the Redirect binding, the communication occurs using HTTP 301 redirects. When it'sPOST, it's usingPOSTrequests sent with<form>elements on a page. When using Artifact, it's using a more secure exchange over a Redirect orPOST.
- RelayStateState used by Supabase Auth to hold information about a request to verify the identity of a user.


### Important SAML 2.0 information#
Below is information about your project's SAML 2.0 configuration which you can share with the company or organization that you're trying to on-board.

Note that SLO (Single Logout) is not supported at this time with Supabase Auth as it is a rarely supported feature by identity providers. However, the URL is registered and advertised for when this does become available. SLO is a best-effort service, so we recommend consideringSession Timebox or Session Inactivity Timeoutinstead to force your end-users to authenticate regularly.

Append?download=trueto the Metadata URL to download the Metadata XML file. This is useful in cases where the identity provider requires a file.

Alternatively, you can use thesupabase sso info --project-ref <your-project>commandto get setup information for your project.


### User accounts and identities#
User accounts and identities created via SSO differ from regular (email, phone, password, social login...) accounts in these ways:

- No automatic linking.Each user account verified using a SSO identity provider will not be automatically linked to existing user accounts in the system. That is, if a uservalid.email@supabase.iohad signed up with a password, and then uses their company SSO login with your project, there will be twovalid.email@supabase.iouser accounts in the system.
- Emails are not necessarily unique.Given the behavior with no automatic linking, email addresses are no longer a unique identifier for a user account. Always use the user's UUID to correctly reference user accounts.
- Sessions may have a maximum duration.Depending on the configuration of the identity provider, a login session established with SSO may forcibly log out a user after a certain period of time.


### Row Level Security#
You can use information about the SSO identity provider in Row Level Security policies.

Here are some commonly used statements to extract SSO related information from the user's JWT:

- auth.jwt()#>>'{amr,0,method}'Returns the name of the last method used to verify the identity of this user. With SAML SSO this issso/saml.
- auth.jwt()#>>'{amr,0,provider}'Returns the UUID of the SSO identity provider used by the user to sign-in.
- auth.jwt()#>>'{user_metadata,iss}'Returns the identity provider's SAML 2.0EntityID

If you useMulti-Factor Authenticationwith SSO, theamrarray may have a different method at index0!

A common use case with SSO is to use the UUID of the identity provider as the identifier for the organization the user belongs to -- frequently known as a tenant. By associating the identity provider's UUID with your tenants, you can use restrictive RLS policies to scope down actions and data that a user is able to access.

For example, let's say you have a table like:

```javascript
1create table organization_settings (2  -- the organization's unique ID3  id uuid not null primary key,4  -- the organization's SSO identity provider5  sso_provider_id uuid unique,6  -- name of the organization7  name text,8  -- billing plan (paid, Free, Enterprise)9  billing_plan text10);
```

You can use the information present in the user's JWT to scope down which rows from this table the user can see, without doing any additional user management:

```javascript
1CREATE POLICY "View organization settings."2  ON organization_settings3  AS RESTRICTIVE4  USING (5    sso_provider_id = (select auth.jwt()#>>'{amr,0,provider}')6  );
```


### Managing SAML 2.0 connections#
Once you've enabled SAML 2.0 support on your project via theAuth Providerspage in the dashboard, you can use theSupabase CLIto add, update, remove and view information about identity providers.


### Add a connection#
To establish a connection to a SAML 2.0 Identity Provider (IdP) you will need:

- A SAML 2.0 Metadata XML file, or a SAML 2.0 Metadata URL pointing to an XML file
- (Optional) Email domains that the organization's IdP uses
- (Optional) Attribute mappings between the user properties of the IdP and the claims stored by Supabase Auth

You should obtain the SAML 2.0 Metadata XML file or URL from the organization whose IdP you wish to connect. Most SAML 2.0 Identity Providers support the Metadata URL standard, and we recommend using a URL if this is available.

Commonly used SAML 2.0 Identity Providers that support Metadata URLs:

- Okta
- Azure AD (Microsoft Entra)
- PingIdentity

Commonly used SAML 2.0 Identity Providers that only support Metadata XML files:

- Google Workspaces (G Suite)
- Any self-hosted or on-prem identity provider behind a VPN

Once you've obtained the SAML 2.0 Metadata XML file or URL you canestablish a connectionwith your project's Supabase Auth server by running:

```javascript
1supabase sso add --type saml --project-ref <your-project> \2  --metadata-url 'https://company.com/idp/saml/metadata' \3  --domains company.com
```

If you wish to use a Metadata XML file instead, you can use:

```javascript
1supabase sso add --type saml --project-ref <your-project> \2  --metadata-file /path/to/saml/metadata.xml \3  --domains company.com
```

This command will register a new identity provider with your project's Auth server. When successful, you will see the details of the provider such as it's SAML information and registered domains.

Note that only persons with write access to the project can register, update or remove identity providers.

Once you've added an identity provider, users who have access to it can sign in to your application. With SAML 2.0 there are two ways that users can sign in to your project:

- By signing-in from your application's user interface, commonly known asSP (Service Provider) Initiated Flow
- By clicking on an icon in the application menu on the company intranet or identity provider page, commonly known asIdentity Provider Initiated (IdP) Flow

To initiate a sign-in request from your application's user interface (i.e. the SP Initiated Flow), you can use:

```javascript
1..({2  : 'company.com',3})
```

CallingsignInWithSSOstarts the sign-in process using the identity provider registered for thecompany.comdomain name. It is not required that identity providers be assigned one or multiple domain names, in which case you can use the provider's unique ID instead.


### Understanding attribute mappings#
When a user signs in using the SAML 2.0 Single Sign-On protocol, an XML document called the SAML Assertion is exchanged between the identity provider and Supabase Auth.

This assertion contains information about the user's identity and other authentication information, such as:

- Unique ID of the user (calledNameIDin SAML)
- Email address
- Name of the user
- Department or organization
- Other attributes present in the users directory managed by the identity provider

With exception of the unique user ID, SAML does not require any other attributes in the assertion. Identity providers can be configured so that only select user information is shared with your project.

Your project can be configured to recognize these attributes and map them into your project's database using a JSON structure. This process is called attribute mapping, and varies according to the configuration of the identity provider.

For example, the following JSON structure configures attribute mapping for theemailandfirst_nameuser identity properties.

```javascript
1{2  "keys": {3    "email": {4      "name": "mail"5    },6    "first_name": {7      "name": "givenName"8    }9  }10}
```

When creating or updating an identity provider with theSupabase CLIyou can include this JSON as a file with the--attribute-mapping-file /path/to/attribute/mapping.jsonflag.

For example, to change the attribute mappings to an existing provider you can use:

```javascript
1supabase sso update <provider-uuid> --project-ref <your-project> \2  --attribute-mapping-file /path/to/attribute/mapping.json
```

Given a SAML 2.0 assertion that includes these attributes:

```javascript
1<saml:AttributeStatement>2  <!-- will be mapped to the email key -->3  <saml:Attribute4    Name="mail"5    NameFormat="urn:oasis:names:tc:SAML:2.0:attrname-format:basic"6    >7    <saml:AttributeValue xsi:type="xs:string">8      valid.email@supabase.io9    </saml:AttributeValue>10  </saml:Attribute>1112  <!-- will be mapped to the first_name key -->13  <saml:Attribute14    Name="givenName"15    NameFormat="urn:oasis:names:tc:SAML:2.0:attrname-format:basic"16    >17    <saml:AttributeValue xsi:type="xs:string">18      Jane Doe19    </saml:AttributeValue>20  </saml:Attribute>21</saml:AttributeStatement>
```

Will result in the following claims in the user's identity in the database and JWT:

```javascript
1{2  "email": "valid.email@supabase.io",3  "custom_claims": {4    "first_name": "Jane Doe"5  }6}
```

Supabase Auth does not require specifying attribute mappings if you only need access to the user's email. It will attempt to find an email attribute specified in the assertion. All other properties will not be automatically included, and it is those you need to map.

At this time it is not possible to have users without an email address, so SAML assertions without one will be rejected.

Most SAML 2.0 identity providers use Lightweight Directory Access Protocol (LDAP) attribute names. However, due to their variability and complexity operators of identity providers are able to customize both theNameand attribute value that is sent to Supabase Auth in an assertion. Refer to the identity provider's documentation and contact the operator for details on what attributes are mapped for your project.

Accessing the stored attributes

The stored attributes, once mapped, show up in the access token (a JWT) of the user. If you need to look these values up in the database, you can find them in theauth.identitiestable under theidentity_dataJSON column. Identities created for SSO providers havesso:<uuid-of-provider>in theprovidercolumn, whileidcontains the uniqueNameIDof the user account.

Furthermore, you can find the same identity data underraw_user_meta_datainsideauth.users.

Advanced attribute mapping

If the SAML assertion contains multiple values for a key, such as the groups that the user has access to, only the first one will be picked up. To change this behavior, mark the key as an array. For example this attribute mapping:

```javascript
1{2  "keys": {3    "groups": {4      "name": "groups",5      "array": true6    }7  }8}
```

Will result in the following claims:

```javascript
1{2  "groups": ["group-a", "group-b", "group-c"]3}
```

You can also specify a default value for a key that may be missing in the SAML assertion.

```javascript
1{2  "keys": {3    "custom_claim": {4      "name": "custom_claim",5      "default": 1236    }7  }8}
```

Some SAML Assertions may expose the same attribute under different names for different users. In this case instead of specifying a single name to look up the value, you can specify multiple names. These will be looked at in-order until a value is found:

```javascript
1{2  "keys": {3    "custom_claim": {4      "names": ["first-look-for-this-attribute", "then-this-one"]5    }6  }7}
```


### Remove a connection#
Once a connection to an identity provider is established, you canremove itby running:

```javascript
1supabase sso remove <provider-id> --project-ref <your-project>
```

If successful, the details of the removed identity provider will be shown. All user accounts from that identity provider will be immediately logged out. User information will remain in the system, but it will no longer be possible for any of those accounts to be accessed in the future, even if you add the connection again.

Alist of allregistered identity providers can be displayed by running:

```javascript
1supabase sso list --project-ref <your-project>
```


### Update a connection#
You may wish to update settings about a connection to a SAML 2.0 identity provider.

Commonly this is necessary when:

- Cryptographic keys are rotated or have expired
- Metadata URL has changed, but is the same identity provider
- Other SAML 2.0 Metadata attributes have changed, but it is still the same identity provider
- You are updating the domains or attribute mapping

You can use this command toupdatethe configuration of an identity provider:

```javascript
1supabase sso update <provider-id> --project-ref <your-project>
```

Use--helpto see all available flags.

It is not possible to change the unique SAML identifier of the identity provider, known asEntityID. Everything else can be updated. If the SAMLEntityIDof your identity provider has changed, it is regarded as a new identity provider and you will have to register it like a new connection.


### Retrieving information about a connection#
You can always obtain alistof all registered providers using:

```javascript
1supabase sso list --project-ref <your-project>
```

This list will only include basic information about each provider. To seeall of the informationabout a provider you can use:

```javascript
1supabase sso show <provider-id> --project-ref <your-project>
```

You can use the-o jsonflag to output the information as JSON, should you need to. Other formats may be supported, use--helpto see all available options.


### Pricing#
$0.015per SSO MAU. You are only charged for usage exceeding your subscription plan's
quota.

For a detailed breakdown of how charges are calculated, refer toManage Monthly Active SSO Users usage.


### Frequently asked questions#

### Publishing your application to an identity provider's marketplace#
Many cloud-based identity providers offer a marketplace where you can register your application for easy on-boarding with customers. When you use Supabase Auth's SAML 2.0 support you can register your project in any one of these marketplaces.

Refer to the relevant documentation for each cloud-based identity provider on how you can do this. Some common marketplaces are:

- Okta Integration Network
- Azure Active Directory App Gallery
- Google Workspaces Pre-integrated SAML apps catalog


### Why do some users get: SAML assertion does not contain email address?#
Identity providers do not have to send back and email address for the user, though they often do. Supabase Auth requires that an email address is present.

The following list of commonly used SAML attribute names is inspected, in order of appearance, to discover the email address in the assertion:

- urn:oid:0.9.2342.19200300.100.1.3
- http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress
- http://schemas.xmlsoap.org/claims/EmailAddress
- mail
- email

Finally if there is no such attribute, it will use the SAMLNameIDvalue but only if the format is advertised asurn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress.

Should you run into this problem, it is most likely a misconfiguration issueon the identity provider side.Instruct your contact at the company to map the user's email address to one of the above listed attribute names, typicallyemail.


### Accessing the private key used for SAML in your project#
At this time it is not possible to extract the RSA private key used by your project's Supabase Auth server. This is done to keep the private key as secure as possible, given that SAML does not offer an easy way to rotate keys without disrupting service. (Use a SAML 2.0 Metadata URL whenever possible for this reason!)

If you really need access to the key,open a support ticketand we'll try to support you as best as possible.


### Is multi-tenant SSO with SAML supported?#
Yes, Supabase supports multi-tenant Single Sign-On (SSO) using SAML 2.0. While the dashboard displays only one SAML field, you can set up multiple SAML connections using the Supabase CLI.
Each connection is assigned a uniquesso_provider_id, which is included in the user's JWT and can be used in Row Level Security (RLS) policies. You can configure custom attribute mappings for each connection to include tenant-specific information, such as roles.
This setup allows you to implement multi-tenant SSO for multiple clients or organizations within a single application. For example, if you have an app with multiple clients using different Azure Active Directories, you can create separate SAML connections for each and use thesso_provider_idto manage access and apply appropriate security policies.


### Is multi-subdomain SSO with SAML supported?#
Yes, also referred to ascross-origin authentication within the same site. To redirect to a URL other than theSite URL, following the SAML response from the IdP, theredirectTooption can be added tosignInWithSSO.

```javascript
1const { ,  } = await ..({2  : 'company.com',3  : {4    : `https://app.company.com/callback`,5  },6})
```

When redirecting to a URL other than the Site URL, a/callbackendpoint is necessary to process the auth code from the IdP and exchange it for a session. This assumes theSupabase SSR clienthas already been configured.

```javascript
1import { error, redirect } from '@sveltejs/kit'2import type { RequestHandler } from './$types'34export const GET: RequestHandler = async ({ url, locals }) => {5  const code = url.searchParams.get('code')67  if (!code) {8    error(400, 'No authorization code provided')9  }1011  const { error: tokenExchangeError } = await locals.supabase.auth.exchangeCodeForSession(code)1213  if (tokenExchangeError) {14    error(400, 'Failed to exchange authorization code for session')15  }1617  redirect(303, '/')18}
```


### Why doesn't IdP-initiated SAML flow work with PKCE, and what's the alternative?#
Traditional IdP-initiated SAML flows aren't compatible with PKCE (Proof Key for Code Exchange) because PKCE requires acode_challengeandcode_verifierthat are generated when your application initiates the authentication flow. In IdP-initiated flows, Supabase receives an unsolicited response without this information, causing the code exchange step to fail.

To achieve the same user experience while maintaining PKCE security, you can implement a "bookmark app" approach:

Create an endpoint in your application (for example,https://your-app.com/auth/saml-init) that initiates the SAML flow usingsignInWithSSO. Then create a bookmark or linked application in your IdP that points to this endpoint. When users access the bookmark app, it triggers a secure SP-initiated flow.

This approach supports custom SAML assertions and lets you embed the link anywhere in your application.


================================================================================


# General configuration
Source: https://supabase.com/docs/guides/auth/general-configuration

General configuration


### General configuration options for Supabase Auth
General configuration options for Supabase Auth

This section covers thegeneral configuration optionsfor Supabase Auth. If you are looking for another type of configuration, you may be interested in one of the following sections:

- Policiesto manage Row Level Security policies for your tables.
- Sign In / Providersto configure authentication providers and login methods for your users.
- Third Party Authto use third-party authentication (TPA) systems based on JWTs to access your project.
- Sessionsto configure settings for user sessions and refresh tokens.
- Rate limitsto safeguard against bursts of incoming traffic to prevent abuse and maximize stability.
- Email Templatesto configure what emails your users receive.
- Custom SMTPto configure how emails are sent.
- Multi-Factorto require users to provide additional verification factors to authenticate.
- URL Configurationto configure site URL and redirect URLs for authentication. Read morein the redirect URLs documentation.
- Attack Protectionto configure security settings to protect your project from attacks.
- Auth Hooks (BETA)to use Postgres functions or HTTP endpoints to customize the behavior of Supabase Auth to meet your needs.
- Audit Logs (BETA)to track and monitor auth events in your project.
- Performanceto configure and optimize authentication server settings.

Supabase Auth provides thesegeneral configuration optionsto control user access to your application:

- Allow new users to sign up: Users will be able to sign up. If this config is disabled, only existing users can sign in.
- Confirm Email: Users will need to confirm their email address before signing in for the first time.HavingConfirm Emaildisabled assumes that the user's email does not need to be verified in order to login and implicitly confirms the user's email in the database.This option can be found in the email provider under the provider-specific configuration.
- Allow anonymous sign-ins: Allow anonymous users to be created.
- Allow manual linking: Allow users to link their accounts manually.

Allow new users to sign up: Users will be able to sign up. If this config is disabled, only existing users can sign in.

Confirm Email: Users will need to confirm their email address before signing in for the first time.

- HavingConfirm Emaildisabled assumes that the user's email does not need to be verified in order to login and implicitly confirms the user's email in the database.
- This option can be found in the email provider under the provider-specific configuration.

Allow anonymous sign-ins: Allow anonymous users to be created.

Allow manual linking: Allow users to link their accounts manually.


================================================================================


# Identities
Source: https://supabase.com/docs/guides/auth/identities

Identities

An identity is an authentication method associated with a user. Supabase Auth supports the following types of identity:

- Email
- Phone
- OAuth
- SAML

A user can have more than one identity. Anonymous users have no identity until they link an identity to their user.


### The user identity object#
The user identity object contains the following attributes:


================================================================================


# JWT Claims Reference
Source: https://supabase.com/docs/guides/auth/jwt-fields

JWT Claims Reference


### Complete reference for claims appearing in JWTs created by Supabase Auth
Complete reference for claims appearing in JWTs created by Supabase Auth

This page provides a comprehensive reference for all JWT claims used in Supabase authentication tokens. This information is essential for server-side JWT validation and serialization, especially when implementing authentication in languages like Rust where field names likerefare reserved keywords.


### JWT structure overview#
Supabase JWTs follow the standard JWT structure with three parts:

- Header: Contains algorithm and key information
- Payload: Contains the claims (user data and metadata)
- Signature: Cryptographic signature for verification

The payload contains various claims that provide user identity, authentication level, and authorization information.


### Required claims#
These claims are always present in Supabase JWTs and cannot be removed:


### Optional claims#
These claims may be present depending on the authentication context:


### Special claims#

### Field value constraints#

### Authenticator assurance level (aal)#

### Role values (role)#

### Audience values (aud)#

### Authentication methods (amr.method)#

### JWT examples#

### Authenticated user token#
```javascript
1{2  "aal": "aal1",3  "amr": [4    {5      "method": "password",6      "timestamp": 16409916007    }8  ],9  "app_metadata": {10    "provider": "email",11    "providers": ["email"]12  },13  "aud": "authenticated",14  "email": "user@example.com",15  "exp": 1640995200,16  "iat": 1640991600,17  "iss": "https://abcdefghijklmnopqrst.supabase.co/auth/v1",18  "phone": "",19  "role": "authenticated",20  "session_id": "123e4567-e89b-12d3-a456-426614174000",21  "sub": "123e4567-e89b-12d3-a456-426614174000",22  "user_metadata": {23    "name": "John Doe"24  },25  "is_anonymous": false26}
```


### Anonymous user token#
```javascript
1{2  "iss": "supabase",3  "ref": "abcdefghijklmnopqrst",4  "role": "anon",5  "iat": 1640991600,6  "exp": 16409952007}
```


### Service role token#
```javascript
1{2  "iss": "supabase",3  "ref": "abcdefghijklmnopqrst",4  "role": "service_role",5  "iat": 1640991600,6  "exp": 16409952007}
```


### Language-Specific considerations#

### Rust#
In Rust, thereffield is a reserved keyword. When deserializing JWTs, you'll need to handle this:

```javascript
1use serde::{Deserialize, Serialize};23#[derive(Debug, Deserialize, Serialize)]4struct JwtClaims {5    iss: String,6    #[serde(rename = "ref")] // Handle reserved keyword7    project_ref: Option<String>,8    role: String,9    iat: i64,10    exp: i64,11    // ... other claims12}
```


### TypeScript/JavaScript#
```javascript
1interface  {2  : string3  : string | string[]4  : number5  : number6  : string7  : string8  : 'aal1' | 'aal2'9  : string10  : string11  : string12  : boolean13  ?: string14  ?: number15  ?: <string, any>16  ?: <string, any>17  ?: <{18    : string19    : number20  }>21  ?: string // Only in anon/service role tokens22}
```


### Python#
```javascript
1from typing import Optional, Union, List, Dict, Any2from dataclasses import dataclass34@dataclass5class AmrEntry:6    method: str7    timestamp: int89@dataclass10class JwtClaims:11    iss: str12    aud: Union[str, List[str]]13    exp: int14    iat: int15    sub: str16    role: str17    aal: str18    session_id: str19    email: str20    phone: str21    is_anonymous: bool22    jti: Optional[str] = None23    nbf: Optional[int] = None24    app_metadata: Optional[Dict[str, Any]] = None25    user_metadata: Optional[Dict[str, Any]] = None26    amr: Optional[List[AmrEntry]] = None27    ref: Optional[str] = None  # Only in anon/service role tokens
```


### Go#
```javascript
1type AmrEntry struct {2    Method    string `json:"method"`3    Timestamp int64  `json:"timestamp"`4}56type JwtClaims struct {7    Iss         string                 `json:"iss"`8    Aud         interface{}            `json:"aud"` // string or []string9    Exp         int64                  `json:"exp"`10    Iat         int64                  `json:"iat"`11    Sub         string                 `json:"sub"`12    Role        string                 `json:"role"`13    Aal         string                 `json:"aal"`14    SessionID   string                 `json:"session_id"`15    Email       string                 `json:"email"`16    Phone       string                 `json:"phone"`17    IsAnonymous bool                   `json:"is_anonymous"`18    Jti         *string                `json:"jti,omitempty"`19    Nbf         *int64                 `json:"nbf,omitempty"`20    AppMetadata map[string]interface{} `json:"app_metadata,omitempty"`21    UserMetadata map[string]interface{} `json:"user_metadata,omitempty"`22    Amr         []AmrEntry             `json:"amr,omitempty"`23    Ref         *string                `json:"ref,omitempty"` // Only in anon/service role tokens24}
```


### Validation guidelines#
When implementing JWT validation on your server:


### Security considerations#
- Always validate the JWT signaturebefore trusting any claims
- Never expose service role tokensto client-side code
- Validate all claimsbefore trusting the JWT
- Check token expirationon every request
- Use HTTPSfor all JWT transmission
- Rotate JWT secretsregularly
- Implement proper error handlingfor invalid tokens


### Related documentation#
- JWT Overview
- Custom Access Token Hooks
- Row Level Security
- Server-Side Auth


================================================================================


# JSON Web Token (JWT)
Source: https://supabase.com/docs/guides/auth/jwts

JSON Web Token (JWT)


### Information on how best to use JSON Web Tokens with Supabase
Information on how best to use JSON Web Tokens with Supabase

AJSON Web Tokenis a type of data structure, represented as a string, that usually contains identity and authorization information about a user. It encodes information about its lifetime and is signed with a cryptographic key to make it tamper-resistant.

Supabase Auth continuously issues a new JWT for each user session, for as long as the user remains signed in. Check the comprehensive guide onSessionsto find out how you can tailor this process for your needs.

JWTs provide the foundation forRow Level Security. Each Supabase product is able to securely decode and verify the validity of a JWT it receives before using Postgres policies and roles to authorize access to the project's data.

Supabase provides a comprehensive system of managingJWT Signing Keysused to create and verify JSON Web Tokens.


### Introduction#
JWTs are strings that have the following structure:

```javascript
1<header>.<payload>.<signature>
```

Each part is a string ofBase64-URLencoded JSON, or bytes for the signature.

Header

```javascript
1{2  "typ": "JWT",3  "alg": "<HS256 | ES256 | RS256>",4  "kid": "<unique key identifier>"5}
```

Gives some basic identifying information about the string, indicating its typetyp, the cryptographic algorithmalgthat can be used to verify the data, and optionally the unique key identifier that should be used when verifying it.

Payload

```javascript
1{2  "iss": "https://project_id.supabase.co/auth/v1",3  "exp": 12345678,4  "sub": "<user ID>",5  "role": "authenticated",6  "email": "someone@example.com",7  "phone": "+15552368"8  // ...9}
```

Provides identifying information (called "claims") about the user (or other entity) that is represented by the token. Usually a JWT conveys information about what the user can access (then called Access Token) or who the user is (then called ID Token). You can use aCustom Access Token Hookto add, remove or change claims present in the token. A few claims are important:

Signature

Adigital signatureusing ashared secretorpublic-key cryptography. The purpose of the signature is to verify the authenticity of the<header>.<payload>string without relying on database access, liveness or performance of the Auth server. To verify the signature avoid implementing the algorithms yourself and instead rely onsupabase.auth.getClaims(), or other high-quality JWT verification libraries for your language.


### Supabase and JWTs#
Supabase creates JWTs in these cases for you:

In addition to creating JWTs, Supabase can also accept JWTs from other Auth servers via theThird-Party Authfeature or ones you've made yourself using the legacy JWT secret or if you've imported inJWT Signing Key.


### Using custom or third-party JWTs#
Thesupabase.auth.getClaims()method is meant to be used only with JWTs issued by Supabase Auth. If you make your own JWTs using the legacy JWT secret or a key you've imported, the verification may fail. We strongly recommend using a JWT verification library for your language to verify this type of JWT based on the claims you're adding in them.

Your Supabase project accepts a JWT in theAuthorization: Bearer <jwt>header. If you're using the Supabase client library, it does this for you.

If you are already using Supabase Auth, when a user is signed in, their access token JWT is automatically managed and sent for you with every API call.

If you wish to send a JWT from a Third-Party Auth provider, or one you made yourself by using the legacy JWT secret or a JWT signing key you imported, you can pass it to the client library using theaccessTokenoption.

```javascript
1import {  } from '@supabase/supabase-js'23const  = (4  'https://<supabase-project>.supabase.co',5  'SUPABASE_PUBLISHABLE_KEY',6  {7    : async () => {8      return '<your JWT here>'9    },10  }11)
```

In the past there was a recommendation to set custom headers on the Supabase client with theAuthorizationheader including your custom JWT. This is no longer recommended as it's less flexible and causes confusion when combined with a user session from Supabase Auth.


### Verifying a JWT from Supabase#
If you're not able to use the Supabase client libraries, the following can be used to help you securely verify JWTs issued by Supabase.

Supabase Auth exposes aJSON Web KeySet URL for each Supabase project:

```javascript
1GET https://project-id.supabase.co/auth/v1/.well-known/jwks.json
```

Which responds with JWKS object containing one or more asymmetricJWT signing keys(only their public keys). Be aware that this endpoint does not return any keys if you are not using asymmetric JWT signing keys.

```javascript
1{2  "keys": [3    {4      "kid": "<match with kid from JWT header>",5      "alg": "<match with alg from JWT header>",6      "kty": "<RSA|EC|OKP>",7      "key_ops": ["verify"]8      // public key fields9    }10  ]11}
```

This endpoint is served directly from the Auth server, but is also additionally cached by the Supabase Edge for 10 minutes, significantly speeding up access to this data regardless of where you're performing the verification. It's important to be aware of the cache expiry time to prevent unintentionally rejecting valid user access tokens. We recommend waiting at least 20 minutes when creating a standby signing key, or revoking a previously used key.

Make sure that you do not cache this data for longer in your application, as it might make revocation difficult. If you do, make sure to provide a way to purge this cache when rotating signing keys to avoid unintentionally rejecting valid user access tokens.

Below is an example of how to use thejose TypeScript JWT verification librarywith Supabase JWTs:

```javascript
1import { jwtVerify, createRemoteJWKSet } from 'jose'23const PROJECT_JWKS = createRemoteJWKSet(4  new URL('https://project-id.supabase.co/auth/v1/.well-known/jwks.json')5)67/**8 * Verifies the provided JWT against the project's JSON Web Key Set.9 */10async function verifyProjectJWT(jwt: string) {11  return jwtVerify(jwt, PROJECT_JWKS)12}
```


### Verifying with the legacy JWT secret or a shared secret signing key#
If your project is still using the legacy JWT secret, or you're using a shared secret (HS256) signing key, we recommend always verifying a user access token directly with the Auth server by sending a request like so:

```javascript
1GET https://project-id.supabase.co/auth/v1/user2apikey: publishable or anon legacy API key3Authorization: Bearer <JWT>
```

If the server responds with HTTP 200 OK, the JWT is valid, otherwise it is not.

Because the Auth server runs only in your project's specified region and is not globally distributed, doing this check can be quite slow depending on where you're performing the check. Avoid doing checks like this from servers or functions running on the edge, and prefer routing to a server within the same geographical region as your project.

If you are using the legacy JWT secret, or you've imported your own shared secret (HS256) signing key, you may wish to verify using the shared secret.We strongly recommend against this approach.

There is almost no benefit from using a JWT signed with a shared secret. Although it's computationally more efficient and verification is simpler to code by hand, using this approach can expose your project's data to significant security vulnerabilities or weaknesses.

Consider the following:

- Using a shared secret can make it more difficult to keep aligned with security compliance frameworks such as SOC2, PCI-DSS, ISO27000, HIPAA, etc.
- A shared secret that is in the hands of a malicious actor can be used to impersonate your users, give them access to privileged actions or data.
- It is difficult to detect or identify when or how a shared secret has been given to a malicious actor.
- Consider who might have even accidental access to the shared secret: systems, staff, devices (and their disk encryption and vulnerability patch status).
- A malicious actor can use a shared secretfar into the future, so lacking current evidence of compromise does not mean your data is secure.
- It can be very easy to accidentally leak the shared secret in publicly available source code such as in your website or frontend, mobile app package or other executable. This is especially true if you accidentally add the secret in environment variables prefixed withNEXT_PUBLIC_,VITE_,PUBLIC_or other conventions by web frameworks.
- Rotating shared secrets might require careful coordination to avoid downtime of your app.

Check the JWT verification libraries for your language on how to securely verify JWTs signed with the legacy JWT secret or a shared secret (HS256) signing key. We strongly recommend relying on the Auth server as described above, or switching to a different signing key based on public key cryptography (RSA, Elliptic Curves) instead.


### Resources#
- JWT debugger:https://jwt.io/
- JWT Signing Keys
- JWT Claims Reference- Complete reference for all JWT claims used by Supabase Auth
- API keys


================================================================================


# User Management
Source: https://supabase.com/docs/guides/auth/managing-user-data

User Management


### View, delete, and export user information.
View, delete, and export user information.

You can view your users on theUsers pageof the Dashboard. You can also view the contents of the Auth schema in theTable Editor.


### Accessing user data via API#
For security, the Auth schema is not exposed in the auto-generated API. If you want to access users data via the API, you can create your own user tables in thepublicschema.

Make sure to protect the table by enablingRow Level Security. Reference theauth.userstable to ensure data integrity. Specifyon delete cascadein the reference. For example, apublic.profilestable might look like this:

```javascript
1create table public.profiles (2  id uuid not null references auth.users on delete cascade,3  first_name text,4  last_name text,56  primary key (id)7);89alter table public.profiles enable row level security;
```

Only use primary keys asforeign key referencesfor schemas and tables likeauth.userswhich are managed by Supabase. Postgres lets you specify a foreign key reference for columns backed by a unique index (not necessarily primary keys).

Primary keys areguaranteed not to change. Columns, indices, constraints or other database objects managed by Supabasemay change at any timeand you should be careful when referencing them directly.

To update yourpublic.profilestable every time a user signs up, set up a trigger. If the trigger fails, it could block signups, so test your code thoroughly.

```javascript
1-- inserts a row into public.profiles2create function public.handle_new_user()3returns trigger4language plpgsql5security definer set search_path = ''6as $$7begin8  insert into public.profiles (id, first_name, last_name)9  values (new.id, new.raw_user_meta_data ->> 'first_name', new.raw_user_meta_data ->> 'last_name');10  return new;11end;12$$;1314-- trigger the function every time a user is created15create trigger on_auth_user_created16  after insert on auth.users17  for each row execute procedure public.handle_new_user();
```


### Adding and retrieving user metadata#
You can assign metadata to users on sign up:

```javascript
1const { ,  } = await ..({2  : 'valid.email@supabase.io',3  : 'example-password',4  : {5    : {6      : 'John',7      : 27,8    },9  },10})
```

User metadata is stored on theraw_user_meta_datacolumn of theauth.userstable. To view the metadata:

```javascript
1const {2  : {  },3} = await ..()4let  = ?.
```


### Deleting users#
You may delete users directly or via the management console at Authentication > Users. Note that deleting a user from theauth.userstable does not automatically sign out a user. As Supabase makes use of JSON Web Tokens (JWT), a user's JWT will remain "valid" until it has expired.

You cannot delete a user if they are the owner of any objects in Supabase Storage.

You will encounter an error when you try to delete an Auth user that owns any Storage objects. If this happens, try deleting all the objects for that user, or reassign ownership to another user.


### Exporting users#
As Supabase is built on top of Postgres, you can query theauth.usersandauth.identitiestable via theSQL Editortab to extract all users:

```javascript
1select * from auth.users;
```

You can then export the results as CSV.


================================================================================


# OAuth 2.1 Server
Source: https://supabase.com/docs/guides/auth/oauth-server

OAuth 2.1 Server

Supabase Auth can act as an OAuth 2.1 and OpenID Connect (OIDC) identity provider. This allows other applications and services to use your Supabase project as their authentication provider, just like "Sign in with Google" or "Sign in with GitHub".

You can use this to build "Sign in with [Your App]" experiences, authenticate AI agents through the Model Context Protocol (MCP), power developer platforms with third-party integrations, or implement standards-compliant enterprise SSO.


### Use cases#
There are several reasons why you might want to enable OAuth 2.1 Server in your Supabase project:

- Developer platforms and marketplaces: Allow third-party developers to build integrations and apps for your platform. Partners can offer "Sign in with [Your App]" to their users, with your control over data access through Row Level Security policies.
- AI agents and automation: Authenticate AI agents, LLM tools, and MCP servers that need to access user data. The Model Context Protocol provides automatic OAuth discovery and client registration for AI applications.
- Mobile and desktop apps: Issue OAuth tokens to your own mobile apps, desktop applications, or other first-party clients. All tokens respect your existing Row Level Security policies and work with Custom Access Token Hooks.
- Enterprise SSO: Provide OpenID Connect (OIDC) authentication for enterprise customers who need standards-compliant identity federation across multiple services.

Developer platforms and marketplaces: Allow third-party developers to build integrations and apps for your platform. Partners can offer "Sign in with [Your App]" to their users, with your control over data access through Row Level Security policies.

AI agents and automation: Authenticate AI agents, LLM tools, and MCP servers that need to access user data. The Model Context Protocol provides automatic OAuth discovery and client registration for AI applications.

Mobile and desktop apps: Issue OAuth tokens to your own mobile apps, desktop applications, or other first-party clients. All tokens respect your existing Row Level Security policies and work with Custom Access Token Hooks.

Enterprise SSO: Provide OpenID Connect (OIDC) authentication for enterprise customers who need standards-compliant identity federation across multiple services.


### Overview#
Supabase Auth implements the OAuth 2.1 authorization code flow with PKCE (Proof Key for Code Exchange). When a third-party application wants to access user data:

Access tokens are standard Supabase JWTs that includeuser_id,role, andclient_idclaims. Your existing Row Level Security policies automatically apply to OAuth tokens, giving you fine-grained control over what each client can access.


### Supported standards#
- OAuth 2.1: Latest OAuth specification with mandatory PKCE
- OpenID Connect: ID tokens (withopenidscope), UserInfo endpoint, and OIDC discovery
- Standard scopes:openid,email,profile, andphonescopes for controlling data access
- Dynamic client registration: Automatic registration for MCP-compatible clients
- JWKS endpoint: Public keys for third parties to validate tokens


### Integration with existing auth#
OAuth 2.1 Server works seamlessly with your existing Supabase Auth configuration:

- Users can authenticate using any enabled method (password, magic link, social providers, MFA, phone)
- Custom Access Token Hooksapply to OAuth tokens, allowing you to customize claims likeaudienceor add client-specific permissions
- Row Level Security policies control data access using theclient_idclaim in tokens
- All standard Supabase features (email templates, hooks, rate limiting) continue to work


### Set up OAuth 2.1 server#
To enable OAuth 2.1 Server in your project, follow these guides:

Getting Started

OAuth Flows

MCP Authentication

Token Security & RLS


### Resources#
- GitHub Discussion- Share your use cases and help shape the roadmap
- Discord Community- Get help and share what you're building


================================================================================


# Getting Started with OAuth 2.1 Server
Source: https://supabase.com/docs/guides/auth/oauth-server/getting-started

Getting Started with OAuth 2.1 Server

This guide will walk you through setting up your Supabase project as an OAuth 2.1 identity provider, from enabling the feature to registering your first client application.


### Prerequisites#
Before you begin, make sure you have:

- A Supabase project (create one atsupabase.com)
- Admin access to your project
- (Optional)Supabase CLIv2.54.11 or higher for local development


### Overview#
Setting up OAuth 2.1 in your Supabase project involves these steps:

Testing OAuth flows is often easier on a Supabase project since it's already accessible on the web, no tunnel or additional configuration needed.


### Enable OAuth 2.1 server#
OAuth 2.1 server is currently in beta and free to use during the beta period on all Supabase plans.

Once enabled, your project will expose the necessary OAuth endpoints:

Use asymmetric JWT signing keys for better security

By default, Supabase uses HS256 (symmetric) for signing JWTs. For OAuth use cases, we recommend migrating to asymmetric algorithms like RS256 or ES256. Asymmetric keys are more scalable and secure because:

- OAuth clients can validate JWTs using the public key from your JWKS endpoint
- No need to share your JWT secret with third-party applications
- More resilient architecture for distributed systems

Learn more aboutconfiguring JWT signing keys.

Note:If you plan to use OpenID Connect ID tokens (by requesting theopenidscope), asymmetric signing algorithms arerequired. ID token generation will fail with HS256.


### Configure your authorization path#
Before registering clients, you need to configure where your authorization UI will live.

The authorization path is combined with your Site URL (configured inAuthentication>URL Configuration) to create the full authorization endpoint URL.

Your authorization UI will be at the combined Site URL + Authorization Path. For example:

- Site URL:https://example.com(fromAuthentication>URL Configuration)
- Authorization Path:/oauth/consent(fromOAuth Serversettings)
- Your authorization UI:https://example.com/oauth/consent

When OAuth clients initiate the authorization flow, Supabase Auth will redirect users to this URL with anauthorization_idquery parameter. You'll useSupabase JavaScript library OAuth methodsto handle the authorization:

- supabase.auth.oauth.getAuthorizationDetails(authorization_id)- Retrieve client and authorization details
- supabase.auth.oauth.approveAuthorization(authorization_id)- Approve the authorization request
- supabase.auth.oauth.denyAuthorization(authorization_id)- Deny the authorization request


### Build your authorization UI#
This is where you build thefrontendfor your authorization flow. When third-party apps initiate OAuth, users will be redirected to your authorization path (configured in the previous step) with anauthorization_idquery parameter.

Your authorization UI should:

The authorization details include ascopefield (singular) containing a space-separated string of scopes requested by the client (e.g.,"openid email profile"). You should display these scopes to the user so they understand what information will be shared.

This is afrontend implementation. You're building the UI that displays the consent screen and handles user interactions. The actual OAuth token generation is handled by Supabase Auth after you call the approve/deny methods.


### Example authorization UI#
Here's how to build a minimal authorization page at your configured path (e.g.,/oauth/consent):

```javascript
1// app/oauth/consent/page.tsx2import { createServerClient } from '@supabase/ssr'3import { cookies } from 'next/headers'4import { redirect } from 'next/navigation'56export default async function ConsentPage({7  searchParams,8}: {9  searchParams: { authorization_id?: string }10}) {11  const authorizationId = (await searchParams).authorization_id1213  if (!authorizationId) {14    return <div>Error: Missing authorization_id</div>15  }1617  const supabase = createServerClient(18    process.env.NEXT_PUBLIC_SUPABASE_URL!,19    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,20    {21      cookies: {22        getAll: async () => (await cookies()).getAll(),23        setAll: async (cookiesToSet) => {24          const cookieStore = await cookies()25          cookiesToSet.forEach(({ name, value, options }) => cookieStore.set(name, value, options))26        },27      },28    }29  )3031  // Check if user is authenticated32  const {33    data: { user },34  } = await supabase.auth.getUser()3536  if (!user) {37    // Redirect to login, preserving authorization_id38    redirect(`/login?redirect=/oauth/consent?authorization_id=${authorizationId}`)39  }4041  // Get authorization details using the authorization_id42  const { data: authDetails, error } =43    await supabase.auth.oauth.getAuthorizationDetails(authorizationId)4445  if (error || !authDetails) {46    return <div>Error: {error?.message || 'Invalid authorization request'}</div>47  }4849  return (50    <div>51      <h1>Authorize {authDetails.client.name}</h1>52      <p>This application wants to access your account.</p>5354      <div>55        <p>56          <strong>Client:</strong> {authDetails.client.name}57        </p>58        <p>59          <strong>Redirect URI:</strong> {authDetails.redirect_uri}60        </p>61        {authDetails.scope && authDetails.scope.trim() && (62          <div>63            <strong>Requested permissions:</strong>64            <ul>65              {authDetails.scope.split(' ').map((scopeItem) => (66                <li key={scopeItem}>{scopeItem}</li>67              ))}68            </ul>69          </div>70        )}71      </div>7273      <form action="/api/oauth/decision" method="POST">74        <input type="hidden" name="authorization_id" value={authorizationId} />75        <button type="submit" name="decision" value="approve">76          Approve77        </button>78        <button type="submit" name="decision" value="deny">79          Deny80        </button>81      </form>82    </div>83  )84}
```

```javascript
1// app/api/oauth/decision/route.ts2import { createServerClient } from '@supabase/ssr'3import { cookies } from 'next/headers'4import { NextResponse } from 'next/server'56export async function POST(request: Request) {7  const formData = await request.formData()8  const decision = formData.get('decision')9  const authorizationId = formData.get('authorization_id') as string1011  if (!authorizationId) {12    return NextResponse.json({ error: 'Missing authorization_id' }, { status: 400 })13  }1415  const supabase = createServerClient(16    process.env.NEXT_PUBLIC_SUPABASE_URL!,17    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,18    {19      cookies: {20        getAll: async () => (await cookies()).getAll(),21        setAll: async (cookiesToSet) => {22          const cookieStore = await cookies()23          cookiesToSet.forEach(({ name, value, options }) => cookieStore.set(name, value, options))24        },25      },26    }27  )2829  if (decision === 'approve') {30    const { data, error } = await supabase.auth.oauth.approveAuthorization(authorizationId)3132    if (error) {33      return NextResponse.json({ error: error.message }, { status: 400 })34    }3536    // Redirect back to the client with authorization code37    return NextResponse.redirect(data.redirect_to)38  } else {39    const { data, error } = await supabase.auth.oauth.denyAuthorization(authorizationId)4041    if (error) {42      return NextResponse.json({ error: error.message }, { status: 400 })43    }4445    // Redirect back to the client with error46    return NextResponse.redirect(data.redirect_to)47  }48}
```


### How it works#
- Callsupabase.auth.oauth.approveAuthorization(authorization_id)ordenyAuthorization(authorization_id)
- These methods handle all OAuth logic internally (generating authorization codes, etc.)
- They return aredirect_toURL


### Register an OAuth client#
Before third-party applications can use your project as an identity provider, you need to register them as OAuth clients.

- Client name: A friendly name for your application
- Redirect URIs: One or more URLs where users will be redirected after authorization
- Client type: Choose between:Public- For mobile and single-page apps (no client secret)Confidential- For server-side apps (includes client secret)

- Public- For mobile and single-page apps (no client secret)
- Confidential- For server-side apps (includes client secret)

You'll receive:

- Client ID: A unique identifier for the client
- Client Secret(for confidential clients): A secret key for authenticating the client

Store the client secret securely. It will only be shown once. If you lose it, you can regenerate a new one from theOAuth Appspage.


### Customizing tokens (optional)#
By default, OAuth access tokens include standard claims likeuser_id,role, andclient_id. If you need to customize tokens—for example, to set a specificaudienceclaim for third-party validation or add client-specific metadata—useCustom Access Token Hooks.

Custom Access Token Hooks are triggered for all token issuance, including OAuth flows. You can use theclient_idparameter to customize tokens based on which OAuth client is requesting them.


### Common use cases#
- Customizeaudienceclaim: Set theaudclaim to the third-party API endpoint for proper JWT validation
- Add client-specific permissions: Include custom claims based on which OAuth client is requesting access
- Implement dynamic scopes: Add metadata that RLS policies can use for fine-grained access control

For more examples, seeToken Security & RLS.


### Redirect URI configuration#
Redirect URIs are critical for OAuth security. Supabase Auth will only redirect to URIs that are explicitly registered with the client.

Not to be confused with general redirect URLs

This section is aboutOAuth client redirect URIs- where to send users after they authorize third-party apps to access your Supabase project. This is different from the generalRedirect URLssetting, which controls where to send users after they sign in TO your app using social providers.

Exact matches only - No wildcards or patterns

OAuth client redirect URIs require exact, complete URL matches. Unlike general redirect URLs (which support wildcards), OAuth client redirect URIs do NOT support wildcards, patterns, or partial URLs. You must register the full, exact callback URL.


### Best practices#
- Use HTTPS in production- Always use HTTPS for redirect URIs in production
- Register exact, complete URLs- Each redirect URI must be the full URL including protocol, domain, path, and port if needed
- Use separate OAuth clients per environment- Create separate OAuth clients for development, staging, and production. This provides better security isolation, allows independent secret rotation, and improves auditability. If you need to use the same client across environments, you can register multiple redirect URIs, but separate clients are recommended.


### Next steps#
Now that you've registered your first OAuth client, you're ready to:

- Understand OAuth flows- Learn how the authorization code and refresh token flows work
- Implement MCP authentication- Enable AI agent authentication
- Secure with RLS- Control data access for OAuth clients


================================================================================


# Model Context Protocol (MCP) Authentication
Source: https://supabase.com/docs/guides/auth/oauth-server/mcp-authentication

Model Context Protocol (MCP) Authentication

The Model Context Protocol (MCP) is an open standard for connecting AI agents and LLM tools to data sources and services. While Supabase doesn't provide MCP server functionality, you can build your own MCP servers that connect to your Supabase project and leverage Supabase Auth's OAuth 2.1 capabilities to authenticate AI agents using your existing user base.


### Why use Supabase Auth for MCP?#
When building MCP servers that connect to your Supabase project, you can leverage your existing Supabase Auth infrastructure to authenticate AI agents:

- Use your existing user base- No need to create separate authentication systems; AI agents authenticate as your existing users
- Standards-compliant OAuth 2.1- Full implementation with PKCE that MCP clients expect
- Automatic discovery- MCP clients auto-configure using Supabase's discovery endpoints
- Dynamic client registration- MCP clients can register themselves automatically with your project
- Row Level Security- Your existing RLS policies automatically apply to MCP clients
- User authorization- Users explicitly approve AI agent access through your authorization flow
- Token management- Automatic refresh token rotation and expiration handled by Supabase


### How MCP authentication works#
When you build an MCP server that connects to your Supabase project, authentication flows through Supabase Auth:

By leveraging Supabase Auth, your MCP server can authenticate AI agents using your existing user accounts without building a separate authentication system.


### Prerequisites#
Before setting up MCP authentication:

- Enable OAuth 2.1 serverin your Supabase project
- Build anauthorization endpoint
- (Optional) Enable dynamic client registration


### Setting up your MCP server#
Configure your MCP server to use your Supabase Auth server:

```javascript
1https://<project-ref>.supabase.co/auth/v1
```

Replace<project-ref>with your project reference ID from the Supabase dashboard.

MCP clients will automatically discover your OAuth configuration from:

```javascript
1https://<project-ref>.supabase.co/.well-known/oauth-authorization-server/auth/v1
```


### OAuth client setup#
Depending on your MCP server implementation, you have two options:

- Pre-register an OAuth client- Manually register your client by following theRegister an OAuth clientguide and use the client credentials in your MCP server
- Dynamic client registration- Enable this inAuthentication>OAuth Serverin your Supabase dashboard to allow MCP clients to register themselves automatically without manual intervention

Dynamic registration allows any MCP client to register with your project. Consider:

- Requiring user approval for all clients
- Monitoring registered clients regularly
- Validating redirect URIs are from trusted domains


### Building an MCP server with Supabase Auth#
When building your own MCP server, integrate with Supabase Auth to authenticate AI agents as your existing users and leverage your RLS policies.

Looking for an easier way to build MCP servers?

FastMCPprovides a streamlined way to build MCP servers with built-in Supabase Auth integration. FastMCP handles OAuth configuration, token management, and authentication flows automatically, letting you focus on building your AI agent's functionality. Check out theirSupabase integration guideto get started quickly.


### Handling MCP tokens in your application#
When your MCP server makes requests to your Supabase APIs on behalf of authenticated users, it will send access tokens issued by Supabase Auth, just like any other OAuth client.


### Validating MCP tokens#
Use the same token validation as other OAuth clients.

SeeToken Security & RLSfor more examples.


### Security considerations#

### User approval#
Always require explicit user approval for MCP clients:

- Show clear information about what the AI agent can access
- Display the client name and description
- List the scopes being requested
- Provide an option to deny access
- Allow users to revoke access later


### Troubleshooting#

### MCP client can't discover OAuth configuration#
Problem: Client shows "OAuth discovery failed" or similar error.

Solutions:

- Verify OAuth 2.1 is enabled in your project
- Check that/.well-known/oauth-authorization-serverreturns valid JSON
- Ensure your project URL is accessible


### Dynamic registration fails#
Problem: Client receives 403 or 404 on registration endpoint.

Solutions:

- Enable dynamic client registration in project settings
- Verify redirect URIs are valid, complete URLs (protocol, domain, path, and port)
- Check for rate limiting on registration endpoint


### Token exchange fails#
Problem: Client receives "invalid_grant" error.

Solutions:

- Verify authorization code hasn't expired (10 minutes)
- Ensure code verifier matches code challenge
- Check that redirect URI exactly matches registration
- Confirm client_id is correct


### RLS policies block MCP access#
Problem: MCP client can't access data despite valid token.

Solutions:

- Check RLS policies include the MCP client'sclient_id
- Verify user has necessary permissions
- Test with service role key to isolate RLS issues
- ReviewToken Security guide


### Next steps#
- Secure with RLS- Create granular policies for MCP clients
- OAuth flows- Deep dive into OAuth implementation
- MCP Specification- Official MCP documentation


================================================================================


# OAuth 2.1 Flows
Source: https://supabase.com/docs/guides/auth/oauth-server/oauth-flows

OAuth 2.1 Flows

Supabase Auth implements OAuth 2.1 with OpenID Connect (OIDC), supporting the authorization code flow with PKCE and refresh token flow. This guide explains how these flows work in detail.

This guide explains the OAuth 2.1 flows forthird-party client applicationsthat authenticate with your Supabase project. These flows require custom implementation and are not available in the@supabase/supabase-jslibrary. Thesupabase-jslibrary is for authenticatingwithSupabase Auth as an identity provider, not for building your own OAuth server.


### Supported grant types#
Supabase Auth supports two OAuth 2.1 grant types:

Other grant types likeclient_credentialsorpasswordare not supported.


### Authorization code flow with PKCE#
The authorization code flow with PKCE (Proof Key for Code Exchange) is the recommended flow for all OAuth clients, including single-page applications, mobile apps, and server-side applications.


### How it works#
The flow consists of several steps:


### Flow diagram#
Here's a visual representation of the complete authorization code flow:

```javascript
1┌─────────────┐              ┌──────────────────┐              ┌──────────────────┐2│             │              │                  │              │                  │3│   Client    │              │   Your Auth UI   │              │  Supabase Auth   │4│     App     │              │   (Frontend)     │              │                  │5│             │              │                  │              │                  │6└──────┬──────┘              └────────┬─────────┘              └────────┬─────────┘7       │                              │                                 │8       │  1. Generate PKCE params                                       │9       │     (code_verifier, code_challenge)                            │10       │                              │                                 │11       │  2. Redirect to /oauth/authorize with code_challenge           │12       ├────────────────────────────────────────────────────────────────>│13       │                              │                                 │14       │                              │  3. Validate params & redirect  │15       │                              │     to authorization_path       │16       │                              │<────────────────────────────────┤17       │                              │                                 │18       │                              │  4. getAuthorizationDetails()   │19       │                              ├────────────────────────────────>│20       │                              │  Return client info             │21       │                              │<────────────────────────────────┤22       │                              │                                 │23       │                              │  5. User login & consent        │24       │                              │                                 │25       │                              │  6. approveAuthorization()      │26       │                              ├────────────────────────────────>│27       │                              │  Return redirect_to with code   │28       │                              │<────────────────────────────────┤29       │                              │                                 │30       │  7. Redirect to client callback with code                      │31       │<───────────────────────────────────────────────────────────────┤32       │                              │                                 │33       │  8. Exchange code for tokens (POST /oauth/token)               │34       │     with code_verifier                                         │35       ├────────────────────────────────────────────────────────────────>│36       │                              │                                 │37       │  9. Return tokens (access, refresh, ID)                        │38       │<────────────────────────────────────────────────────────────────┤39       │                              │                                 │40       │  10. Access resources with access_token                        │41       │                              │                                 │42       │  11. Refresh tokens (POST /oauth/token with refresh_token)     │43       ├────────────────────────────────────────────────────────────────>│44       │                              │                                 │45       │  12. Return new tokens                                         │46       │<────────────────────────────────────────────────────────────────┤47       │                              │                                 │
```

Key points:

- Third-party client redirects user toSupabase Auth's authorize endpoint(not directly to your UI)
- Supabase Auth validates OAuth parameters and redirects toyour authorization path
- Your frontend UI handles authentication and consent usingsupabase-jsOAuth methods
- Supabase Auth handles all backend OAuth logic (code generation, token issuance)


### Step 1: Generate PKCE parameters#
Before initiating the flow, the client must generate PKCE parameters:

```javascript
1// Generate a random code verifier (43-128 characters)2function generateCodeVerifier() {3  const array = new Uint8Array(32)4  crypto.getRandomValues(array)5  return base64URLEncode(array)6}78// Create code challenge from verifier9async function generateCodeChallenge(verifier) {10  const encoder = new TextEncoder()11  const data = encoder.encode(verifier)12  const hash = await crypto.subtle.digest('SHA-256', data)13  return base64URLEncode(new Uint8Array(hash))14}1516function base64URLEncode(buffer) {17  return btoa(String.fromCharCode(...buffer))18    .replace(/\+/g, '-')19    .replace(/\//g, '_')20    .replace(/=/g, '')21}2223// Generate and store verifier (you'll need it later)24const codeVerifier = generateCodeVerifier()25sessionStorage.setItem('code_verifier', codeVerifier)2627// Generate challenge to send in authorization request28const codeChallenge = await generateCodeChallenge(codeVerifier)
```


### Step 2: Authorization request#
The client redirects the user to your authorization endpoint with the following parameters:

```javascript
1https://<project-ref>.supabase.co/auth/v1/oauth/authorize?2  response_type=code3  &client_id=<client-id>4  &redirect_uri=<configured-redirect-uri>5  &state=<random-state>6  &code_challenge=<code-challenge>7  &code_challenge_method=S256
```


### Required parameters#

### Optional parameters#
Always include astateparameter to protect against CSRF attacks. Generate a random string, store it in session storage, and verify it matches when the user returns.


### Step 3: User authentication and consent#
After receiving the authorization request, Supabase Auth validates the OAuth parameters (client_id, redirect_uri, PKCE, etc.) and then redirects the user to your configuredauthorization path(e.g.,https://example.com/oauth/consent?authorization_id=<id>).

The URL will contain anauthorization_idquery parameter that identifies this authorization request.

Your frontend application at the authorization path should:

- Callsupabase.auth.oauth.approveAuthorization(authorization_id)to approve
- Callsupabase.auth.oauth.denyAuthorization(authorization_id)to deny
- Redirect user to the returnedredirect_toURL

This is afrontend implementationusingsupabase-js. Supabase Auth handles all the backend OAuth logic (generating authorization codes, validating requests, etc.) after you call the approve/deny methods.

See theGetting Started guidefor complete implementation examples.


### Step 4: Authorization code issued#
If the user approves access, Supabase Auth redirects back to the client's redirect URI with an authorization code:

```javascript
1https://client-app.com/callback?2  code=<authorization-code>3  &state=<state-from-request>
```

The authorization code is:

- Short-lived- Valid for 10 minutes
- Single-use- Can only be exchanged once
- Bound to PKCE- Can only be exchanged with the correct code verifier

If the user denies access, Supabase Auth redirects with error information in query parameters:

```javascript
1https://client-app.com/callback?2  error=access_denied3  &error_description=The+user+denied+the+authorization+request4  &state=<state-from-request>
```

The error parameters allow clients to display relevant error messages to users:


### Step 5: Token exchange#
The client exchanges the authorization code for tokens by making a POST request to the token endpoint:

```javascript
1curl -X POST 'https://<project-ref>.supabase.co/auth/v1/oauth/token' \2  -H 'Content-Type: application/x-www-form-urlencoded' \3  -d 'grant_type=authorization_code' \4  -d 'code=<authorization-code>' \5  -d 'client_id=<client-id>' \6  -d 'redirect_uri=<redirect-uri>' \7  -d 'code_verifier=<code-verifier>'
```

For confidential clients (with client secret):

```javascript
1curl -X POST 'https://<project-ref>.supabase.co/auth/v1/oauth/token' \2  -H 'Content-Type: application/x-www-form-urlencoded' \3  -d 'grant_type=authorization_code' \4  -d 'code=<authorization-code>' \5  -d 'client_id=<client-id>' \6  -d 'client_secret=<client-secret>' \7  -d 'redirect_uri=<redirect-uri>' \8  -d 'code_verifier=<code-verifier>'
```


### Example in JavaScript#
```javascript
1// Retrieve the code verifier from storage2const codeVerifier = sessionStorage.getItem('code_verifier')34// Exchange code for tokens5const response = await fetch(`https://<project-ref>.supabase.co/auth/v1/oauth/token`, {6  method: 'POST',7  headers: {8    'Content-Type': 'application/x-www-form-urlencoded',9  },10  body: new URLSearchParams({11    grant_type: 'authorization_code',12    code: authorizationCode,13    client_id: '<client-id>',14    redirect_uri: '<redirect-uri>',15    code_verifier: codeVerifier,16  }),17})1819const tokens = await response.json()
```


### Step 6: Token response#
On success, Supabase Auth returns a JSON response with tokens:

```javascript
1{2  "access_token": "eyJhbGc...",3  "token_type": "bearer",4  "expires_in": 3600,5  "refresh_token": "MXff...",6  "scope": "openid email profile",7  "id_token": "eyJhbGc..."8}
```


### Access token structure#
Access tokens are JWTs containing standard Supabase claims plus OAuth-specific claims:

```javascript
1{2  "aud": "authenticated",3  "exp": 1735819200,4  "iat": 1735815600,5  "iss": "https://<project-ref>.supabase.co/auth/v1",6  "sub": "user-uuid",7  "email": "user@example.com",8  "phone": "",9  "app_metadata": {10    "provider": "email",11    "providers": ["email"]12  },13  "user_metadata": {},14  "role": "authenticated",15  "aal": "aal1",16  "amr": [17    {18      "method": "password",19      "timestamp": 173581560020    }21  ],22  "session_id": "session-uuid",23  "client_id": "9a8b7c6d-5e4f-3a2b-1c0d-9e8f7a6b5c4d"24}
```


### OAuth-specific claims#
All other claims follow the standardSupabase JWT structure.


### Available scopes#
The following scopes are currently supported:

Default scope:When no scope is specified in the authorization request, the default scope isemail.

Scopes affect what information is included in ID tokens and returned by the UserInfo endpoint. All OAuth access tokens have full access to user data (same as regular session tokens), with the addition of theclient_idclaim. Use Row Level Security policies with theclient_idclaim to control which data each OAuth client can access.

Custom scopes are not currently supported.Only the standard scopes listed above are available. Support for custom scopes is planned for a future release, which will allow you to define application-specific permissions and fine-grained access control.


### Refresh token flow#
Refresh tokens allow clients to obtain new access tokens without requiring the user to re-authenticate.


### When to refresh#
Clients should refresh access tokens when:

- The access token is expired (check theexpclaim)
- The access token is about to expire (proactive refresh)
- An API call returns a 401 Unauthorized error


### Refresh request#
Make a POST request to the token endpoint with the refresh token:

```javascript
1curl -X POST 'https://<project-ref>.supabase.co/auth/v1/oauth/token' \2  -H 'Content-Type: application/x-www-form-urlencoded' \3  -d 'grant_type=refresh_token' \4  -d 'refresh_token=<refresh-token>' \5  -d 'client_id=<client-id>'
```

For confidential clients:

```javascript
1curl -X POST 'https://<project-ref>.supabase.co/auth/v1/oauth/token' \2  -H 'Content-Type: application/x-www-form-urlencoded' \3  -d 'grant_type=refresh_token' \4  -d 'refresh_token=<refresh-token>' \5  -d 'client_id=<client-id>' \6  -d 'client_secret=<client-secret>'
```


### Example in JavaScript#
```javascript
1async function refreshAccessToken(refreshToken) {2  const response = await fetch(`https://<project-ref>.supabase.co/auth/v1/oauth/token`, {3    method: 'POST',4    headers: {5      'Content-Type': 'application/x-www-form-urlencoded',6    },7    body: new URLSearchParams({8      grant_type: 'refresh_token',9      refresh_token: refreshToken,10      client_id: '<client-id>',11    }),12  })1314  if (!response.ok) {15    throw new Error('Failed to refresh token')16  }1718  return await response.json()19}
```


### Refresh response#
The response contains a new access token and optionally a new refresh token:

```javascript
1{2  "access_token": "eyJhbGc...",3  "token_type": "bearer",4  "expires_in": 3600,5  "refresh_token": "v1.MXff...",6  "scope": "openid email profile"7}
```

Refresh tokens may be rotated (a new refresh token is issued). Always update your stored refresh token when a new one is provided.


### OpenID Connect (OIDC)#
Supabase Auth supports OpenID Connect, an identity layer on top of OAuth 2.1.

ID tokens are only included when theopenidscope is requested.To receive an ID token, includeopenidin the space-separated list of scopes in your authorization request. ID tokens are valid for 1 hour.


### ID tokens#
ID tokens are JWTs that contain user identity information. They are signed by Supabase Auth and can be verified by clients.

The claims included in the ID token depend on the scopes requested during authorization. For example, requestingopenid email profilewill include email and profile-related claims, while requesting onlyopenid emailwill include only email-related claims.


### Example ID token#
```javascript
1{2  "iss": "https://<project-ref>.supabase.co/auth/v1",3  "sub": "user-uuid",4  "aud": "client-id",5  "exp": 1735819200,6  "iat": 1735815600,7  "auth_time": 1735815600,8  "nonce": "random-nonce-from-request",9  "email": "user@example.com",10  "email_verified": true,11  "phone_number": "+1234567890",12  "phone_number_verified": false,13  "name": "John Doe",14  "picture": "https://example.com/avatar.jpg"15}
```


### Standard OIDC claims#

### UserInfo endpoint#
Clients can retrieve user information by calling the UserInfo endpoint with an access token:

```javascript
1curl 'https://<project-ref>.supabase.co/auth/v1/oauth/userinfo' \2  -H 'Authorization: Bearer <access-token>'
```

The information returned depends on the scopes granted in the access token. For example:

Withemailscope:

```javascript
1{2  "sub": "user-uuid",3  "email": "user@example.com",4  "email_verified": true5}
```

Withemail profile phonescopes:

```javascript
1{2  "sub": "user-uuid",3  "email": "user@example.com",4  "email_verified": true,5  "phone_number": "+1234567890",6  "phone_number_verified": false,7  "name": "John Doe",8  "picture": "https://example.com/avatar.jpg"9}
```


### OIDC discovery#
Supabase Auth exposes OpenID Connect and OAuth 2.1 discovery endpoints that describe its capabilities:

```javascript
1https://<project-ref>.supabase.co/auth/v1/.well-known/openid-configuration2https://<project-ref>.supabase.co/auth/v1/.well-known/oauth-authorization-server
```

Both endpoints return the same metadata and can be used interchangeably. They are provided for compatibility with different OAuth and OIDC clients that may expect one or the other.

These endpoints return metadata about:

- Available endpoints (authorization, token, userinfo, JWKS)
- Supported grant types and response types
- Supported scopes and claims
- Token signing algorithms

This enables automatic integration with OIDC-compliant libraries and tools.


### Token validation#
Third-party clients should validate access tokens to ensure they're authentic and not tampered with.

Recommended: Use asymmetric JWT signing keys

For OAuth implementations, we strongly recommend using asymmetric signing algorithms (RS256 or ES256) instead of the default HS256. With asymmetric keys, third-party clients can validate JWTs using the public key from your JWKS endpoint without needing access to your JWT secret. This is more secure, scalable, and follows OAuth best practices.

Learn how toconfigure asymmetric JWT signing keysin your project.

ID tokens require asymmetric signing algorithms

If you request theopenidscope to receive ID tokens, your project must be configured to use asymmetric signing algorithms (RS256 or ES256). ID token generation will fail with an error if your project is still using the default HS256 symmetric algorithm. This is a security requirement of the OpenID Connect specification.


### JWKS endpoint#
Supabase Auth exposes a JSON Web Key Set (JWKS) endpoint containing public keys for token verification:

```javascript
1https://<project-ref>.supabase.co/auth/v1/.well-known/jwks.json
```

Example response:

```javascript
1{2  "keys": [3    {4      "kty": "RSA",5      "kid": "key-id",6      "use": "sig",7      "alg": "RS256",8      "n": "...",9      "e": "AQAB"10    }11  ]12}
```


### Validating tokens#
Use a JWT library to verify tokens:

```javascript
1import { createRemoteJWKSet, jwtVerify } from 'jose'23const JWKS = createRemoteJWKSet(4  new URL('https://<project-ref>.supabase.co/auth/v1/.well-known/jwks.json')5)67async function verifyAccessToken(token) {8  try {9    const { payload } = await jwtVerify(token, JWKS, {10      issuer: 'https://<project-ref>.supabase.co/auth/v1',11      audience: 'authenticated',12    })13    return payload14  } catch (error) {15    console.error('Token verification failed:', error)16    return null17  }18}
```


### What to validate#
Always verify:


### Managing user grants#
Users can view and manage the OAuth applications they've authorized to access their account. This is important for transparency and security, allowing users to audit and revoke access when needed.


### Viewing authorized applications#
Users can retrieve a list of all OAuth clients they've authorized:

```javascript
1const { data: grants, error } = await supabase.auth.oauth.getUserGrants()23if (error) {4  console.error('Error fetching grants:', error)5} else {6  console.log('Authorized applications:', grants)7}
```

The response includes details about each authorized OAuth client:

```javascript
1[2  {3    "id": "grant-uuid",4    "client_id": "9a8b7c6d-5e4f-3a2b-1c0d-9e8f7a6b5c4d",5    "client_name": "My Third-Party App",6    "scopes": ["email", "profile"],7    "created_at": "2025-01-15T10:30:00.000Z",8    "updated_at": "2025-01-15T10:30:00.000Z"9  }10]
```


### Revoking access#
Users can revoke access for a specific OAuth client at any time. When access is revoked, all active sessions and refresh tokens for that client are immediately invalidated:

```javascript
1const { error } = await supabase.auth.oauth.revokeGrant(clientId)23if (error) {4  console.error('Error revoking access:', error)5} else {6  console.log('Access revoked successfully')7}
```

After revoking access:

- All refresh tokens for that client are deleted
- The user will need to re-authorize the application to grant access again

Build a settings page for your users

It's a good practice to provide a settings page where users can view all authorized applications and revoke access to any they no longer trust or use. This increases transparency and gives users control over their data.

For complete API reference, see theOAuth methods in supabase-js.


### Next steps#
- Implement MCP authentication- Enable AI agent authentication
- Secure with RLS- Control data access for OAuth clients
- Learn about JWTs- Understand Supabase token structure


================================================================================


# Token Security and Row Level Security
Source: https://supabase.com/docs/guides/auth/oauth-server/token-security

Token Security and Row Level Security

When you enable OAuth 2.1 in your Supabase project, third-party applications can access user data on their behalf. Row Level Security (RLS) policies are crucial for controlling exactly what data each OAuth client can access.

Scopes control OIDC data, not database access

The OAuth scopes (openid,email,profile,phone) control what user information is included in ID tokens and returned by the UserInfo endpoint. They donotcontrol access to your database tables or API endpoints.

Use RLS to define which OAuth clients can access which data, regardless of the scopes they requested.


### How OAuth tokens work with RLS#
OAuth access tokens issued by Supabase Auth are JWTs that include all standard Supabase claims plus OAuth-specific claims. This means your existing RLS policies continue to work, and you can add OAuth-specific logic to create granular access controls.


### Token structure#
Every OAuth access token includes:

```javascript
1{2  "sub": "user-uuid",3  "role": "authenticated",4  "aud": "authenticated",5  "user_id": "user-uuid",6  "email": "user@example.com",7  "client_id": "9a8b7c6d-5e4f-3a2b-1c0d-9e8f7a6b5c4d",8  "aal": "aal1",9  "amr": [{ "method": "password", "timestamp": 1735815600 }],10  "session_id": "session-uuid",11  "iss": "https://<project-ref>.supabase.co/auth/v1",12  "iat": 1735815600,13  "exp": 173581920014}
```

The key OAuth-specific claim is:

You can use this claim in RLS policies to grant different permissions to different clients.


### Extracting OAuth claims in RLS#
Use theauth.jwt()function to access token claims in your policies:

```javascript
1-- Get the client ID from the token2(auth.jwt() ->> 'client_id')34-- Check if the token is from an OAuth client5(auth.jwt() ->> 'client_id') IS NOT NULL67-- Check if the token is from a specific client8(auth.jwt() ->> 'client_id') = 'mobile-app-client-id'
```


### Common RLS patterns for OAuth#

### Pattern 1: Grant specific client full access#
Allow a specific OAuth client to access all user data:

```javascript
1CREATE POLICY "Mobile app can access user data"2ON user_data FOR ALL3USING (4  auth.uid() = user_id AND5  (auth.jwt() ->> 'client_id') = 'mobile-app-client-id'6);
```


### Pattern 2: Grant multiple clients read-only access#
Allow several OAuth clients to read data, but not modify it:

```javascript
1CREATE POLICY "Third-party apps can read profiles"2ON profiles FOR SELECT3USING (4  auth.uid() = user_id AND5  (auth.jwt() ->> 'client_id') IN (6    'analytics-client-id',7    'reporting-client-id',8    'dashboard-client-id'9  )10);
```


### Pattern 3: Restrict sensitive data from OAuth clients#
Prevent OAuth clients from accessing sensitive data:

```javascript
1CREATE POLICY "OAuth clients cannot access payment info"2ON payment_methods FOR ALL3USING (4  auth.uid() = user_id AND5  (auth.jwt() ->> 'client_id') IS NULL  -- Only direct user sessions6);
```


### Pattern 4: Client-specific data access#
Different clients access different subsets of data:

```javascript
1-- Analytics client can only read aggregated data2CREATE POLICY "Analytics client reads summaries"3ON user_metrics FOR SELECT4USING (5  auth.uid() = user_id AND6  (auth.jwt() ->> 'client_id') = 'analytics-client-id'7);89-- Admin client can read and modify all data10CREATE POLICY "Admin client full access"11ON user_data FOR ALL12USING (13  auth.uid() = user_id AND14  (auth.jwt() ->> 'client_id') = 'admin-client-id'15);
```


### Real-world examples#

### Example 1: Multi-platform application#
You have a web app, mobile app, and third-party integrations:

```javascript
1-- Web app: Full access2CREATE POLICY "Web app full access"3ON profiles FOR ALL4USING (5  auth.uid() = user_id AND6  (7    (auth.jwt() ->> 'client_id') = 'web-app-client-id'8    OR (auth.jwt() ->> 'client_id') IS NULL  -- Direct user sessions9  )10);1112-- Mobile app: Read-only access to profiles13CREATE POLICY "Mobile app reads profiles"14ON profiles FOR SELECT15USING (16  auth.uid() = user_id AND17  (auth.jwt() ->> 'client_id') = 'mobile-app-client-id'18);1920-- Third-party integration: Limited data access21CREATE POLICY "Integration reads public data"22ON profiles FOR SELECT23USING (24  auth.uid() = user_id AND25  (auth.jwt() ->> 'client_id') = 'integration-client-id' AND26  is_public = true27);
```


### Custom access token hooks#
Custom Access Token Hookswork with OAuth tokens, allowing you to inject custom claims based on the OAuth client. This is particularly useful for customizing standard JWT claims likeaudience(aud) or adding client-specific metadata.

Custom Access Token Hooks are triggered foralltoken issuance. Useclient_idorauthentication_method(oauth_provider/authorization_codefor OAuth flows) to differentiate OAuth from regular authentication.


### Customizing the audience claim#
A common use case is customizing theaudienceclaim for different OAuth clients. This allows third-party services to validate that tokens were issued specifically for them:

```javascript
1import { serve } from 'https://deno.land/std@0.168.0/http/server.ts'23serve(async (req) => {4  const { user, claims, client_id } = await req.json()56  // Customize audience based on OAuth client7  if (client_id === 'mobile-app-client-id') {8    return new Response(9      JSON.stringify({10        claims: {11          aud: 'https://api.myapp.com',12          app_version: '2.0.0',13        },14      }),15      { headers: { 'Content-Type': 'application/json' } }16    )17  }1819  if (client_id === 'analytics-partner-id') {20    return new Response(21      JSON.stringify({22        claims: {23          aud: 'https://analytics.partner.com',24          access_level: 'read-only',25        },26      }),27      { headers: { 'Content-Type': 'application/json' } }28    )29  }3031  // Default audience for non-OAuth flows32  return new Response(JSON.stringify({ claims: {} }), {33    headers: { 'Content-Type': 'application/json' },34  })35})
```

Theaudienceclaim is especially important for:

- JWT validation by third parties: Services can verify tokens were issued for their specific API
- Multi-tenant applications: Different audiences for different client applications
- Compliance: Meeting security requirements that mandate audience validation


### Adding client-specific claims#
You can also add custom claims and metadata based on the OAuth client:

```javascript
1import { serve } from 'https://deno.land/std@0.168.0/http/server.ts'2import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'34serve(async (req) => {5  const { user, claims, client_id } = await req.json()67  const supabase = createClient(Deno.env.get('SUPABASE_URL')!, Deno.env.get('SUPABASE_SECRET_KEY')!)89  // Add custom claims based on OAuth client10  let customClaims = {}1112  if (client_id === 'mobile-app-client-id') {13    customClaims.aud = 'https://mobile.myapp.com'14    customClaims.app_version = '2.0.0'15    customClaims.platform = 'mobile'16  } else if (client_id === 'analytics-client-id') {17    customClaims.aud = 'https://analytics.myapp.com'18    customClaims.read_only = true19    customClaims.data_retention_days = 9020  } else if (client_id?.startsWith('mcp-')) {21    // MCP AI agents22    const { data: agent } = await supabase23      .from('approved_ai_agents')24      .select('name, max_data_retention_days')25      .eq('client_id', client_id)26      .single()2728    customClaims.aud = `https://mcp.myapp.com/${client_id}`29    customClaims.ai_agent = true30    customClaims.agent_name = agent?.name31    customClaims.max_retention = agent?.max_data_retention_days32  }3334  return new Response(JSON.stringify({ claims: customClaims }), {35    headers: { 'Content-Type': 'application/json' },36  })37})
```

Use these custom claims in RLS:

```javascript
1-- Policy based on custom claims2CREATE POLICY "Read-only clients cannot modify"3ON user_data FOR UPDATE4USING (5  auth.uid() = user_id AND6  (auth.jwt() -> 'user_metadata' ->> 'read_only')::boolean IS NOT TRUE7);89-- Policy based on audience claim10CREATE POLICY "Only specific audience can access"11ON api_data FOR SELECT12USING (13  auth.uid() = user_id AND14  (auth.jwt() ->> 'aud') IN (15    'https://api.myapp.com',16    'https://mobile.myapp.com'17  )18);
```


### Security best practices#

### 1. Principle of least privilege#
Grant OAuth clients only the minimum permissions they need:

```javascript
1-- Bad: Grant all access by default2CREATE POLICY "OAuth clients full access"3ON user_data FOR ALL4USING (auth.uid() = user_id);56-- Good: Grant specific access per client7CREATE POLICY "Specific client specific access"8ON user_data FOR SELECT9USING (10  auth.uid() = user_id AND11  (auth.jwt() ->> 'client_id') = 'trusted-client-id'12);
```


### 2. Separate policies for OAuth clients#
Create dedicated policies for OAuth clients rather than mixing them with user policies:

```javascript
1-- User access2CREATE POLICY "Users access their own data"3ON user_data FOR ALL4USING (5  auth.uid() = user_id AND6  (auth.jwt() ->> 'client_id') IS NULL7);89-- OAuth client access (separate policy)10CREATE POLICY "OAuth clients limited access"11ON user_data FOR SELECT12USING (13  auth.uid() = user_id AND14  (auth.jwt() ->> 'client_id') IN ('client-1', 'client-2')15);
```


### 3. Regularly audit OAuth clients#
Track and review which clients have access:

```javascript
1-- View all active OAuth clients2SELECT3  oc.client_id,4  oc.name,5  oc.created_at,6  COUNT(DISTINCT s.user_id) as active_users7FROM auth.oauth_clients oc8LEFT JOIN auth.sessions s ON s.client_id = oc.client_id9WHERE s.created_at > NOW() - INTERVAL '30 days'10GROUP BY oc.client_id, oc.name, oc.created_at;
```


### Testing your policies#
Always test your RLS policies before deploying to production:

```javascript
1-- Test as a specific OAuth client2SET request.jwt.claims = '{3  "sub": "test-user-uuid",4  "role": "authenticated",5  "client_id": "test-client-id"6}';78-- Test queries9SELECT * FROM user_data WHERE user_id = 'test-user-uuid';1011-- Reset12RESET request.jwt.claims;
```

Or use the Supabase Dashboard'sRLS policy tester.


### Troubleshooting#

### Policy not working for OAuth client#
Problem: OAuth client can't access data despite having a valid token.

Check:

```javascript
1-- Debug: See what client_id is in the token2SELECT auth.jwt() ->> 'client_id';34-- Debug: Test without RLS5SET LOCAL role = service_role;6SELECT * FROM your_table;
```


### Policy too permissive#
Problem: OAuth client has access to data it shouldn't.

Solution: UseAS RESTRICTIVEpolicies to add additional constraints:

```javascript
1-- This policy runs in addition to permissive policies2CREATE POLICY "Restrict OAuth clients"3ON sensitive_data4AS RESTRICTIVE5FOR ALL6TO authenticated7USING (8  -- OAuth clients cannot access this table at all9  (auth.jwt() ->> 'client_id') IS NULL10);
```


### Can't differentiate between users and OAuth clients#
Problem: Need to apply different logic for direct user sessions vs OAuth.

Solution: Check ifclient_idis present:

```javascript
1-- Direct user sessions (no OAuth)2CREATE POLICY "Direct users full access"3ON user_data FOR ALL4USING (5  auth.uid() = user_id AND6  (auth.jwt() ->> 'client_id') IS NULL7);89-- OAuth clients (limited access)10CREATE POLICY "OAuth clients read only"11ON user_data FOR SELECT12USING (13  auth.uid() = user_id AND14  (auth.jwt() ->> 'client_id') IS NOT NULL15);
```


### Next steps#
- Learn about JWTs- Deep dive into Supabase token structure
- Row Level Security- Complete RLS guide
- Custom Access Token Hooks- Inject custom claims
- OAuth flows- Understand token issuance


================================================================================


# Password security
Source: https://supabase.com/docs/guides/auth/password-security

Password security


### Help your users to protect their password security
Help your users to protect their password security

A password is more secure if it is harder to guess or brute-force. In theory, a password is harder to guess if it is longer. It is also harder to guess if it uses a larger set of characters (for example, digits, lowercase and uppercase letters, and symbols).

This table shows theminimumnumber of guesses that need to be tried to access a user's account:

In reality though, passwords are not always generated at random. They often contain variations of names, words, dates, and common phrases. Malicious actors can use these properties to guess a password in fewer attempts.

There are hundreds of millions (and growing!) known passwords out there. Malicious actors can use these lists of leaked passwords to automate login attempts (known as credential stuffing) and steal or access sensitive user data.


### Password strength and leaked password protection#
To help protect your users, Supabase Auth allows you fine-grained control over the strength of the passwords used on your project. You can configure these in your project'sAuth settings:

- Set a large minimum password length. Anything less than 8 characters is not recommended.
- Set the required characters that must appear at least once in a user's password. Use the strongest option of requiring digits, lowercase and uppercase letters, and symbols. The allowed symbols are:!@#$%^&*()_+-=[]{};'\:"|<>?,./`~
- Prevent the use of leaked passwords. Supabase Auth uses the open-sourceHaveIBeenPwned.org Pwned Passwords APIto reject passwords that have been leaked and are known by malicious actors.

Leaked password protection is available on the Pro Plan and above.


### Additional recommendations#
In addition to choosing suitable password strength settings and preventing the use of leaked passwords, consider asking your users to:

- Use a password manager to store and generate passwords.
- Avoid password reuse across websites and apps.
- Avoid using personal information in passwords.
- UseMulti-Factor Authentication.


### Frequently asked questions#

### How are passwords stored?#
Supabase Auth usesbcrypt, a strong password hashing function, to store hashes of users' passwords. Only hashed passwords are stored. You cannot impersonate a user with the password hash. Each hash is accompanied by a randomly generated salt parameter for extra security.

The hash is stored in theencrypted_passwordcolumn of theauth.userstable. The column's name is a misnomer (cryptographic hashing is not encryption), but is kept for backward compatibility.


### How will strengthened password requirements affect current users?#
Existing users can still sign in with their current password even if it doesn't meet the new, strengthened password requirements. However, if their password falls short of these updated standards, they will encounter aWeakPasswordErrorduring thesignInWithPasswordprocess, explaining why it's considered weak. This change is also applicable to new users and existing users changing their passwords, ensuring everyone adheres to the enhanced security standards.


================================================================================


# Password-based Auth
Source: https://supabase.com/docs/guides/auth/passwords

Password-based Auth


### Allow users to sign in with a password connected to their email or phone number.
Allow users to sign in with a password connected to their email or phone number.

Users often expect to sign in to your site with a password. Supabase Auth helps you implement password-based auth safely, using secure configuration options and best practices for storing and verifying passwords.

Users can associate a password with their identity using theiremail addressor aphone number.


### With email#

### Enabling email and password-based authentication#
Email authentication is enabled by default.

You can configure whether users need to verify their email to sign in. On hosted Supabase projects, this is true by default. On self-hosted projects or in local development, this is false by default.

Change this setting on theAuth Providers pagefor hosted projects, or in theconfiguration filefor self-hosted projects.


### Signing up with an email and password#
There are two possible flows for email signup:implicit flowandPKCE flow. If you're using SSR, you're using the PKCE flow. If you're using client-only code, the default flow depends upon the client library. The implicit flow is the default in JavaScript and Dart, and the PKCE flow is the default in Swift.

The instructions in this section assume that email confirmations are enabled.

The implicit flow only works for client-only apps. Your site directly receives the access token after the user confirms their email.

To sign up the user, callsignUp()with their email address and password.

You can optionally specify a URL to redirect to after the user clicks the confirmation link. This URL must be configured as aRedirect URL, which you can do in thedashboardfor hosted projects, or in theconfiguration filefor self-hosted projects.

If you don't specify a redirect URL, the user is automatically redirected to your site URL. This defaults tolocalhost:3000, but you can also configure this.

```javascript
1async function () {2  const { ,  } = await ..({3    : 'valid.email@supabase.io',4    : 'example-password',5    : {6      : 'https://example.com/welcome',7    },8  })9}
```


### Signing in with an email and password#
When your user signs in, callsignInWithPassword()with their email address and password:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'valid.email@supabase.io',4    : 'example-password',5  })6}
```


### Resetting a password#

### Step 1: Create a reset password page#
Create areset passwordpage. This page should be publicly accessible.

Collect the user's email address and request a password reset email. Specify the redirect URL, which should point to the URL of achange passwordpage. This URL needs to be configured in yourredirect URLs.

```javascript
1await ..('valid.email@supabase.io', {2  : 'http://example.com/account/update-password',3})
```


### Step 2: Create a change password page#
Create achange passwordpage at the URL you specified in the previous step. This page should be accessible only to authenticated users.

Collect the user's new password and callupdateUserto update their password.

```javascript
1await ..({ : 'new_password' })
```


### Email sending#
The signup confirmation and password reset flows require an SMTP server to send emails.

The Supabase platform comes with a default email-sending service for you to try out. The service has a rate limit of2emails per hour, and availability is on a best-effort basis. For production use, you should consider configuring a custom SMTP server.

Consider configuring a custom SMTP server for production.

See theCustom SMTP guidefor instructions.


### Local development with Mailpit#
You can test email flows on your local machine. The Supabase CLI automatically captures emails sent locally by usingMailpit.

In your terminal, runsupabase statusto get the Mailpit URL. Go to this URL in your browser, and follow the instructions to find your emails.


### With phone#
You can use a user's mobile phone number as an identifier, instead of an email address, when they sign up with a password.

This practice is usually discouraged because phone networks recycle mobile phone numbers. Anyone receiving a recycled phone number gets access to the original user's account. To mitigate this risk,implement MFA.

Protect users who use a phone number as a password-based auth identifier by enabling MFA.


### Enabling phone and password-based authentication#
Enable phone authentication on theAuth Providers pagefor hosted Supabase projects.

For self-hosted projects or local development, use theconfiguration file. See the configuration variables namespaced underauth.sms.

If you want users to confirm their phone number on signup, you need to set up an SMS provider. Each provider has its own configuration. Supported providers include MessageBird, Twilio, Vonage, and TextLocal (community-supported).


### Configuring SMS Providers

### Signing up with a phone number and password#
To sign up the user, callsignUp()with their phone number and password:

```javascript
1const { ,  } = await ..({2  : '+13334445555',3  : 'some-password',4})
```

If you have phone verification turned on, the user receives an SMS with a 6-digit pin that you must verify within 60 seconds:

You should present a form to the user so they can input the 6 digit pin, then send it along with the phone number toverifyOtp:

```javascript
1const {2  : {  },3  ,4} = await ..({5  : '+13334445555',6  : '123456',7  : 'sms',8})
```


### Signing in a with a phone number and password#
Call the function to sign in with the user's phone number and password:

```javascript
1const { ,  } = await ..({2  : '+13334445555',3  : 'some-password',4})
```


================================================================================


# Phone Login
Source: https://supabase.com/docs/guides/auth/phone-login

Phone Login

Phone Login is a method of authentication that allows users to log in to a website or application without using a password. The user authenticates through a one-time password (OTP) sent via a channel (SMS or WhatsApp).

At this time,WhatsAppis only supported as a channel for the Twilio and Twilio Verify Providers.

Users can also log in with their phones using Native Mobile Login with the built-in identity provider. For Native Mobile Login with Android and iOS, see theSocial Login guides.

Phone OTP login can:

- Improve the user experience by not requiring users to create and remember a password
- Increase security by reducing the risk of password-related security breaches
- Reduce support burden of dealing with password resets and other password-related flows

To keep SMS sending costs under control, make sure you adjust your project's rate limits andconfigure CAPTCHA. See theProduction Checklistto learn more.

Some countries have special regulations for services that send SMS messages to users, (e.g India's TRAI DLT regulations). Remember to look up and follow the regulations of countries where you operate.


### Enabling phone login#
Enable phone authentication on theAuth Providers pagefor hosted Supabase projects.

For self-hosted projects or local development, use theconfiguration file. See the configuration variables namespaced underauth.sms.

You also need to set up an SMS provider. Each provider has its own configuration. Supported providers include MessageBird, Twilio, Vonage, and TextLocal (community-supported).


### Configuring SMS Providers
By default, a user can only request an OTP once every60 secondsand they expire after1 hour.


### Signing in with phone OTP#
With OTP, a user can sign in without setting a password on their account. They need to verify their phone number each time they sign in.

```javascript
1const { ,  } = await ..({2  : '+13334445555',3})
```

The user receives an SMS with a 6-digit pin that you must verify within 60 seconds.


### Verifying a phone OTP#
To verify the one-time password (OTP) sent to the user's phone number, callverifyOtp()with the phone number and OTP:

You should present a form to the user so they can input the 6 digit pin, then send it along with the phone number toverifyOtp:

```javascript
1const {2  : {  },3  ,4} = await ..({5  : '13334445555',6  : '123456',7  : 'sms',8})
```

If successful the user will now be logged in and you should receive a valid session like:

```javascript
1{2  "access_token": "<ACCESS_TOKEN>",3  "token_type": "bearer",4  "expires_in": 3600,5  "refresh_token": "<REFRESH_TOKEN>"6}
```

The access token can be sent in the Authorization header as a Bearer token for any CRUD operations on supabase-js. See our guide onRow Level Securityfor more info on restricting access on a user basis.


### Updating a phone number#
To update a user's phone number, the user must be logged in. CallupdateUser()with their phone number:

```javascript
1const { ,  } = await ..({2  : '123456789',3})
```

The user receives an SMS with a 6-digit pin that you mustverifywithin 60 seconds.
Use thephone_changetype when callingverifyOTPto update a user’s phone number.


================================================================================


# Use Supabase Auth with Next.js
Source: https://supabase.com/docs/guides/auth/quickstarts/nextjs

Use Supabase Auth with Next.js


### Learn how to configure Supabase Auth for the Next.js App Router.
Learn how to configure Supabase Auth for the Next.js App Router.


### Create a new Supabase project
Head over todatabase.newand create a new Supabase project.

Your new database has a table for storing your users. You can see that this table is currently empty by running some SQL in theSQL Editor.

```javascript
1select * from auth.users;
```


### Create a Next.js app
Use thecreate-next-appcommand and thewith-supabasetemplate, to create a Next.js app pre-configured with:

- Cookie-based Auth
- TypeScript
- Tailwind CSSExplore drop-in UI components for your Supabase app.UI components built on shadcn/ui that connect to Supabase via a single command.Explore Components

Cookie-based Auth

TypeScript

Tailwind CSS

UI components built on shadcn/ui that connect to Supabase via a single command.

```javascript
1npx create-next-app -e with-supabase
```


### Declare Supabase Environment Variables
Rename.env.exampleto.env.localand populate with your Supabase connection variables:

```javascript
1NEXT_PUBLIC_SUPABASE_URL=your-project-url2NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY=sb_publishable_... or anon key
```

You can also get the Project URL and key fromthe project'sConnectdialog.

Supabase is changing the way keys work to improve project security and developer experience. You canread the full announcement, but in the transition period, you can use both the currentanonandservice_rolekeys and the new publishable key with the formsb_publishable_xxxwhich will replace the older keys.

In most cases, you can get the correct key fromthe Project'sConnectdialog, but if you want a specific key, you can find all keys inthe API Keys section of a Project's Settings page:

- For legacy keys, copy theanonkey for client-side operations and theservice_rolekey for server-side operations from theLegacy API Keystab.
- For new keys, open theAPI Keystab, if you don't have a publishable key already, clickCreate new API Keys, and copy the value from thePublishable keysection.

Read the API keys docsfor a full explanation of all key types and their uses.


### Start the app
Start the development server, go tohttp://localhost:3000in a browser, and you should see the contents ofapp/page.tsx.

To sign up a new user, navigate tohttp://localhost:3000/auth/sign-up, and clickSign up.

```javascript
1npm run dev
```


### Learn more#
- Setting up Server-Side Auth for Next.jsfor a Next.js deep dive
- Supabase Auth docsfor more Supabase authentication methods


================================================================================


# Use Supabase Auth with React
Source: https://supabase.com/docs/guides/auth/quickstarts/react

Use Supabase Auth with React


### Learn how to use Supabase Auth with React.js.
Learn how to use Supabase Auth with React.js.


### Create a new Supabase project
Launch a new projectin the Supabase Dashboard.

Your new database has a table for storing your users. You can see that this table is currently empty by running some SQL in theSQL Editor.

```javascript
1select * from auth.users;
```


### Create a React app
Create a React app using aVitetemplate.

```javascript
1npm create vite@latest my-app -- --template react
```


### Install the Supabase client library
Navigate to the React app and install the Supabase libraries.

```javascript
1cd my-app && npm install @supabase/supabase-js
```


### Declare Supabase Environment Variables
Rename.env.exampleto.env.localand populate with your Supabase connection variables:

```javascript
1VITE_SUPABASE_URL=your-project-url2VITE_SUPABASE_PUBLISHABLE_DEFAULT_KEY=your-publishable-key-or-anon-key
```

You can also get the Project URL and key fromthe project'sConnectdialog.

Supabase is changing the way keys work to improve project security and developer experience. You canread the full announcement, but in the transition period, you can use both the currentanonandservice_rolekeys and the new publishable key with the formsb_publishable_xxxwhich will replace the older keys.

In most cases, you can get the correct key fromthe Project'sConnectdialog, but if you want a specific key, you can find all keys inthe API Keys section of a Project's Settings page:

- For legacy keys, copy theanonkey for client-side operations and theservice_rolekey for server-side operations from theLegacy API Keystab.
- For new keys, open theAPI Keystab, if you don't have a publishable key already, clickCreate new API Keys, and copy the value from thePublishable keysection.

Read the API keys docsfor a full explanation of all key types and their uses.


### Set up your login component
UI components built on shadcn/ui that connect to Supabase via a single command.

InApp.jsx, create a Supabase client using your Project URL and key.

The code uses thegetClaimsmethod inApp.jsxto validate the local JWT before showing the signed-in user.

```javascript
1import "./index.css";2import { useState, useEffect } from "react";3import { createClient } from "@supabase/supabase-js";45const supabase = createClient(import.meta.env.VITE_SUPABASE_URL, import.meta.env.VITE_SUPABASE_PUBLISHABLE_DEFAULT_KEY);67export default function App() {8    const [loading, setLoading] = useState(false);9    const [email, setEmail] = useState("");10    const [claims, setClaims] = useState(null);1112    // Check URL params on initial render13    const params = new URLSearchParams(window.location.search);14    const hasTokenHash = params.get("token_hash");1516    const [verifying, setVerifying] = useState(!!hasTokenHash);17    const [authError, setAuthError] = useState(null);18    const [authSuccess, setAuthSuccess] = useState(false);1920    useEffect(() => {21        // Check if we have token_hash in URL (magic link callback)22        const params = new URLSearchParams(window.location.search);23        const token_hash = params.get("token_hash");24        const type = params.get("type");2526        if (token_hash) {27            // Verify the OTP token28            supabase.auth.verifyOtp({29                token_hash,30                type: type || "email",31            }).then(({ error }) => {32                if (error) {33                    setAuthError(error.message);34                } else {35                    setAuthSuccess(true);36                    // Clear URL params37                    window.history.replaceState({}, document.title, "/");38                }39                setVerifying(false);40            });41        }4243        // Check for existing session using getClaims44        supabase.auth.getClaims().then(({ data: { claims } }) => {45            setClaims(claims);46        });4748        // Listen for auth changes49        const {50            data: { subscription },51        } = supabase.auth.onAuthStateChange(() => {52            supabase.auth.getClaims().then(({ data: { claims } }) => {53                setClaims(claims);54            });55        });5657        return () => subscription.unsubscribe();58    }, []);5960    const handleLogin = async (event) => {61        event.preventDefault();62        setLoading(true);63        const { error } = await supabase.auth.signInWithOtp({64            email,65            options: {66                emailRedirectTo: window.location.origin,67            }68        });69        if (error) {70            alert(error.error_description || error.message);71        } else {72            alert("Check your email for the login link!");73        }74        setLoading(false);75    };7677    const handleLogout = async () => {78        await supabase.auth.signOut();79        setClaims(null);80    };8182    // Show verification state83    if (verifying) {84        return (85            <div>86                <h1>Authentication</h1>87                <p>Confirming your magic link...</p>88                <p>Loading...</p>89            </div>90        );91    }9293    // Show auth error94    if (authError) {95        return (96            <div>97                <h1>Authentication</h1>98                <p>✗ Authentication failed</p>99                <p>{authError}</p>100                <button101                    onClick={() => {102                        setAuthError(null);103                        window.history.replaceState({}, document.title, "/");104                    }}105                >106                    Return to login107                </button>108            </div>109        );110    }111112    // Show auth success (briefly before claims load)113    if (authSuccess && !claims) {114        return (115            <div>116                <h1>Authentication</h1>117                <p>✓ Authentication successful!</p>118                <p>Loading your account...</p>119            </div>120        );121    }122123    // If user is logged in, show welcome screen124    if (claims) {125        return (126            <div>127                <h1>Welcome!</h1>128                <p>You are logged in as: {claims.email}</p>129                <button onClick={handleLogout}>130                    Sign Out131                </button>132            </div>133        );134    }135136    // Show login form137    return (138        <div>139            <h1>Supabase + React</h1>140            <p>Sign in via magic link with your email below</p>141            <form onSubmit={handleLogin}>142                <input143                    type="email"144                    placeholder="Your email"145                    value={email}146                    required={true}147                    onChange={(e) => setEmail(e.target.value)}148                />149                <button disabled={loading}>150                    {loading ? <span>Loading</span> : <span>Send magic link</span>}151                </button>152            </form>153        </div>154    );155}
```


### Customize email template
Before proceeding, change the email template to support support a server-side authentication flow that sends a token hash:

- Go to theAuth templatespage in your dashboard.
- Select the Confirm sign up template.
- Change{{ .ConfirmationURL }}to{{ .SiteURL }}?token_hash={{ .TokenHash }}&type=email.
- Change yourSite URLtohttps://localhost:5173


### Start the app
Start the app, go tohttp://localhost:5173in a browser, and open the browser console and you should be able to register and log in.

```javascript
1npm run dev
```


================================================================================


# Use Supabase Auth with React Native
Source: https://supabase.com/docs/guides/auth/quickstarts/react-native

Use Supabase Auth with React Native


### Learn how to use Supabase Auth with React Native
Learn how to use Supabase Auth with React Native


### Create a new Supabase project
Launch a new projectin the Supabase Dashboard.

Your new database has a table for storing your users. You can see that this table is currently empty by running some SQL in theSQL Editor.

```javascript
1select * from auth.users;
```


### Create a React app
Create a React app using thecreate-expo-appcommand.

```javascript
1npx create-expo-app -t expo-template-blank-typescript my-app
```


### Install the Supabase client library
Installsupabase-jsand the required dependencies.

```javascript
1cd my-app && npx expo install @supabase/supabase-js @react-native-async-storage/async-storage @rneui/themed react-native-url-polyfill
```


### Set up your login component
Create a helper filelib/supabase.tsthat exports a Supabase client using your Project URL and key.

Rename.env.exampleto.envand populate with your Supabase connection variables:

```javascript
1import { AppState, Platform } from 'react-native'2import 'react-native-url-polyfill/auto'3import AsyncStorage from '@react-native-async-storage/async-storage'4import { createClient, processLock } from '@supabase/supabase-js'56const supabaseUrl = process.env.EXPO_PUBLIC_SUPABASE_URL!7const supabaseAnonKey = process.env.EXPO_PUBLIC_SUPABASE_ANON_KEY!89export const supabase = createClient(supabaseUrl, supabaseAnonKey, {10  auth: {11    ...(Platform.OS !== 'web' ? { storage: AsyncStorage } : {}),12    autoRefreshToken: true,13    persistSession: true,14    detectSessionInUrl: false,15    lock: processLock,16  },17})1819// Tells Supabase Auth to continuously refresh the session automatically20// if the app is in the foreground. When this is added, you will continue21// to receive `onAuthStateChange` events with the `TOKEN_REFRESHED` or22// `SIGNED_OUT` event if the user's session is terminated. This should23// only be registered once.24if (Platform.OS !== 'web') {25  AppState.addEventListener('change', (state) => {26    if (state === 'active') {27      supabase.auth.startAutoRefresh()28    } else {29      supabase.auth.stopAutoRefresh()30    }31  })32}
```

You can also get the Project URL and key fromthe project'sConnectdialog.

Supabase is changing the way keys work to improve project security and developer experience. You canread the full announcement, but in the transition period, you can use both the currentanonandservice_rolekeys and the new publishable key with the formsb_publishable_xxxwhich will replace the older keys.

In most cases, you can get the correct key fromthe Project'sConnectdialog, but if you want a specific key, you can find all keys inthe API Keys section of a Project's Settings page:

- For legacy keys, copy theanonkey for client-side operations and theservice_rolekey for server-side operations from theLegacy API Keystab.
- For new keys, open theAPI Keystab, if you don't have a publishable key already, clickCreate new API Keys, and copy the value from thePublishable keysection.

Read the API keys docsfor a full explanation of all key types and their uses.


### Create a login component
Create a React Native component to manage logins and sign ups. The app later uses thegetClaimsmethod inApp.tsxto validate the local JWT before showing the signed-in user.

```javascript
1import React, { useState } from 'react'2import { Alert, StyleSheet, View, Text, TextInput, TouchableOpacity } from 'react-native'3import { supabase } from '../lib/supabase'45export default function Auth() {6  const [email, setEmail] = useState('')7  const [password, setPassword] = useState('')8  const [loading, setLoading] = useState(false)910  async function signInWithEmail() {11    setLoading(true)12    const { error } = await supabase.auth.signInWithPassword({13      email: email,14      password: password,15    })1617    if (error) Alert.alert(error.message)18    setLoading(false)19  }2021  async function signUpWithEmail() {22    setLoading(true)23    const {24      data: { session },25      error,26    } = await supabase.auth.signUp({27      email: email,28      password: password,29    })3031    if (error) Alert.alert(error.message)32    if (!session) Alert.alert('Please check your inbox for email verification!')33    setLoading(false)34  }3536  return (37    <View style={styles.container}>38      <View style={[styles.verticallySpaced, styles.mt20]}>39        <Text style={styles.label}>Email</Text>40        <TextInput41          onChangeText={(text) => setEmail(text)}42          value={email}43          placeholder="email@address.com"44          autoCapitalize="none"45          style={styles.input}46        />47      </View>48      <View style={styles.verticallySpaced}>49        <Text style={styles.label}>Password</Text>50        <TextInput51          onChangeText={(text) => setPassword(text)}52          value={password}53          secureTextEntry={true}54          placeholder="Password"55          autoCapitalize="none"56          style={styles.input}57        />58      </View>59      <View style={[styles.verticallySpaced, styles.mt20]}>60        <TouchableOpacity61          style={[styles.button, loading && styles.buttonDisabled]}62          onPress={() => signInWithEmail()}63          disabled={loading}64        >65          <Text style={styles.buttonText}>Sign in</Text>66        </TouchableOpacity>67      </View>68      <View style={styles.verticallySpaced}>69        <TouchableOpacity70          style={[styles.button, loading && styles.buttonDisabled]}71          onPress={() => signUpWithEmail()}72          disabled={loading}73        >74          <Text style={styles.buttonText}>Sign up</Text>75        </TouchableOpacity>76      </View>77    </View>78  )79}8081const styles = StyleSheet.create({82  container: {83    marginTop: 40,84    padding: 12,85  },86  verticallySpaced: {87    paddingTop: 4,88    paddingBottom: 4,89    alignSelf: 'stretch',90  },91  mt20: {92    marginTop: 20,93  },94  label: {95    fontSize: 16,96    fontWeight: '600',97    color: '#86939e',98    marginBottom: 6,99  },100  input: {101    borderWidth: 1,102    borderColor: '#86939e',103    borderRadius: 4,104    padding: 12,105    fontSize: 16,106  },107  button: {108    backgroundColor: '#2089dc',109    borderRadius: 4,110    padding: 12,111    alignItems: 'center',112  },113  buttonDisabled: {114    opacity: 0.5,115  },116  buttonText: {117    color: '#fff',118    fontSize: 16,119    fontWeight: '600',120  },121})
```


### Add the Auth component to your app
Add theAuthcomponent to yourApp.tsxfile. If the user is logged in, print the user id to the screen.

```javascript
1import 'react-native-url-polyfill/auto'2import { useState, useEffect } from 'react'3import { supabase } from './lib/supabase'4import Auth from './components/Auth'5import { View, Text } from 'react-native'6import { JwtPayload } from '@supabase/supabase-js'78export default function App() {9  const [claims, setClaims] = useState<JwtPayload | null>(null)1011  useEffect(() => {12    supabase.auth.getClaims().then(({ data: { claims } }) => {13      setClaims(claims)14    })1516    supabase.auth.onAuthStateChange(() => {17      supabase.auth.getClaims().then(({ data: { claims } }) => {18        setClaims(claims)19      })20    })21  }, [])2223  return (24    <View>25      <Auth />26      {claims && <Text>{claims.sub}</Text>}27    </View>28  )29}
```


### Start the app
Start the app, and follow the instructions in the terminal.

```javascript
1npm start
```


================================================================================


# Build a Social Auth App with Expo React Native
Source: https://supabase.com/docs/guides/auth/quickstarts/with-expo-react-native-social-auth

Build a Social Auth App with Expo React Native

This tutorial demonstrates how to build a React Native app withExpothat implements social authentication. The app showcases a complete authentication flow with protected navigation using:

- Supabase Database- a Postgres database for storing your user data withRow Level Securityto ensure data is protected and users can only access their own information.
- Supabase Auth- enables users to log in through social authentication providers (Apple and Google).



If you get stuck while working through this guide, refer to thefull example on GitHub.


### Project setup#
Before you start building you need to set up the Database and API. You can do this by starting a new Project in Supabase and then creating a "schema" inside the database.


### Create a project#

### Set up the database schema#
Now set up the database schema. You can use the "User Management Starter" quickstart in the SQL Editor, or you can copy/paste the SQL from below and run it.

You can pull the database schema down to your local project by running thedb pullcommand. Read thelocal development docsfor detailed instructions.

```javascript
1supabase link --project-ref <project-id>2# You can get <project-id> from your project's dashboard URL: https://supabase.com/dashboard/project/<project-id>3supabase db pull
```


### Get API details#
Now that you've created some database tables, you are ready to insert data using the auto-generated API.

To do this, you need to get the Project URL and key fromthe projectConnectdialog.

Supabase is changing the way keys work to improve project security and developer experience. You canread the full announcement, but in the transition period, you can use both the currentanonandservice_rolekeys and the new publishable key with the formsb_publishable_xxxwhich will replace the older keys.

In most cases, you can get the correct key fromthe Project'sConnectdialog, but if you want a specific key, you can find all keys inthe API Keys section of a Project's Settings page:

- For legacy keys, copy theanonkey for client-side operations and theservice_rolekey for server-side operations from theLegacy API Keystab.
- For new keys, open theAPI Keystab, if you don't have a publishable key already, clickCreate new API Keys, and copy the value from thePublishable keysection.

Read the API keys docsfor a full explanation of all key types and their uses.


### Building the app#
Start by building the React Native app from scratch.


### Initialize a React Native app#
UseExpoto initialize an app calledexpo-social-authwith thestandard template:

```javascript
1npx create-expo-app@latest23cd expo-social-auth
```

Install the additional dependencies:

- supabase-js
- @react-native-async-storage/async-storage- A key-value store for React Native.
- expo-secure-store- Provides a way to securely store key-value pairs locally on the device.
- expo-splash-screen- Provides a way to programmatically manage the splash screen.

```javascript
1npx expo install @supabase/supabase-js @react-native-async-storage/async-storage expo-secure-store expo-splash-screen
```

Now, create a helper file to initialize the Supabase client for both web and React Native platforms using platform-specificstorage adapters:Expo SecureStorefor mobile andAsyncStoragefor web.

```javascript
1import AsyncStorage from '@react-native-async-storage/async-storage';2import { createClient } from '@supabase/supabase-js';3import 'react-native-url-polyfill/auto';45const ExpoWebSecureStoreAdapter = {6  getItem: (key: string) => {7    console.debug("getItem", { key })8    return AsyncStorage.getItem(key)9  },10  setItem: (key: string, value: string) => {11    return AsyncStorage.setItem(key, value)12  },13  removeItem: (key: string) => {14    return AsyncStorage.removeItem(key)15  },16};1718export const supabase = createClient(19  process.env.EXPO_PUBLIC_SUPABASE_URL ?? '',20  process.env.EXPO_PUBLIC_SUPABASE_ANON_KEY ?? '',21  {22    auth: {23      storage: ExpoWebSecureStoreAdapter,24      autoRefreshToken: true,25      persistSession: true,26      detectSessionInUrl: false,27    },28  },29);
```


### Set up environment variables#
You need the API URL and theanonkey copiedearlier.
These variables are safe to expose in your Expo app since Supabase hasRow Level Securityenabled on your database.

Create a.envfile containing these variables:

```javascript
1EXPO_PUBLIC_SUPABASE_URL=YOUR_SUPABASE_URL2EXPO_PUBLIC_SUPABASE_ANON_KEY=YOUR_SUPABASE_ANON_KEY
```


### Set up protected navigation#
Next, you need to protect app navigation to prevent unauthenticated users from accessing protected routes. Use theExpoSplashScreento display a loading screen while fetching the user profile and verifying authentication status.


### Create theAuthContext#
Createa React contextto manage the authentication session, making it accessible from any component:

```javascript
1import { Session } from '@supabase/supabase-js'2import { createContext, useContext } from 'react'34export type AuthData = {5  session?: Session | null6  profile?: any | null7  isLoading: boolean8  isLoggedIn: boolean9}1011export const AuthContext = createContext<AuthData>({12  session: undefined,13  profile: undefined,14  isLoading: true,15  isLoggedIn: false,16})1718export const useAuthContext = () => useContext(AuthContext)
```


### Create theAuthProvider#
Next, create a provider component to manage the authentication session throughout the app:

```javascript
1import { AuthContext } from '@/hooks/use-auth-context'2import { supabase } from '@/lib/supabase'3import type { Session } from '@supabase/supabase-js'4import { PropsWithChildren, useEffect, useState } from 'react'56export default function AuthProvider({ children }: PropsWithChildren) {7  const [session, setSession] = useState<Session | undefined | null>()8  const [profile, setProfile] = useState<any>()9  const [isLoading, setIsLoading] = useState<boolean>(true)1011  // Fetch the session once, and subscribe to auth state changes12  useEffect(() => {13    const fetchSession = async () => {14      setIsLoading(true)1516      const {17        data: { session },18        error,19      } = await supabase.auth.getSession()2021      if (error) {22        console.error('Error fetching session:', error)23      }2425      setSession(session)26      setIsLoading(false)27    }2829    fetchSession()3031    const {32      data: { subscription },33    } = supabase.auth.onAuthStateChange((_event, session) => {34      console.log('Auth state changed:', { event: _event, session })35      setSession(session)36    })3738    // Cleanup subscription on unmount39    return () => {40      subscription.unsubscribe()41    }42  }, [])4344  // Fetch the profile when the session changes45  useEffect(() => {46    const fetchProfile = async () => {47      setIsLoading(true)4849      if (session) {50        const { data } = await supabase51          .from('profiles')52          .select('*')53          .eq('id', session.user.id)54          .single()5556        setProfile(data)57      } else {58        setProfile(null)59      }6061      setIsLoading(false)62    }6364    fetchProfile()65  }, [session])6667  return (68    <AuthContext.Provider69      value={{70        session,71        isLoading,72        profile,73        isLoggedIn: session != undefined,74      }}75    >76      {children}77    </AuthContext.Provider>78  )79}
```


### Create theSplashScreenController#
Create aSplashScreenControllercomponent to display theExpoSplashScreenwhile the authentication session is loading:

```javascript
1import { useAuthContext } from '@/hooks/use-auth-context'2import { SplashScreen } from 'expo-router'34SplashScreen.preventAutoHideAsync()56export function SplashScreenController() {7  const { isLoading } = useAuthContext()89  if (!isLoading) {10    SplashScreen.hideAsync()11  }1213  return null14}
```


### Create a logout component#
Create a logout button component to handle user sign-out:

```javascript
1import { supabase } from '@/lib/supabase'2import React from 'react'3import { Button } from 'react-native'45async function onSignOutButtonPress() {6  const { error } = await supabase.auth.signOut()78  if (error) {9    console.error('Error signing out:', error)10  }11}1213export default function SignOutButton() {14  return <Button title="Sign out" onPress={onSignOutButtonPress} />15}
```

And add it to theapp/(tabs)/index.tsxfile used to display the user profile data and the logout button:

```javascript
1import { Image } from 'expo-image'2import { StyleSheet } from 'react-native'34import { HelloWave } from '@/components/hello-wave'5import ParallaxScrollView from '@/components/parallax-scroll-view'6import { ThemedText } from '@/components/themed-text'7import { ThemedView } from '@/components/themed-view'8import SignOutButton from '@/components/social-auth-buttons/sign-out-button'9import { useAuthContext } from '@/hooks/use-auth-context'1011export default function HomeScreen() {12  const { profile } = useAuthContext()1314  return (15    <ParallaxScrollView16      headerBackgroundColor={{ light: '#A1CEDC', dark: '#1D3D47' }}17      headerImage={18        <Image19          source={require('@/assets/images/partial-react-logo.png')}20          style={styles.reactLogo}21        />22      }23    >24      <ThemedView style={styles.titleContainer}>25        <ThemedText type="title">Welcome!</ThemedText>26        <HelloWave />27      </ThemedView>28      <ThemedView style={styles.stepContainer}>29        <ThemedText type="subtitle">Username</ThemedText>30        <ThemedText>{profile?.username}</ThemedText>31        <ThemedText type="subtitle">Full name</ThemedText>32        <ThemedText>{profile?.full_name}</ThemedText>33      </ThemedView>34      <SignOutButton />35    </ParallaxScrollView>36  )37}3839const styles = StyleSheet.create({40  titleContainer: {41    flexDirection: 'row',42    alignItems: 'center',43    gap: 8,44  },45  stepContainer: {46    gap: 8,47    marginBottom: 8,48  },49  reactLogo: {50    height: 178,51    width: 290,52    bottom: 0,53    left: 0,54    position: 'absolute',55  },56})
```


### Create a login screen#
Next, create a basic login screen component:

```javascript
1import { Link, Stack } from 'expo-router'2import { StyleSheet } from 'react-native'34import { ThemedText } from '@/components/themed-text'5import { ThemedView } from '@/components/themed-view'67export default function LoginScreen() {8  return (9    <>10      <Stack.Screen options={{ title: 'Login' }} />11      <ThemedView style={styles.container}>12        <ThemedText type="title">Login</ThemedText>13        <Link href="/" style={styles.link}>14          <ThemedText type="link">Try to navigate to home screen!</ThemedText>15        </Link>16      </ThemedView>17    </>18  )19}2021const styles = StyleSheet.create({22  container: {23    flex: 1,24    alignItems: 'center',25    justifyContent: 'center',26    padding: 20,27  },28  link: {29    marginTop: 15,30    paddingVertical: 15,31  },32})
```


### Implement protected routes#
Wrap the navigation with theAuthProviderandSplashScreenController.

UsingExpo Router's protected routes, you can secure navigation:

```javascript
1import { DarkTheme, DefaultTheme, ThemeProvider } from '@react-navigation/native'2import { useFonts } from 'expo-font'3import { Stack } from 'expo-router'4import { StatusBar } from 'expo-status-bar'5import 'react-native-reanimated'67import { SplashScreenController } from '@/components/splash-screen-controller'89import { useAuthContext } from '@/hooks/use-auth-context'10import { useColorScheme } from '@/hooks/use-color-scheme'11import AuthProvider from '@/providers/auth-provider'1213// Separate RootNavigator so we can access the AuthContext14function RootNavigator() {15  const { isLoggedIn } = useAuthContext()1617  return (18    <Stack>19      <Stack.Protected guard={isLoggedIn}>20        <Stack.Screen name="(tabs)" options={{ headerShown: false }} />21      </Stack.Protected>22      <Stack.Protected guard={!isLoggedIn}>23        <Stack.Screen name="login" options={{ headerShown: false }} />24      </Stack.Protected>25      <Stack.Screen name="+not-found" />26    </Stack>27  )28}2930export default function RootLayout() {31  const colorScheme = useColorScheme()3233  const [loaded] = useFonts({34    SpaceMono: require('../assets/fonts/SpaceMono-Regular.ttf'),35  })3637  if (!loaded) {38    // Async font loading only occurs in development.39    return null40  }4142  return (43    <ThemeProvider value={colorScheme === 'dark' ? DarkTheme : DefaultTheme}>44      <AuthProvider>45        <SplashScreenController />46        <RootNavigator />47        <StatusBar style="auto" />48      </AuthProvider>49    </ThemeProvider>50  )51}
```

You can now test the app by running:

```javascript
1npx expo prebuild2npx expo start --clear
```

Verify that the app works as expected. The splash screen displays while fetching the user profile, and the login page appears even when attempting to navigate to the home screen using theLinkbutton.

By default Supabase Auth requires email verification before a session is created for the user. To support email verification you need toimplement deep link handling!

While testing, you can disable email confirmation in yourproject's email auth provider settings.


### Integrate social authentication#
Now integrate social authentication with Supabase Auth, starting with Apple authentication.
If you only need to implement Google authentication, you can skip to theGoogle authenticationsection.


### Apple authentication#
Start by adding the button inside the login screen:

```javascript
1...2import AppleSignInButton from '@/components/social-auth-buttons/apple/apple-sign-in-button';3...4export default function LoginScreen() {5  return (6    <>7      <Stack.Screen options={{ title: 'Login' }} />8      <ThemedView style={styles.container}>9        ...10        <AppleSignInButton />11        ...12      </ThemedView>13    </>14  );15}16...
```

For Apple authentication, you can choose between:

- Invertase's React Native Apple Authentication library- that supports iOS, Android
- react-apple-signin-auth- that supports Web, also suggested by Invertase
- Expo's AppleAuthentication library- that supports iOS only

For either option, you need to obtain a Service ID from theApple Developer Console.

To enable Apple sign-up on Android and Web, you also need to register the tunnelled URL (e.g.,https://arnrer1-anonymous-8081.exp.direct) obtained by running:

```javascript
1npx expo start --tunnel
```

And add it to theRedirect URLsfield inyour Supabase dashboard Authentication configuration.

For more information, follow theSupabase Login with Appleguide.


### Prerequisites#
Before proceeding, ensure you have followed the Invertase prerequisites documented in theInvertase Initial Setup Guideand theInvertase Android Setup Guide.

You need to add two new environment variables to the.envfile:

```javascript
1EXPO_PUBLIC_APPLE_AUTH_SERVICE_ID="YOUR_APPLE_AUTH_SERVICE_ID"2EXPO_PUBLIC_APPLE_AUTH_REDIRECT_URI="YOUR_APPLE_AUTH_REDIRECT_URI"
```


### iOS#
Install the@invertase/react-native-apple-authenticationlibrary:

```javascript
1npx expo install @invertase/react-native-apple-authentication
```

Then create the iOS specific button componentAppleSignInButton:

```javascript
1import { supabase } from '@/lib/supabase';2import { AppleButton, appleAuth } from '@invertase/react-native-apple-authentication';3import type { SignInWithIdTokenCredentials } from '@supabase/supabase-js';4import { router } from 'expo-router';5import { Platform } from 'react-native';67async function onAppleButtonPress() {8  // Performs login request9  const appleAuthRequestResponse = await appleAuth.performRequest({10    requestedOperation: appleAuth.Operation.LOGIN,11    // Note: it appears putting FULL_NAME first is important, see issue #29312    requestedScopes: [appleAuth.Scope.FULL_NAME, appleAuth.Scope.EMAIL],13  });1415  // Get the current authentication state for user16  // Note: This method must be tested on a real device. On the iOS simulator it always throws an error.17  const credentialState = await appleAuth.getCredentialStateForUser(appleAuthRequestResponse.user);1819  console.log('Apple sign in successful:', { credentialState, appleAuthRequestResponse });2021  if (credentialState === appleAuth.State.AUTHORIZED && appleAuthRequestResponse.identityToken && appleAuthRequestResponse.authorizationCode) {22    const signInWithIdTokenCredentials: SignInWithIdTokenCredentials = {23      provider: 'apple',24      token: appleAuthRequestResponse.identityToken,25      nonce: appleAuthRequestResponse.nonce,26      access_token: appleAuthRequestResponse.authorizationCode,27    };2829    const { data, error } = await supabase.auth.signInWithIdToken(signInWithIdTokenCredentials);3031    if (error) {32      console.error('Error signing in with Apple:', error);33    }3435    if (data) {36      console.log('Apple sign in successful:', data);37      router.navigate('/(tabs)/explore');38    }39  }40}4142export default function AppleSignInButton() {43  if (Platform.OS !== 'ios') { return <></>; }4445  return <AppleButton46    buttonStyle={AppleButton.Style.BLACK}47    buttonType={AppleButton.Type.SIGN_IN}48    style={{ width: 160, height: 45 }}49    onPress={() => onAppleButtonPress()}50  />;51}
```

To test functionality on the simulator, remove thegetCredentialStateForUsercheck:

```javascript
1...2const credentialState = await appleAuth.getCredentialStateForUser(appleAuthRequestResponse.user);3...
```

Enable the Apple authentication capability in iOS:

```javascript
1{2  "expo": {3    ...4    "ios": {5      ...6      "usesAppleSignIn": true7      ...8    },9    ...10  }11}
```

Add the capabilities to theInfo.plistfile by following theExpo documentation.

Before testing the app, if you've already built the iOS app, clean the project artifacts:

```javascript
1npx react-native-clean-project clean-project-auto
```

If issues persist, try completely cleaning the cache, as reported by many users in thisclosed issue.

Finally, update the iOS project by installing the Pod library and running the Expo prebuild command:

```javascript
1cd ios2pod install3cd ..4npx expo prebuild
```

Now test the application on a physical device:

```javascript
1npx expo run:ios --no-build-cache --device
```

You should see the login screen with the Apple authentication button.

If you get stuck while working through this guide, refer to thefull Invertase example on GitHub.


### Android#
Install the required libraries:

```javascript
1npx expo install @invertase/react-native-apple-authentication react-native-get-random-values uuid
```

Next, create the Android-specificAppleSignInButtoncomponent:

```javascript
1import { supabase } from '@/lib/supabase';2import { appleAuthAndroid, AppleButton } from '@invertase/react-native-apple-authentication';3import { SignInWithIdTokenCredentials } from '@supabase/supabase-js';4import { Platform } from 'react-native';5import 'react-native-get-random-values';6import { v4 as uuid } from 'uuid';78async function onAppleButtonPress() {9  // Generate secure, random values for state and nonce10  const rawNonce = uuid();11  const state = uuid();1213  // Configure the request14  appleAuthAndroid.configure({15    // The Service ID you registered with Apple16    clientId: process.env.EXPO_PUBLIC_APPLE_AUTH_SERVICE_ID ?? '',1718    // Return URL added to your Apple dev console. We intercept this redirect, but it must still match19    // the URL you provided to Apple. It can be an empty route on your backend as it's never called.20    redirectUri: process.env.EXPO_PUBLIC_APPLE_AUTH_REDIRECT_URI ?? '',2122    // The type of response requested - code, id_token, or both.23    responseType: appleAuthAndroid.ResponseType.ALL,2425    // The amount of user information requested from Apple.26    scope: appleAuthAndroid.Scope.ALL,2728    // Random nonce value that will be SHA256 hashed before sending to Apple.29    nonce: rawNonce,3031    // Unique state value used to prevent CSRF attacks. A UUID will be generated if nothing is provided.32    state,33  });3435  // Open the browser window for user sign in36  const credentialState = await appleAuthAndroid.signIn();37  console.log('Apple sign in successful:', credentialState);3839  if (credentialState.id_token && credentialState.code && credentialState.nonce) {40    const signInWithIdTokenCredentials: SignInWithIdTokenCredentials = {41      provider: 'apple',42      token: credentialState.id_token,43      nonce: credentialState.nonce,44      access_token: credentialState.code,45    };4647    const { data, error } = await supabase.auth.signInWithIdToken(signInWithIdTokenCredentials);4849    if (error) {50      console.error('Error signing in with Apple:', error);51    }5253    if (data) {54      console.log('Apple sign in successful:', data);55    }56  }57}5859export default function AppleSignInButton() {60  if (Platform.OS !== 'android' || appleAuthAndroid.isSupported !== true) { return <></> }6162  return <AppleButton63    buttonStyle={AppleButton.Style.BLACK}64    buttonType={AppleButton.Type.SIGN_IN}65    onPress={() => onAppleButtonPress()}66  />;67}
```

You should now be able to test the authentication by running it on a physical device or simulator:

```javascript
1npx expo run:android --no-build-cache
```


### Google authentication#
Start by adding the button to the login screen:

```javascript
1...2import GoogleSignInButton from '@/components/social-auth-buttons/google/google-sign-in-button';3...4export default function LoginScreen() {5  return (6    <>7      <Stack.Screen options={{ title: 'Login' }} />8      <ThemedView style={styles.container}>9        ...10        <GoogleSignInButton />11        ...12      </ThemedView>13    </>14  );15}16...
```

For Google authentication, you can choose between the following options:

- GN Google Sign In Premium- that supports iOS, Android, and Web by using the latest Google's One Tap sign-in (butit requires a subscription)
- @react-oauth/google- that supports Web (so it's not a good option for mobile, but it works)
- Relying on the``signInWithOAuthfunction of the Supabase Auth - that also supports iOS, Android and Web (useful also to manage any other OAuth provider)

TheGN Google Sign In Freedoesn't support iOS or Android, asit doesn't allow to pass a custom nonceto the sign-in request.

For either option, you need to obtain a Web Client ID from the Google Cloud Engine, as explained in theGoogle Sign Inguide.

This guide only uses the@react-oauth/google@latestoption for the Web, and thesignInWithOAuthfor the mobile platforms.

Before proceeding, add a new environment variable to the.envfile:

```javascript
1EXPO_PUBLIC_GOOGLE_AUTH_WEB_CLIENT_ID="YOUR_GOOGLE_AUTH_WEB_CLIENT_ID"
```

Install the@react-oauth/googlelibrary:

```javascript
1npx expo install @react-oauth/google
```

Enable theexpo-web-browserplugin inapp.json:

```javascript
1{2  "expo": {3    ...4    "plugins": {5      ...6      [7        "expo-web-browser",8        {9          "experimentalLauncherActivity": false10        }11      ]12      ...13    },14    ...15  }16}
```

Then create the iOS specific button componentGoogleSignInButton:

```javascript
1import { supabase } from '@/lib/supabase';2import { CredentialResponse, GoogleLogin, GoogleOAuthProvider } from '@react-oauth/google';3import { SignInWithIdTokenCredentials } from '@supabase/supabase-js';4import { useEffect, useState } from 'react';56import 'react-native-get-random-values';78export default function GoogleSignInButton() {910  // Generate secure, random values for state and nonce11  const [nonce, setNonce] = useState('');12  const [sha256Nonce, setSha256Nonce] = useState('');1314  async function onGoogleButtonSuccess(authRequestResponse: CredentialResponse) {15    console.debug('Google sign in successful:', { authRequestResponse });16    if (authRequestResponse.clientId && authRequestResponse.credential) {17      const signInWithIdTokenCredentials: SignInWithIdTokenCredentials = {18        provider: 'google',19        token: authRequestResponse.credential,20        nonce: nonce,21      };2223      const { data, error } = await supabase.auth.signInWithIdToken(signInWithIdTokenCredentials);2425      if (error) {26        console.error('Error signing in with Google:', error);27      }2829      if (data) {30        console.log('Google sign in successful:', data);31      }32    }33  }3435  function onGoogleButtonFailure() {36    console.error('Error signing in with Google');37  }3839  useEffect(() => {40    function generateNonce(): string {41      const array = new Uint32Array(1);42      window.crypto.getRandomValues(array);43      return array[0].toString();44    }4546    async function generateSha256Nonce(nonce: string): Promise<string> {47      const buffer = await window.crypto.subtle.digest('sha-256', new TextEncoder().encode(nonce));48      const array = Array.from(new Uint8Array(buffer));49      return array.map(b => b.toString(16).padStart(2, '0')).join('');50    }5152    let nonce = generateNonce();53    setNonce(nonce);5455    generateSha256Nonce(nonce)56      .then((sha256Nonce) => { setSha256Nonce(sha256Nonce) });57  }, []);5859  return (60    <GoogleOAuthProvider61      clientId={process.env.EXPO_PUBLIC_GOOGLE_AUTH_WEB_CLIENT_ID ?? ''}62      nonce={sha256Nonce}63    >64      <GoogleLogin65        nonce={sha256Nonce}66        onSuccess={onGoogleButtonSuccess}67        onError={onGoogleButtonFailure}68        useOneTap={true}69        auto_select={true}70      />71    </GoogleOAuthProvider>72  );73}
```

Test the authentication in your browser using the tunnelled HTTPS URL:

```javascript
1npx expo start --tunnel
```

To allow the Google Sign In to work, as you did before for Apple, you need to register the tunnelled URL (e.g.,https://arnrer1-anonymous-8081.exp.direct) obtained to the Authorized JavaScript origins list of yourGoogle Cloud Console's OAuth 2.0 Client IDsconfiguration.


================================================================================


# Rate limits
Source: https://supabase.com/docs/guides/auth/rate-limits

Rate limits


### Rate limits protect your services from abuse
Rate limits protect your services from abuse

Supabase Auth enforces rate limits on endpoints to prevent abuse. Some rate limits arecustomizable.

You can also manage rate limits using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Get current rate limits6curl -X GET "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  | jq 'to_entries | map(select(.key | startswith("rate_limit_"))) | from_entries'910# Update rate limits11curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \12  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \13  -H "Content-Type: application/json" \14  -d '{15    "rate_limit_anonymous_users": 10,16    "rate_limit_email_sent": 10,17    "rate_limit_sms_sent": 10,18    "rate_limit_verify": 10,19    "rate_limit_token_refresh": 10,20    "rate_limit_otp": 10,21    "rate_limit_web3": 1022  }'
```


### Footnotes#
The rate limit is only applied on/auth/v1/userif this endpoint is called to update the user's email address.↩

The rate limit is only applied on/auth/v1/signupif this endpoint is called without passing in an email or phone number in the request body.↩


================================================================================


# Redirect URLs
Source: https://supabase.com/docs/guides/auth/redirect-urls

Redirect URLs


### Set up redirect urls with Supabase Auth.
Set up redirect urls with Supabase Auth.


### Overview#
Supabase Auth allows you to control how theuser sessionsare handled by your application.

Looking for OAuth client redirect URIs?

This guide covers redirect URLs for users signingintoyour application (using social providers like Google, GitHub, etc.). If you're setting up your Supabase project as anOAuth 2.1 providerfor third-party applications, see theOAuth Server Redirect URI configurationinstead.

When usingpasswordless sign-insorthird-party providers, the Supabase client library provides aredirectToparameter to specify where to redirect the user after authentication. The URL inredirectToshould match theRedirect URLslist configuration.

To configure allowed redirect URLs, go to theURL Configurationpage. Once you've added necessary URLs, you can use the URL you want the user to be redirected to in theredirectToparameter.

The Site URL inURL Configurationdefines thedefault redirect URLwhen noredirectTois specified in the code. Change this fromhttp://localhost:3000to your production URL (e.g.,https://example.com). This setting is critical for email confirmations and password resets.

When usingSign in with Web3, the message signed by the user in the Web3 wallet application will indicate the URL on which the signature took place. Supabase Auth will reject messages that are signed for URLs that are not on the allowed list.

In local development or self-hosted projects, use theconfiguration file. See below for more information on configuringSITE_URLwhen deploying to Vercel or Netlify.


### Use wildcards in redirect URLs#
Supabase allows you to specify wildcards when adding redirect URLs to theallow list. You can use wildcard match patterns to support preview URLs from providers like Netlify and Vercel.

The separator characters in a URL are defined as.and/. Usethis toolto test your patterns.

While the "globstar" (**) is useful for local development and preview URLs, we recommend setting the exact redirect URL path for your site URL in production.


### Redirect URL examples with wildcards#

### Netlify preview URLs#
For deployments with Netlify, set theSITE_URLto your official site URL. Add the following additional redirect URLs for local development and deployment previews:

- http://localhost:3000/**
- https://**--my_org.netlify.app/**


### Vercel preview URLs#
For deployments with Vercel, set theSITE_URLto your official site URL. Add the following additional redirect URLs for local development and deployment previews:

- http://localhost:3000/**
- https://*-<team-or-account-slug>.vercel.app/**

Vercel provides an environment variable for the URL of the deployment calledNEXT_PUBLIC_VERCEL_URL. See theVercel docsfor more details. You can use this variable to dynamically redirect depending on the environment. You should also set the value of the environment variable called NEXT_PUBLIC_SITE_URL, this should be set to your site URL in production environment to ensure that redirects function correctly.

```javascript
1const getURL = () => {2  let url =3    process?.env?.NEXT_PUBLIC_SITE_URL ?? // Set this to your site URL in production env.4    process?.env?.NEXT_PUBLIC_VERCEL_URL ?? // Automatically set by Vercel.5    'http://localhost:3000/'6  // Make sure to include `https://` when not localhost.7  url = url.startsWith('http') ? url : `https://${url}`8  // Make sure to include a trailing `/`.9  url = url.endsWith('/') ? url : `${url}/`10  return url11}1213const { data, error } = await supabase.auth.signInWithOAuth({14  provider: 'github',15  options: {16    redirectTo: getURL(),17  },18})
```


### Email templates when usingredirectTo#
When using aredirectTooption, you may need to replace the{{ .SiteURL }}with{{ .RedirectTo }}in your email templates. See theEmail Templates guidefor more information.

For example, change the following:

```javascript
1<!-- Old -->2<a href="{{ .SiteURL }}/auth/confirm?token_hash={{ .TokenHash }}&type=email">Confirm your mail</a>34<!-- New -->5<a href="{{ .RedirectTo }}/auth/confirm?token_hash={{ .TokenHash }}&type=email"6  >Confirm your mail</a7>
```


### Mobile deep linking URIs#
For mobile applications you can use deep linking URIs. For example, for yourSITE_URLyou can specify something likecom.supabase://login-callback/and for additional redirect URLs something likecom.supabase.staging://login-callback/if needed.

Read more about deep linking and find code examples for different frameworkshere.


### Error handling#
When authentication fails, the user will still be redirected to the redirect URL provided. However, the error details will be returned as query fragments in the URL. You can parse these query fragments and show a custom error message to the user. For example:

```javascript
1const params = new URLSearchParams(window.location.hash.slice())23if (params.get('error_code').startsWith('4')) {4  // show error message if error is a 4xx error5  window.alert(params.get('error_description'))6}
```


================================================================================


# Server-Side Rendering
Source: https://supabase.com/docs/guides/auth/server-side

Server-Side Rendering


### How SSR works with Supabase Auth.
How SSR works with Supabase Auth.

SSR frameworks move rendering and data fetches to the server, to reduce client bundle size and execution time.

Supabase Auth is fully compatible with SSR. You need to make a few changes to the configuration of your Supabase client, to store the user session in cookies instead of local storage. After setting up your Supabase client, follow the instructions for any flow in the How-To guides.

Make sure to use the PKCE flow instructions where those differ from the implicit flow instructions. If no difference is mentioned, don't worry about this.


### @supabase/ssr#
We have developed an@supabase/ssrpackage to make setting up the Supabase client as simple as possible. This package is currently in beta. Adoption is recommended but be aware that the API is still unstable and may have breaking changes in the future.


### Framework quickstarts#
Next.js

SvelteKit


================================================================================


# Advanced guide
Source: https://supabase.com/docs/guides/auth/server-side/advanced-guide

Advanced guide


### Details about SSR Auth flows and implementation for advanced users.
Details about SSR Auth flows and implementation for advanced users.

When a user authenticates with Supabase Auth, two pieces of information are issued by the server:

The default behavior if you're not using SSR is to store this information in local storage. Local storage isn't accessible by the server, so for SSR, the tokens instead need to be stored in a secure cookie. The cookie can then be passed back and forth between your app code in the client and your app code in the server.

If you're not using SSR, you might also be using theimplicit flowto get the access and refresh tokens. The server can't access the tokens in this flow, so for SSR, you should change to thePKCE flow. You can change the flow type when initiating your Supabase client if your client library provides this option.

In the@supabase/ssrpackage, Supabase clients are initiated to use the PKCE flow by default. They are also automatically configured to handle the saving and retrieval of session information in cookies.


### How it works#
In the PKCE flow, a redirect is made to your app, with an Auth Code contained in the URL. When you exchange this code usingexchangeCodeForSession, you receive the session information, which contains the access and refresh tokens.

To maintain the session, these tokens must be stored in a storage medium securely shared between client and server, which is traditionally cookies. Whenever the session is refreshed, the auth and refresh tokens in the shared storage medium must be updated. Supabase client libraries provide a customizablestorageoption when a client is initiated, allowing you to change where tokens are stored.


### Frequently asked questions#

### No session on the server side with Next.js route prefetching?#
When you use route prefetching in Next.js using<Link href="/...">components or theRouter.push()APIs can send server-side requests before the browser processes the access and refresh tokens. This means that those requests may not have any cookies set and your server code will render unauthenticated content.

To improve experience for your users, we recommend redirecting users to one specific page after sign-in that does not include any route prefetching from Next.js. Once the Supabase client library running in the browser has obtained the access and refresh tokens from the URL fragment, you can send users to any pages that use prefetching.


### How do I make the cookiesHttpOnly?#
This is not necessary. Both the access token and refresh token are designed to be passed around to different components in your application. The browser-based side of your application needs access to the refresh token to properly maintain a browser session anyway.


### My server is getting invalid refresh token errors. What's going on?#
It is likely that the refresh token sent from the browser to your server is stale. Make sure theonAuthStateChangelistener callback is free of bugs and is registered relatively early in your application's lifetime

When you receive this error on the server-side, try to defer rendering to the browser where the client library can access an up-to-date refresh token and present the user with a better experience.


### Should I set a shorterMax-Ageparameter on the cookies?#
TheMax-AgeorExpirescookie parameters only control whether the browser sends the value to the server. Since a refresh token represents the long-lived authentication session of the user on that browser, setting a shortMax-AgeorExpiresparameter on the cookies only results in a degraded user experience.

The only way to ensure that a user has logged out or their session has ended is to get the user's details withgetUser(). ThegetClaims()method only checks local JWT validation (signature and expiration), but it doesn't verify with the auth server whether the session is still valid or if the user has logged out server-side.


### What should I use for theSameSiteproperty?#
Make sure youunderstand the behavior of the property in different situationsas some properties can degrade the user experience.

A good default is to useLaxwhich sends cookies when users are navigating to your site. Cookies typically require theSecureattribute, which only sends them over HTTPS. However, this can be a problem when developing onlocalhost.


### Can I use server-side rendering with a CDN or cache?#
Yes, but you need to be careful to include at least the refresh token cookie value in the cache key. Otherwise you may be accidentally serving pages with data belonging to different users!

Also be sure you set proper cache control headers. We recommend invalidating cache keys every hour or less.


### Which authentication flows have PKCE support?#
At present, PKCE is supported on the Magic Link, OAuth, Sign Up, and Password Recovery routes. These correspond to thesignInWithOtp,signInWithOAuth,signUp, andresetPasswordForEmailmethods on the Supabase client library. When using PKCE with Phone and Email OTPs, there is no behavior change with respect to the implicit flow - an access token will be returned in the body when a request is successful.


================================================================================


# Creating a Supabase client for SSR
Source: https://supabase.com/docs/guides/auth/server-side/creating-a-client

Creating a Supabase client for SSR


### Configure your Supabase client to use cookies
Configure your Supabase client to use cookies

To use Server-Side Rendering (SSR) with Supabase, you need to configure your Supabase client to use cookies. The@supabase/ssrpackage helps you do this for JavaScript/TypeScript applications.


### Install#
Install the@supabase/supabase-jsand@supabase/ssrhelper packages:

```javascript
1npm install @supabase/supabase-js @supabase/ssr
```


### Set environment variables#
Create a.env.localfile in the project root directory. In the file, set the project's Supabase URL and Key:

You can also get the Project URL and key fromthe project'sConnectdialog.

Supabase is changing the way keys work to improve project security and developer experience. You canread the full announcement, but in the transition period, you can use both the currentanonandservice_rolekeys and the new publishable key with the formsb_publishable_xxxwhich will replace the older keys.

In most cases, you can get the correct key fromthe Project'sConnectdialog, but if you want a specific key, you can find all keys inthe API Keys section of a Project's Settings page:

- For legacy keys, copy theanonkey for client-side operations and theservice_rolekey for server-side operations from theLegacy API Keystab.
- For new keys, open theAPI Keystab, if you don't have a publishable key already, clickCreate new API Keys, and copy the value from thePublishable keysection.

Read the API keys docsfor a full explanation of all key types and their uses.

```javascript
1NEXT_PUBLIC_SUPABASE_URL=supabase_project_url2NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY=supabase_publishable_key
```


### Create a client#
You need setup code to configure a Supabase client to use cookies. Once you have the utility code, you can use thecreateClientutility functions to get a properly configured Supabase client.

Use the browser client in code that runs on the browser, and the server client in code that runs on the server.


### Write utility functions to create Supabase clients#
To access Supabase from a Next.js app, you need 2 types of Supabase clients:

Since Next.js Server Components can't write cookies, you need aProxyto refresh expired Auth tokens and store them.

The Proxy is responsible for:

Create alib/supabasefolder at the root of your project, or inside the./srcfolder if you are using one, with a file for each type of client. Then copy the lib utility functions for each client type.

```javascript
1import { createBrowserClient } from '@supabase/ssr'23export function createClient() {4  return createBrowserClient(5    process.env.NEXT_PUBLIC_SUPABASE_URL!,6    process.env.NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY!7  )8}
```


### Hook up proxy#
The code adds amatcherso the Proxy doesn't run on routes that don't access Supabase.

Be careful when protecting pages. The server gets the user session from the cookies, which can be spoofed by anyone.

Always usesupabase.auth.getClaims()to protect pages and user data.

Nevertrustsupabase.auth.getSession()inside server code such as Proxy. It isn't guaranteed to revalidate the Auth token.

It's safe to trustgetClaims()because it validates the JWT signature against the project's published public keys every time.

```javascript
1import { type NextRequest } from "next/server"2import { updateSession } from "@/lib/supabase/proxy"34export async function proxy(request: NextRequest) {5  return await updateSession(request)6}78export const config = {9  matcher: [10    /*11     * Match all request paths except for the ones starting with:12     * - _next/static (static files)13     * - _next/image (image optimization files)14     * - favicon.ico (favicon file)15     * Feel free to modify this pattern to include more paths.16     */17    "/((?!_next/static|_next/image|favicon.ico|.*\\.(?:svg|png|jpg|jpeg|gif|webp)$).*)",18  ],19}
```


### Congratulations#
You're done! To recap, you've successfully:

- Called Supabase from a Server Action.
- Called Supabase from a Server Component.
- Set up a Supabase client utility to call Supabase from a Client Component. You can use this if you need to call Supabase from a Client Component, for example to set up a realtime subscription.
- Set up Proxy to automatically refresh the Supabase Auth session.

You can now use any Supabase features from your client or server code!


### Next steps#
- ImplementAuthentication using Email and Password
- ImplementAuthentication using OAuth
- Learn more about SSR


================================================================================


# Migrating to the SSR package from Auth Helpers
Source: https://supabase.com/docs/guides/auth/server-side/migrating-to-ssr-from-auth-helpers

Migrating to the SSR package from Auth Helpers

The newssrpackage takes the core concepts of the Auth Helpers and makes them available to any server language or framework. This page will guide you through migrating from the Auth Helpers package tossr.


### Replacing Supabase packages#
```javascript
1npm uninstall @supabase/auth-helpers-nextjs
```

```javascript
1npm install @supabase/ssr
```


### Creating a client#
The newssrpackage exports two functions for creating a Supabase client. ThecreateBrowserClientfunction is used in the client, and thecreateServerClientfunction is used in the server.

Read theCreating a clientpage for examples of creating a client in your frameworkand our migration guide.


### Next steps#
- ImplementAuthentication using Email and Password
- ImplementAuthentication using OAuth
- Learn more about SSR


================================================================================


# User sessions
Source: https://supabase.com/docs/guides/auth/sessions

User sessions

Supabase Auth provides fine-grained control over your user's sessions.

Some security sensitive applications, or those that need to be SOC 2, HIPAA, PCI-DSS or ISO27000 compliant will require some sort of additional session controls to enforce timeouts or provide additional security guarantees. Supabase Auth makes it easy to build compliant applications.


### What is a session?#
A session is created when a user signs in. By default, it lasts indefinitely and a user can have an unlimited number of active sessions on as many devices.

A session is represented by the Supabase Auth access token in the form of a JWT, and a refresh token which is a unique string.

Access tokens are designed to be short lived, usually between 5 minutes and 1 hour while refresh tokens never expire but can only be used once. You can exchange a refresh token only once to get a new access and refresh token pair.

This process is calledrefreshing the session.

A session terminates, depending on configuration, when:

- The user clicks sign out.
- The user changes their password or performs a security sensitive action.
- It times out due to inactivity.
- It reaches its maximum lifetime.
- A user signs in on another device.


### Access token (JWT) claims#
Every access token contains asession_idclaim, a UUID, uniquely identifying the session of the user. You can correlate this ID with the primary key of theauth.sessionstable.


### Initiating a session#
A session is initiated when a user signs in. The session is stored in theauth.sessionstable, and your app should receive the access and refresh tokens.

There are two flows for initiating a session and receiving the tokens:

- Implicit flow
- PKCE flow


### Limiting session lifetime and number of allowed sessions per user#
This feature is only available on Pro Plans and up.

Supabase Auth can be configured to limit the lifetime of a user's session. By default, all sessions are active until the user signs out or performs some other action that terminates a session.

In some applications, it's useful or required for security to ensure that users authenticate often, or that sessions are not left active on devices for too long.

There are three ways to limit the lifetime of a session:

- Time-boxed sessions, which terminate after a fixed amount of time.
- Set an inactivity timeout, which terminates sessions that haven't been refreshed within the timeout duration.
- Enforce a single-session per user, which only keeps the most recently active session.

To make sure that users are required to re-authenticate periodically, you can set a positive value for theTime-box user sessionsoption in theAuth settingsfor your project.

To make sure that sessions expire after a period of inactivity, you can set a positive duration for theInactivity timeoutoption in theAuth settings.

You can also enforce only one active session per user per device or browser. When this is enabled, the session from the most recent sign in will remain active, while the rest are terminated. Enable this via theSingle session per useroption in theAuth settings.

Sessions are not proactively destroyed when you change these settings, but rather the check is enforced whenever a session is refreshed next. This can confuse developers because the actual duration of a session is the configured timeout plus the JWT expiration time. For single session per user, the effect will only be noticed at intervals of the JWT expiration time. Make sure you adjust this setting depending on your needs. We do not recommend going below 5 minutes for the JWT expiration time.

Otherwise sessions are progressively deleted from the database 24 hours after they expire, which prevents you from causing a high load on your project by accident and allows you some freedom to undo changes without adversely affecting all users.


### Frequently asked questions#

### What are recommended values for access token (JWT) expiration?#
Most applications should use the default expiration time of 1 hour. This can be customized in your project'sAuth settingsin the Advanced Settings section.

Setting a value over 1 hour is generally discouraged for security reasons, but it may make sense in certain situations.

Values below 5 minutes, and especially below 2 minutes, should not be used in most situations because:

- The shorter the expiration time, the more frequently refresh tokens are used, which increases the load on the Auth server.
- Time is not absolute. Servers can often be off sync for tens of seconds, but user devices like laptops, desktops or mobile devices can sometimes be off by minutes or even hours. Having too short expiration time can cause difficult-to-debug errors due to clock skew.
- Supabase's client libraries always try to refresh the session ahead of time, which won't be possible if the expiration time is too short.
- Access tokens should generally be valid for at least as long as the longest running request in your application. This helps you avoid issues where the access token becomes invalid midway through processing.


### What is refresh token reuse detection and what does it protect from?#
As your users continue using your app, refresh tokens are being constantly exchanged for new access tokens.

The general rule is that a refresh token can only be used once. However, strictly enforcing this can cause certain issues to arise. There are two exceptions to this design to prevent the early and unexpected termination of user's sessions:

- A refresh token can be used more than once within a defined reuse interval. By default this is 10 seconds and we do not recommend changing this value. This exception is granted for legitimate situations such as:Using server-side rendering where the same refresh token needs to be reused on the server and soon after on the clientTo allow some leeway for bugs or issues with serializing access to the refresh token request
- If the parent of the currently active refresh token for the user's session is being used, the active token will be returned. This exception solves an important and often common situation:All clients such as browsers, mobile or desktop apps, and even some servers are inherently unreliable due to network issues. A request does not indicate that they received a response or even processed the response they received.If a refresh token is revoked after being used only once, and the response wasn't received and processed by the client, when the client comes back online, it will attempt to use the refresh token that was already used. Since this might happen outside of the reuse interval, it can cause sudden and unexpected session termination.

- Using server-side rendering where the same refresh token needs to be reused on the server and soon after on the client
- To allow some leeway for bugs or issues with serializing access to the refresh token request

- All clients such as browsers, mobile or desktop apps, and even some servers are inherently unreliable due to network issues. A request does not indicate that they received a response or even processed the response they received.
- If a refresh token is revoked after being used only once, and the response wasn't received and processed by the client, when the client comes back online, it will attempt to use the refresh token that was already used. Since this might happen outside of the reuse interval, it can cause sudden and unexpected session termination.

Should the reuse attempt not fall under these two exceptions, the whole session is regarded as terminated and all refresh tokens belonging to it are marked as revoked. You can disable this behavior in the Advanced Settings of theAuth settingspage, though it is generally not recommended.

The purpose of this mechanism is to guard against potential security issues where a refresh token could have been stolen from the user, for example by exposing it accidentally in logs that leak (like logging cookies, request bodies or URL params) or via vulnerable third-party servers. It does not guard against the case where a user's session is stolen from their device.


### What are the benefits of using access and refresh tokens instead of traditional sessions?#
Traditionally user sessions were implemented by using a unique string stored in cookies that identified the authorization that the user had on a specific browser. Applications would use this unique string to constantly fetch the attached user information on every API call.

This approach has some tradeoffs compared to using a JWT-based approach:

- If the authentication server or its database crashes or is unavailable for even a few seconds, the whole application goes down. Scheduling maintenance or dealing with transient errors becomes very challenging.
- A failing authentication server can cause a chain of failures across other systems and APIs, paralyzing the whole application system.
- All requests that require authentication has to be routed through the authentication, which adds an additional latency overhead to all requests.

Supabase Auth prefers a JWT-based approach using access and refresh tokens because session information is encoded within the short-lived access token, enabling transfer across APIs and systems without dependence on a central server's availability or performance. This approach enhances an application's tolerance to transient failures or performance issues. Furthermore, proactively refreshing the access token allows the application to function reliably even during significant outages.

It's better for cost optimization and scaling as well, as the authentication system's servers and database only handle traffic for this use case.


### How to ensure an access token (JWT) cannot be used after a user signs out#
Most applications rarely need such strong guarantees. Consider adjusting the JWT expiry time to an acceptable value. If this is still necessary, you should try to use this validation logic only for the most sensitive actions within your application.

When a user signs out, the sessions affected by the logout are removed from the database entirely. You can check that thesession_idclaim in the JWT corresponds to a row in theauth.sessionstable. If such a row does not exist, it means that the user has logged out.

Note that sessions are not proactively terminated when their maximum lifetime (time-box) or inactivity timeout are reached. These sessions are cleaned up progressively 24 hours after reaching that status. This allows you to tweak the values or roll back changes without causing unintended user friction.


### Using HTTP-only cookies to store access and refresh tokens#
This is possible, but only for apps that use the traditional server-only web app approach where all of the application logic is implemented on the server and it returns rendered HTML only.

If your app uses any client side JavaScript to build a rich user experience, using HTTP-Only cookies is not feasible since only your server will be able to read and refresh the session of the user. The browser will not have access to the access and refresh tokens.

Because of this, the Supabase JavaScript libraries provide only limited support. You can override thestorageoption when creating the Supabase clienton the serverto store the values in cookies or your preferred storage choice, for example:

```javascript
1import {  } from '@supabase/supabase-js'23const  = ('SUPABASE_URL', 'SUPABASE_PUBLISHABLE_KEY', {4  : {5    : {6      : () => {7        return .('FETCHED_COOKIE')8      },9      : () => {},10      : () => {},11    },12  },13})
```

ThecustomStorageObjectshould implement thegetItem,setItem, andremoveItemmethods from theStorageinterface. Async versions of these methods are also supported.

When using cookies to store access and refresh tokens, make sure that theExpiresorMax-Ageattributesof the cookies is set to a timestamp very far into the future. Browsers will clear the cookies, but the session will remain active in Supabase Auth. Therefore it's best to let Supabase Auth control the validity of these tokens and instruct the browser to always store the cookies indefinitely.


================================================================================


# Implicit flow
Source: https://supabase.com/docs/guides/auth/sessions/implicit-flow

Implicit flow


### About authenticating with implicit flow.
About authenticating with implicit flow.

The implicit flow is one of two ways that a user can authenticate and your app can receive the necessary access and refresh tokens.

The flow is an implementation detail handled for you by Supabase Auth, but understanding the difference between implicit andPKCE flowis important for understanding the difference between client-only and server-side auth.


### How it works#
After a successful signin, the user is redirected to your app with a URL that looks like this:

```javascript
1https://yourapp.com/...#access_token=<...>&refresh_token=<...>&...
```

The access and refresh tokens are contained in the URL fragment.

The client libraries:

- Detect this type of URL
- Extract the access token, refresh token, and some extra information
- Persist this information to local storage for further use by the library and your app


### Limitations#
The implicit flow only works on the client. Web browsers do not send the URL fragment to the server by design. This is a security feature:

- You may be hosting your single-page app on a third-party server. The third-party service shouldn't get access to your user's credentials.
- Even if the server is under your direct control,GETrequests and their full URLs are often logged. This approach avoids leaking credentials in request or access logs.

If you wish to obtain the access token and refresh token on a server, use thePKCE flow.


================================================================================


# PKCE flow
Source: https://supabase.com/docs/guides/auth/sessions/pkce-flow

PKCE flow


### About authenticating with PKCE flow.
About authenticating with PKCE flow.

The Proof Key for Code Exchange (PKCE) flow is one of two ways that a user can authenticate and your app can receive the necessary access and refresh tokens.

The flow is an implementation detail handled for you by Supabase Auth, but understanding the difference between PKCE andimplicit flowis important for understanding the difference between client-only and server-side auth.


### How it works#
After a successful verification, the user is redirected to your app with a URL that looks like this:

```javascript
1https://yourapp.com/...?code=<...>
```

Thecodeparameter is commonly known as the Auth Code and can be exchanged for an access token by callingexchangeCodeForSession(code).

For security purposes, the code has a validity of 5 minutes and can only be exchanged for an access token once. You will need to restart the authentication flow from scratch if you wish to obtain a new access token.

As the flow is run server side,localStoragemay not be available. You may configure the client library to use a custom storage adapter and an alternate backing storage such as cookies by setting thestorageoption to an object with the following methods:

```javascript
1const :  = {2    : () => {3    if (!()) {4        // Configure alternate storage5        return null6    }7    return ..()8    },9    : (, ) => {10    if (!()) {11        // Configure alternate storage here12        return13    }14    ..(, )15    },16    : () => {17    if (!()) {18        // Configure alternate storage here19        return20    }21    ..()22    },23}
```

You may also configure the client library to automatically exchange it for a session after a successful redirect. This can be done by setting thedetectSessionInUrloption totrue.

Putting it all together, your client library initialization may look like this:

```javascript
1const  = ('https://xyzcompany.supabase.co', 'publishable-or-anon-key', {2  // ...3  : {4    // ...5    : true,6    : 'pkce',7    : {8      : () => .('FETCHED_TOKEN'),9      : () => {},10      : () => {},11    },12  },13  // ...14})
```


### Limitations#
Behind the scenes, the code exchange requires a code verifier. Both the code in the URL and the code verifier are sent back to the Auth server for a successful exchange.

The code verifier is created and stored locally when the Auth flow is first initiated. That means the code exchange must be initiated on the same browser and device where the flow was started.


### Resources#
- OAuth 2.0 guideto PKCE flow


================================================================================


# JWT Signing Keys
Source: https://supabase.com/docs/guides/auth/signing-keys

JWT Signing Keys


### Best practices on managing keys used by Supabase Auth to create and verify JSON Web Tokens
Best practices on managing keys used by Supabase Auth to create and verify JSON Web Tokens

Supabase Auth continuously issues a new JWT for each user session, for as long as the user remains signed in. JWT signing keys provide fine grained control over this important process for the security of your application.

Before continuing check the comprehensive guide onSessionsfor all the details about how Auth creates tokens for a user's session. Read up onJWTsif you are not familiar with the basics.


### Overview#
When a JWT is issued by Supabase Auth, the key used to create itssignatureis known as the signing key. Supabase provides two systems for dealing with signing keys: the Legacy system based on the JWT secret, and the new Signing keys system.


### Benefits of the signing keys system#
We've designed the Signing keys system to address many problems the legacy system had. It goes hand-in-hand with thepublishable and secret API keys.


### Getting started#
You can start migrating away from the legacy JWT secret through the Supabase dashboard. This process does not cause downtime for your application.

- Make sure your app does not directly rely on the legacy JWT secret. If it's verifying every JWT against the legacy JWT secret (using a library likejose,jsonwebtokenor similar), continuing with the rotation might break those components.
- If you're usingEdge Functionsthat have the Verify JWT setting, continuing with the rotation might break your app. You will need to turn off this setting.
- In both cases, change or add code to your app or Edge Function that verifies the JWT. Use thesupabase.auth.getClaims()function or read more aboutVerifying a JWT from Supabaseon the best way to do this.

- If your access token expiry time is configured to be 1 hour, wait at least 1 hour and 15 minutes before revoking the legacy JWT secret -- now under thePreviously usedsection.
- This prevents currently active users from being forcefully signed out.
- In some situations, such as an active security incident you may want to revoke the legacy JWT secret immediately.


### Rotating and revoking keys#
Key rotation and revocation are one of the most important processes for maintaining the security of your project and applications. The signing keys system allows you to efficiently execute these without causing downtime of your app, a deficiency present in the legacy system. Below are some common reasons when and why you should consider key rotation and revocation.

Malicious actors abusing the legacy JWT secret, or imported private key

- The legacy JWT secret has been leaked in logs, committed to source control, or accidentally exposed in the frontend build of your application, a library, desktop or mobile app package, etc.
- You suspect that amember of your organizationhas lost control of their devices, and a malicious actor may have accessed the JWT secret via the Supabase dashboard or by accessing your application's backend configuration.
- You suspect that an ex-team-member of your organization may be a malicious actor, by abusing the power the legacy JWT secret provides.
- Make sure you also switch topublishable and secret API keysand disable theanonandservice_rolekeys.
- If you've imported a private key, and you're suspecting that this private key has been compromised on your end similarly.

Closer alignment to security best practices and compliance frameworks (SOC2, PCI-DSS, ISO27000, HIPAA, ...)

- It is always prudent to rotate signing keys at least once a year.
- Some security compliance frameworks strongly encourage or require frequent cryptographic key rotation.
- If you're using Supabase as part of a large enterprise, this may be required by your organization's security department.
- Creating muscle memory for the time you'll need to respond to an active security incident.

Changing key algorithm for technical reasons

- You may wish to switch signing algorithms due to compatibility problems or to simplify development on your end.


### Lifetime of a signing key#
A newly created key starts off as standby, before being rotated into in use (becoming the current key) while the existing current key becomes previously used.

At any point you can move a key from the previously used or revoked states back to being a standby key, and rotate to it. This gives you the confidence to revert back to an older key if you identify problems with the rotation, such as forgetting to update a component of your application that is relying on a specific key (for example, the legacy JWT secret).

Each action on a key is reversible (except permanent deletion).


### Public key discovery and caching#
When your signing keys use an asymmetric algorithm based onpublic-key cryptographySupabase Auth exposes the public key in the JSON Web Key Set discovery endpoint, for anyone to see. This is an important security feature allowing you to rotate and revoke keys without needing to deploy new versions of your app's backend infrastructure.

Access the currently trusted signing keys at the following endpoint:

```javascript
1GET https://project-id.supabase.co/auth/v1/.well-known/jwks.json
```

Note that this is secure as public keys are irreversible and can only be used to verify the signature of JSON Web Tokens, but not create new ones.

This discovery endpoint is cached by Supabase's edge servers for 10 minutes. Furthermore the Supabase client libraries may cache the keys in memory for an additional 10 minutes. Your application may be using different caching behavior if you're not relying only on the Supabase client library.

This multi-level cache is a trade-off allowing fast JWT verification without placing the Auth server in the hot path of your application, increasing its reliability and performance.

Importantly Supabase productsdo not rely on this cache, so stronger security guarantees are provided especially when keys are revoked. If your application only usesRow Level Securitypolicies and does not have any other backend components (such as APIs, Edge Functions, servers, etc.) key rotation and revocation are instantaneous.

Finally this multi-level cache is cleared every 20 minutes, or longer if you have a custom setup. Consider the following problems that may arise due to it:

- Urgent key revocation.If you are in a security incident where a signing key must be urgently revoked, due to the multi-level cache your application components may still trust and authenticate JWTs signed with the revoked key. Supabase products (Auth, Data API, Storage, Realtime)do not rely on this cache and revocation is instantaneous.Should this be an issue for you, ensure you've built a cache busting mechanism as part of your app's backend infrastructure.
- Quick key creation and rotation.If you're migrating away from the legacy JWT secret or when only using thesupabase.auth.getClaims()method this case is handled for you automatically. If you're verifying JWTs on your own, without the help of the Supabase client library, ensure thatall caches in your apphave picked up the newly created standby key before proceeding to rotation.


### Choosing the right signing algorithm#
To strike the right balance between performance, security and ease-of-use, JWT signing keys are based on capabilities available in theWeb Crypto API.

There is almost no benefit from using a JWT signed with a shared secret. Although it's computationally more efficient and verification is simpler to code by hand, using this approach can expose your project's data to significant security vulnerabilities or weaknesses.

Consider the following:

- Using a shared secret can make it more difficult to keep aligned with security compliance frameworks such as SOC2, PCI-DSS, ISO27000, HIPAA, etc.
- A shared secret that is in the hands of a malicious actor can be used to impersonate your users, give them access to privileged actions or data.
- It is difficult to detect or identify when or how a shared secret has been given to a malicious actor.
- Consider who might have even accidental access to the shared secret: systems, staff, devices (and their disk encryption and vulnerability patch status).
- A malicious actor can use a shared secretfar into the future, so lacking current evidence of compromise does not mean your data is secure.
- It can be very easy to accidentally leak the shared secret in publicly available source code such as in your website or frontend, mobile app package or other executable. This is especially true if you accidentally add the secret in environment variables prefixed withNEXT_PUBLIC_,VITE_,PUBLIC_or other conventions by web frameworks.
- Rotating shared secrets might require careful coordination to avoid downtime of your app.


### Frequently asked questions#

### Why is it not possible to extract the private key or shared secret from Supabase?#
You can only extract the legacy JWT secret. Once you've moved to using the JWT signing keys feature extracting of the private key or shared secret from Supabase is not possible. This ensures that no one in your organization is able to impersonate your users or gain privileged access to your project's data.

This guarantee provides your application with close alignment with security compliance frameworks (SOC2, PCI-DSS, ISO27000, HIPAA) and security best practices.


### How to create (mint) JWTs if access to the private key or shared secret is not possible?#
If you wish to make your own JWTs or have access to the private key or shared secret used by Supabase, you can create a new JWT signing key by importing a private key or setting a shared secret yourself.

Use theSupabase CLIto quickly and securely generate a private key ready for import:

```javascript
1supabase gen signing-key --algorithm ES256
```

Make sure you store this private key in a secure location, as it will not be extractable from Supabase.

To import the generated private key to your project, create anew standby keyfrom the dashboard:

```javascript
1{2  "kty": "EC",3  "kid": "3a18cfe2-7226-43b0-bbb4-7c5242f2406e",4  "d": "RDbwqThwtGP4WnvACvO_0nL0oMMSmMFSYMPosprlAog",5  "crv": "P-256",6  "x": "gyLVvp9dyEgylYH7nR2E2qdQ_-9Pv5i1tk7c2qZD4Nk",7  "y": "CD9RfYOTyjR5U-PC9UDlsthRpc7vAQQQ2FTt8UsX0fY"8}
```

Once imported, clickRotate keyto activate your new signing key. Any JWT signed by your old key will continue to be usable until your old signing key is manually revoked.

To mint a new JWT using the asymmetric signing key, you need to set the followingJWT headersto match your generated private key.

```javascript
1{2  "alg": "ES256",3  "kid": "3a18cfe2-7226-43b0-bbb4-7c5242f2406e",4  "typ": "JWT"5}
```

Thekidheader is used to identify your public key for verification. You must use the same value when importing on platform.

In addition, you need to provide the following custom claims as the JWT payload.

```javascript
1{2  "sub": "ef0493c9-3582-425f-a362-aef909588df7",3  "role": "authenticated",4  "exp": 17577494665}
```

- subis an optional UUID that uniquely identifies a user you want to impersonate inauth.userstable.
- rolemust be set to an existing Postgres role in your database, such asanon,authenticated, orservice_role.
- expmust be set to a timestamp in the future (seconds since 1970) when this token expires. Prefer shorter-lived tokens.

For simplicity, use the following CLI command to generate tokens with the desired header and payload.

```javascript
1supabase gen bearer-jwt --role authenticated --sub ef0493c9-3582-425f-a362-aef909588df7
```

Finally, you can use your newly minted JWT by setting theAuthorization: Bearer <JWT>header to allData API requests.

A separateapikeyheader is required to access your project's APIs. This can be apublishable, secret or the legacyanonorservice_rolekeys. Using your minted JWT is not possible in this header.


### Why is a 5 minute wait imposed when changing signing key states?#
Changing a JWT signing key's state sets off many changes inside the Supabase platform. To ensure a consistent setup, most actions that change the state of a JWT signing key are throttled for approximately 5 minutes.


### Why is deleting the legacy JWT secret disallowed?#
This is to ensure you have the ability, should you need it, to go back to the legacy JWT secret. In the future this capability will be allowed from the dashboard.


### Why does revoking the legacy JWT secret require disabling ofanonandservice_roleAPI keys?#
Unfortunatelyanonandservice_roleare not just API keys, but are also valid JSON Web Tokens, signed by the legacy JWT secret. Revoking the legacy JWT secret means that your application no longer trusts any JWT signed with it. Therefore before you revoke the legacy JWT secret, you must disable theanonandservice_roleto ensure a consistent security setup.


================================================================================


# Signing out
Source: https://supabase.com/docs/guides/auth/signout

Signing out


### Signing out a user
Signing out a user

Signing out a user works the same way no matter what method they used to sign in.

Call the sign out method from the client library. It removes the active session and clears Auth data from the storage medium.

```javascript
1async function () {2  const {  } = await ..()3}
```


### Sign out and scopes#
Supabase Auth allows you to specify three different scopes for when a user invokes thesign out APIin your application:

- global(default) when all sessions active for the user are terminated.
- localwhich only terminates the current session for the user but keep sessions on other devices or browsers active.
- othersto terminate all but the current session for the user.

You can invoke these by providing thescopeoption:

```javascript
1// defaults to the global scope2await ..()34// sign out from the current session only5await ..({ : 'local' })
```

Upon sign out, all refresh tokens and potentially other database objects related to the affected sessions are destroyed and the client library removes the session stored in the local storage medium.

Access Tokens of revoked sessions remain valid until their expiry time, encoded in theexpclaim. The user won't be immediately logged out and will only be logged out when the Access Token expires.


================================================================================


# Social Login
Source: https://supabase.com/docs/guides/auth/social-login

Social Login

Social Login (OAuth) is an open standard for authentication that allows users to log in to one website or application using their credentials from another website or application. OAuth allows users to grant third-party applications access to their online accounts without sharing their passwords.
OAuth is commonly used for things like logging in to a social media account from a third-party app. It is a secure and convenient way to authenticate users and share information between applications.


### Benefits#
There are several reasons why you might want to add social login to your applications:

- Improved user experience: Users can register and log in to your application using their existing social media accounts, which can be faster and more convenient than creating a new account from scratch. This makes it easier for users to access your application, improving their overall experience.
- Better user engagement: You can access additional data and insights about your users, such as their interests, demographics, and social connections. This can help you tailor your content and marketing efforts to better engage with your users and provide a more personalized experience.
- Increased security: Social login can improve the security of your application by leveraging the security measures and authentication protocols of the social media platforms that your users are logging in with. This can help protect against unauthorized access and account takeovers.

Improved user experience: Users can register and log in to your application using their existing social media accounts, which can be faster and more convenient than creating a new account from scratch. This makes it easier for users to access your application, improving their overall experience.

Better user engagement: You can access additional data and insights about your users, such as their interests, demographics, and social connections. This can help you tailor your content and marketing efforts to better engage with your users and provide a more personalized experience.

Increased security: Social login can improve the security of your application by leveraging the security measures and authentication protocols of the social media platforms that your users are logging in with. This can help protect against unauthorized access and account takeovers.


### Set up a social provider with Supabase Auth#
Supabase supports a suite of social providers. Follow these guides to configure a social provider for your platform.


### Provider tokens#
You can use the provider token and provider refresh token returned to make API calls to the OAuth provider. For example, you can use the Google provider token to access Google APIs on behalf of your user.

Supabase Auth does not manage refreshing the provider token for the user. Your application will need to use the provider refresh token to obtain a new provider token. If no provider refresh token is returned, then it could mean one of the following:

- The OAuth provider does not return a refresh token
- Additional scopes need to be specified in order for the OAuth provider to return a refresh token.

Provider tokens are intentionally not stored in your project's database. This is because provider tokens give access to potentially sensitive user data in third-party systems. Different applications have different needs, and one application's OAuth scopes may be significantly more permissive than another. If you want to use the provider token outside of the browser that completed the OAuth flow, it is recommended to send it to a trusted and secure server you control.


================================================================================


# Login with Apple
Source: https://supabase.com/docs/guides/auth/social-login/auth-apple

Login with Apple

Supabase Auth supports usingSign in with Appleon the web and in native apps for iOS, macOS, watchOS or tvOS.


### Overview#
To support Sign in with Apple, you need to configure theApple provider in the Supabase dashboardfor your project.

There are three general ways to use Sign in with Apple, depending on the application you're trying to build:

- Sign in on the web or in web-based appsUsing an OAuth flow initiated by Supabase Auth using theSign in with Apple REST API.UsingSign in with Apple JSdirectly in the browser, usually suitable for websites.
- Sign in natively inside iOS, macOS, watchOS or tvOS apps usingApple's Authentication Services

- Using an OAuth flow initiated by Supabase Auth using theSign in with Apple REST API.
- UsingSign in with Apple JSdirectly in the browser, usually suitable for websites.

In some cases you're able to use the OAuth flow within web-based native apps such as withReact Native,Expoor other similar frameworks. It is best practice to use native Sign in with Apple capabilities on those platforms instead.

When developing with Expo, you can test Sign in with Apple via the Expo Go app, in all other cases you will need to obtain anApple Developeraccount to enable the capability.

If you're using the OAuth flow (web, Flutter web, Kotlin non-iOS platforms), Apple requires you to generate a new secret key every 6 months using the signing key (.p8file). This is a critical maintenance task that will cause authentication failures if missed.

- Set a recurring calendar reminder for every 6 months to rotate your secret key
- Store the.p8file securely - you'll need it for each rotation
- If you lose the.p8file or it's compromised, immediately revoke it in the Apple Developer Console and create a new one
- Consider automating this process if possible to prevent service disruptions

This requirement only applies if you're configuring OAuth settings (Services ID, signing key, etc.). Native-only implementations don't require secret key rotation.

Apple's identity token does not include the user's full name in its claims. This means the Supabase Auth server cannot automatically populate the user's name metadata when users sign in with Apple.

- Apple only provides the user's full name during thefirst sign-in attempt(when the user initially authorizes your app)
- All subsequent sign-ins returnnullfor the full name fields
- The full name must be captured from Apple's native authentication response and manually saved using theupdateUsermethod

Recommended Approach:After a successful Sign in with Apple, check if the full name is available in the authentication response, and if so, use theupdateUsermethod to save it to the user's metadata:

```javascript
1// Example: Handling full name after successful sign in2if (credential.fullName) {3  // Full name is only provided on first sign-in4  await supabase.auth.updateUser({5    data: {6      full_name: `${credential.fullName.givenName} ${credential.fullName.familyName}`,7      given_name: credential.fullName.givenName,8      family_name: credential.fullName.familyName,9    },10  })11}
```

If a user revokes your app's access and then re-authorizes it, Apple will provide the full name again as if it were a first sign-in.

The platform-specific examples below demonstrate how to implement this pattern for each SDK.


### Using the OAuth flow for web#
Sign in with Apple's OAuth flow is designed for web or browser based sign in methods. It can be used on web-based apps as well as websites, though some users can benefit by using Sign in with Apple JS directly.

Behind the scenes, Supabase Auth uses theREST APIsprovided by Apple.

Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

To initiate sign in, you can use thesignInWithOAuth()method from the Supabase JavaScript library:

```javascript
1..({2  : 'apple',3})
```

This call takes the user to Apple's consent screen. Once the flow ends, the user's profile information is exchanged and validated with Supabase Auth before it redirects back to your web application with an access and refresh token representing the user's session.

When using the OAuth flow, the user's full name is not accessible from Apple's response. Apple only provides the full name through native authentication methods (Sign in with Apple JS, or native iOS/macOS SDKs) during the first sign-in.

If you need to collect user names, consider:

- Using Sign in with Apple JS instead (see below)
- Collecting the name through a separate onboarding form
- Using a profiles table to store user information

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```


### Configuration#
You will require the following information:

You can also configure the Apple auth provider using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Configure Apple auth provider6curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  -H "Content-Type: application/json" \9  -d '{10    "external_apple_enabled": true,11    "external_apple_client_id": "your-services-id",12    "external_apple_secret": "your-generated-secret-key"13  }'
```

Use this tool to generate a new Apple client secret. No keys leave your browser! Be aware that this tool does not currently work in Safari, so use Firefox or a Chrome-based browser instead.


### Using sign in with Apple JS#
Sign in with Apple JSis an official Apple framework for authenticating Apple users on websites. Although it can be used in web-based apps, those use cases will benefit more with the OAuth flow described above. We recommend using this method on classic websites only.

You can use thesignInWithIdToken()method from the Supabase JavaScript library on the website to obtain an access and refresh token once the user has given consent using Sign in with Apple JS:

```javascript
1async function signIn() {2  try {3    // Generate a nonce for security4    const nonce = crypto.randomUUID() // or use your preferred nonce generation method56    const data = await AppleID.auth.signIn()78    const { data: authData, error } = await supabase.auth.signInWithIdToken({9      provider: 'apple',10      token: data.id_token,11      nonce: nonce,12    })1314    if (error) {15      throw error16    }1718    // Apple only provides the user's name on the first sign-in19    // The user object contains name information from Apple's response20    if (data.user && data.user.name) {21      const fullName = [22        data.user.name.firstName,23        data.user.name.middleName,24        data.user.name.lastName25      ].filter(Boolean).join(' ')2627      // Save the name to user metadata for future use28      await supabase.auth.updateUser({29        data: {30          full_name: fullName,31          given_name: data.user.name.firstName,32          family_name: data.user.name.lastName,33        }34      })35    }36  } catch (error) {37    console.error('Apple sign in failed:', error)38    // Handle sign-in errors appropriately39  }40}
```

Alternatively, you can use theAppleIDSignInOnSuccessevent with theusePopupoption:

```javascript
1// Generate and store nonce for verification2const nonce = crypto.randomUUID()34// Initialize Apple ID with nonce5AppleID.auth.init({6  clientId: 'your-services-id',7  scope: 'name email',8  redirectURI: 'https://your-domain.com/auth/callback',9  usePopup: true,10  nonce: nonce,11})1213// Listen for authorization success14document.addEventListener('AppleIDSignInOnSuccess', async (event) => {15  try {16    const { data: authData, error } = await supabase.auth.signInWithIdToken({17      provider: 'apple',18      token: event.detail.authorization.id_token,19      nonce: nonce,20    })2122    if (error) {23      throw error24    }2526    // Apple only provides the user's name on the first sign-in27    if (event.detail.user && event.detail.user.name) {28      const fullName = [29        event.detail.user.name.firstName,30        event.detail.user.name.middleName,31        event.detail.user.name.lastName32      ].filter(Boolean).join(' ')3334      // Save the name to user metadata for future use35      await supabase.auth.updateUser({36        data: {37          full_name: fullName,38          given_name: event.detail.user.name.firstName,39          family_name: event.detail.user.name.lastName,40        }41      })42    }43  } catch (error) {44    console.error('Apple sign in failed:', error)45  }46})
```

Make sure you request the scopename emailwhen initializing the library, as shown in the example above.


### Configuration#
To use Sign in with Apple JS you need to configure these options:

If you're using Sign in with Apple JS you do not need to configure the OAuth settings.


================================================================================


# Login with Azure (Microsoft)
Source: https://supabase.com/docs/guides/auth/social-login/auth-azure

Login with Azure (Microsoft)

To enable Azure (Microsoft) Auth for your project, you need to set up an Azure OAuth application and add the application credentials to your Supabase Dashboard.


### Overview#
Setting up OAuth with Azure consists of four broad steps:

- Create an OAuth application under Azure Entra ID.
- Add a secret to the application.
- Add the Supabase Auth callback URL to the allowlist in the OAuth application in Azure.
- Configure the client ID and secret of the OAuth application within the Supabase Auth dashboard.


### Access your Azure Developer account#
- Go toportal.azure.com.
- Login and select Microsoft Entra ID under the list of Azure Services.


### Register an application#
- Under Microsoft Entra ID, selectApp registrationsin the side panel and selectNew registration.
- Choose a name and select your preferred option for the supported account types.
- Specify aWebRedirect URI. It should look like this:https://<project-ref>.supabase.co/auth/v1/callback
- Finally, selectRegisterat the bottom of the screen.




### Obtain a client ID and secret#

### Local development with Azure OAuth#
Azure does not allow127.0.0.1as a redirect URI hostname and requires
the use oflocalhost.

To enable Azure OAuth during local Supabase development, configure the
Supabase API external URL in yourconfig.toml:

```javascript
1[api]2external_url = "http://localhost:54321"
```

- Once your app has been registered, the client ID can be found under thelist of app registrationsunder the column titledApplication (client) ID.
- You can also find it in the app overview screen.
- Place the Client ID in the Azure configuration screen in the Supabase Auth dashboard.



- SelectAdd a certificate or secretin the app overview screen and open theClient secretstab.
- SelectNew client secretto create a new client secret.
- Choose a preferred expiry time of the secret. Make sure you record this in your calendar days in advance so you have enough time to create a new one without suffering from any downtime.
- Once the secret is generated place theValuecolumn (notSecret ID) in the Azure configuration screen in the Supabase Auth dashboard.



You can also configure the Azure auth provider using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Configure Azure auth provider6curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  -H "Content-Type: application/json" \9  -d '{10    "external_azure_enabled": true,11    "external_azure_client_id": "your-azure-client-id",12    "external_azure_secret": "your-azure-client-secret",13    "external_azure_url": "your-azure-url"14  }'
```


### Guarding against unverified email domains#
Microsoft Entra ID can send out unverified email domains in certain cases. This may open up your project to a vulnerability where a malicious user can impersonate already existing accounts on your project.

This only applies in at least one of these cases:

- You have configured theauthenticationBehaviorssetting of your OAuth application to allow unverified email domains
- You are using an OAuth app configured as single-tenant in the supported account types
- Your OAuth app was created before June 20th 2023 after Microsoft announced this vulnerability, and the app had used unverified emails prior

This means that most OAuth appsare not susceptibleto this vulnerability.

Despite this, we recommend configuring theoptionalxms_edovclaimon the OAuth app. This claim allows Supabase Auth to identify with certainty whether the email address sent over by Microsoft Entra ID is verified or not.

Configure this in the following way:

- Select theApp registrationsmenu in Microsoft Entra ID on the Azure portal.
- Select the OAuth app.
- Select theManifestmenu in the sidebar.
- Make a backup of the JSON just in case.
- Identify theoptionalClaimskey.
- Edit it by specifying the following object:1"optionalClaims":{2"idToken":[3{4"name":"xms_edov",5"source":null,6"essential":false,7"additionalProperties":[]8},9{10"name":"email",11"source":null,12"essential":false,13"additionalProperties":[]14}15],16"accessToken":[17{18"name":"xms_edov",19"source":null,20"essential":false,21"additionalProperties":[]22}23],24"saml2Token":[]25},
- SelectSaveto apply the new configuration.

```javascript
1"optionalClaims": {2      "idToken": [3          {4              "name": "xms_edov",5              "source": null,6              "essential": false,7              "additionalProperties": []8          },9          {10              "name": "email",11              "source": null,12              "essential": false,13              "additionalProperties": []14          }15      ],16      "accessToken": [17          {18              "name": "xms_edov",19              "source": null,20              "essential": false,21              "additionalProperties": []22          }23      ],24      "saml2Token": []25  },
```


### Configure a tenant URL (optional)#
A Microsoft Entra tenant is the directory of users who are allowed to access your project. This section depends on what your OAuth registration uses forSupported account types.

By default, Supabase Auth uses thecommonMicrosoft tenant (https://login.microsoftonline.com/common) which generally allows any Microsoft account to sign in to your project. Microsoft Entra further limits what accounts can access your project depending on the type of OAuth application you registered.

If your app is registered asPersonal Microsoft accounts onlyfor theSupported account typesset Microsoft tenant toconsumers(https://login.microsoftonline.com/consumers).

If your app is registered asMy organization onlyfor theSupported account typesyou may want to configure Supabase Auth with the organization's tenant URL. This will use the tenant's authorization flows instead, and will limit access at the Supabase Auth level to Microsoft accounts arising from only the specified tenant.

Configure this by storing a value underAzure Tenant URLin the Supabase Auth provider configuration page for Azure that has the following formathttps://login.microsoftonline.com/<tenant-id>.


### Add login code to your client app#
Supabase Auth requires that Azure returns a valid email address. Therefore you must request theemailscope in thesignInWithOAuthmethod.

Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withazureas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'azure',4    : {5      : 'email',6    },7  })8}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()3}
```


### Obtain the provider refresh token#
Azure OAuth2.0 doesn't return theprovider_refresh_tokenby default. If you need theprovider_refresh_tokenreturned, you will need to include the following scope:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'azure',4    : {5      : 'offline_access',6    },7  })8}
```


### Resources#
- Azure Developer Account
- GitHub Discussion
- Potential Risk of Privilege Escalation in Azure AD Applications


================================================================================


# Login with Bitbucket
Source: https://supabase.com/docs/guides/auth/social-login/auth-bitbucket

Login with Bitbucket

To enable Bitbucket Auth for your project, you need to set up a Bitbucket OAuth application and add the application credentials to your Supabase Dashboard.


### Overview#
Setting up Bitbucket logins for your application consists of 3 parts:

- Create and configure a Bitbucket OAuth Consumer onBitbucket
- Add your Bitbucket OAuth Consumer keys to yourSupabase Project
- Add the login code to yourSupabase JS Client App


### Access your Bitbucket account#
- Go tobitbucket.org.
- Click onLoginat the top right to log in.




### Find your callback URL#
The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onBitbucketfrom the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.


### Create a Bitbucket OAuth app#
- Click on your profile icon at the bottom left
- Click onAll Workspaces
- Select a workspace and click on it to select it
- Click onSettingson the left
- Click onOAuth consumerson the left underApps and Features(near the bottom)
- ClickAdd Consumerat the top
- Enter the name of your app underName
- InCallback URL, type the callback URL of your app
- Check the permissions you need (Email, Read should be enough)
- ClickSaveat the bottom
- Click on your app name (the name of your new OAuth Consumer)
- Copy yourKey(client_key) andSecret(client_secret) codes


### Add your Bitbucket credentials into your Supabase project#
- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onBitBucketfrom the accordion list to expand and turnBitBucket Enabledto ON
- Enter yourBitBucket Client IDandBitBucket Client Secretsaved in the previous step
- ClickSave


### Add login code to your client app#
Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withbitbucketas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'bitbucket',4  })5}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()3}
```


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- Bitbucket Account


================================================================================


# Login with Discord
Source: https://supabase.com/docs/guides/auth/social-login/auth-discord

Login with Discord

To enable Discord Auth for your project, you need to set up a Discord Application and add the Application OAuth credentials to your Supabase Dashboard.


### Overview#
Setting up Discord logins for your application consists of 3 parts:

- Create and configure a Discord ApplicationDiscord Developer Portal
- Add your Discord OAuth Consumer keys to yourSupabase Project
- Add the login code to yourSupabase JS Client App


### Access your Discord account#
- Go todiscord.com.
- Click onLoginat the top right to log in.



- Once logged in, go todiscord.com/developers.




### Find your callback URL#
The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onDiscordfrom the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.


### Create a Discord application#
- Click onNew Applicationat the top right.
- Enter the name of your application and clickCreate.
- Click onOAuth2underSettingsin the left side panel.
- ClickAdd RedirectunderRedirects.
- Type or paste yourcallback URLinto theRedirectsbox.
- ClickSave Changesat the bottom.
- Copy yourClient IDandClient SecretunderClient information.


### Add your Discord credentials into your Supabase project#
- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onDiscordfrom the accordion list to expand and turnDiscord Enabledto ON
- Enter yourDiscord Client IDandDiscord Client Secretsaved in the previous step
- ClickSave

You can also configure the Discord auth provider using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Configure Discord auth provider6curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  -H "Content-Type: application/json" \9  -d '{10    "external_discord_enabled": true,11    "external_discord_client_id": "your-discord-client-id",12    "external_discord_secret": "your-discord-client-secret"13  }'
```


### Add login code to your client app#
Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withdiscordas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'discord',4  })5}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

If your user is already signed in, Discord prompts the user again for authorization.

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()3}
```


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- Discord Account
- Discord Developer Portal


================================================================================


# Login with Facebook
Source: https://supabase.com/docs/guides/auth/social-login/auth-facebook

Login with Facebook

To enable Facebook Auth for your project, you need to set up a Facebook OAuth application and add the application credentials to your Supabase Dashboard.


### Overview#
Setting up Facebook logins for your application consists of 4 parts:

- Create and configure a Facebook Application on theFacebook Developers Site
- Configure email permissionsin your Facebook app (required for Supabase Auth)
- Add your Facebook keys to yourSupabase Project
- Add the login code to yourSupabase JS Client App


### Access your Facebook Developer account#
- Go todevelopers.facebook.com.
- Click onLog Inat the top right to log in.




### Create a Facebook app#
- Click onMy Appsat the top right.
- ClickCreate Appnear the top right.
- Select your app type and clickContinue.
- Fill in your app information, then clickCreate App.
- This should bring you to the screen:Add Products to Your App. (Alternatively you can click onAdd Productin the left sidebar to get to this screen.)

The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onFacebookfrom the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.


### Set up Facebook login for your Facebook app#
From theAdd Products to your Appscreen:

- ClickSetupunderFacebook Login
- Skip the Quickstart screen. Instead, in the left sidebar, clickSettingsunderFacebook Login
- Enter your callback URI underValid OAuth Redirect URIson theFacebook Login Settingspage
- ClickSave Changesat the bottom right

Your callback URI follows this pattern:https://<project-ref>.supabase.co/auth/v1/callback

You can find your project's callback URI in theSupabase DashboardunderAuthentication > Providers > Facebook.


### Configure email permissions (required)#
This step isrequiredfor Supabase Auth to work correctly. Without email permissions, Facebook will not return the user's email address, which may cause authentication failures or incomplete user profiles.

You must configure the email permission in your Facebook app's Use Cases:

You can verify the permissions are set correctly by checking that bothpublic_profileandemailappear with a green check mark or "Ready for testing" status.


### Copy your Facebook app ID and secret#
- ClickSettings / Basicin the left sidebar
- Copy your App ID from the top of theBasic Settingspage
- UnderApp SecretclickShowthen copy your secret
- Make sure all required fields are completed on this screen.


### Enter your Facebook app ID and secret into your Supabase project#
- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onFacebookfrom the accordion list to expand and turnFacebook Enabledto ON
- Enter yourFacebook Client IDandFacebook Client Secretsaved in the previous step
- ClickSave

You can also configure the Facebook auth provider using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Configure Facebook auth provider6curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  -H "Content-Type: application/json" \9  -d '{10    "external_facebook_enabled": true,11    "external_facebook_client_id": "your-facebook-app-id",12    "external_facebook_secret": "your-facebook-app-secret"13  }'
```


### Add login code to your client app#
Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withfacebookas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'facebook',4  })56  if () {7    .('Error signing in with Facebook:', .)8    return9  }1011  // The user will be redirected to Facebook for authentication12}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()34  if () {5    .('Error signing out:', .)6    return7  }89  // User has been signed out10}
```


### Testing your integration#
Facebook apps start inDevelopmentmode, which has the following limitations:

- Only users with a role on the app (administrators, developers, testers) can authenticate
- Other users will see an "App Not Setup" error when trying to log in

To add test users:

Development mode is sufficient for local development and testing. You only need to submit for App Review when you're ready to allow any Facebook user to authenticate with your app.


### Going live with app review#
Before your app can be used by the general public, you need to complete Facebook's App Review process:

Complete App Settings: In your Facebook app'sSettings > Basic, fill in all required fields including:

- App Icon
- Privacy Policy URL
- Terms of Service URL (if applicable)
- App Domain

Request Permissions: Navigate toApp Review > Permissions and Featuresand request the permissions you need:

- public_profile- Usually pre-approved
- email- Requires verification that your app needs email access

Submit for Review: ClickSubmit for Reviewand provide:

- Detailed instructions for how Facebook reviewers should test your login flow
- A screencast video demonstrating the Facebook Login feature
- Explanation of how user data will be used

Wait for Approval: Facebook typically reviews apps within 1-5 business days

If you only need basic authentication (name and profile picture), you may not need full App Review. Apps requesting onlypublic_profileandemailwith the "Authenticate and request data from users with Facebook Login" use case can often go live without a detailed review.

For more details, see theFacebook App Review documentation.


### Troubleshooting#

### "App not setup" error#
This error occurs when a user without a role on your app tries to log in while the app is in Development mode.

Solution: Either add the user as a tester in your Facebook app settings, or complete the App Review process to make your app available to all users.


### User's email not returned#
Facebook only returns the email address if:

- The user has a confirmed email on their Facebook account
- Your app has been granted theemailpermission
- Theemailpermission is marked as "Ready for testing" inUse Cases > Authentication and Account Creation

Solution: Check that theemailpermission is properly configured in your Facebook app's Use Cases settings.


### "Redirect URI mismatch" error#
This error indicates the callback URL configured in Facebook doesn't match the one used during authentication.

Solution: Verify that theValid OAuth Redirect URIsin your Facebook app settings exactly matcheshttps://<project-ref>.supabase.co/auth/v1/callback. Make sure there are no trailing slashes or typos.


### Login works in development but not production#
If login works locally but fails in production, check:

- Your production URL is added toValid OAuth Redirect URIsin Facebook
- The App ID and Secret in your Supabase dashboard match your Facebook app
- Your Facebook app is inLivemode (not Development mode)


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- Facebook Developers Dashboard


================================================================================


# Login with Figma
Source: https://supabase.com/docs/guides/auth/social-login/auth-figma

Login with Figma

To enable Figma Auth for your project, you need to set up a Figma OAuth application and add the application credentials to your Supabase Dashboard.


### Overview#
Setting up Figma logins for your application consists of 3 parts:

- Create and configure a Figma App on theFigma Developers page.
- Add your Figmaclient_idandclient_secretto yourSupabase Project.
- Add the login code to yourSupabase JS Client App.


### Access the Figma Developers page#
- Go to theFigma Developers page
- Log in (if necessary)


### Find your callback URL#
The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onFigmafrom the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.


### Create a Figma OAuth app#
Enter yourApp name, select the owner for the app and clickCreate appbutton



Copy and save your newly-generatedClient ID

Copy and save your newly-generatedClient Secret

Then, go toOAuth credentialsand click onAdd a redirect URLbutton



Add your URL from the previous step (callback URL on Supabase) and click onAddbutton

Go toOAuth scopesand selectcurrent_user:readunderUsers.




### Enter your Figma credentials into your Supabase project#
- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onFigmafrom the accordion list to expand and turnFigma Enabledto ON
- Enter yourFigma Client IDandFigma Client Secretsaved in the previous step
- ClickSave


### Add login code to your client app#
Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withfigmaas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'figma',4  })5}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()3}
```


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- Figma Developers page


================================================================================


# Login with GitHub
Source: https://supabase.com/docs/guides/auth/social-login/auth-github

Login with GitHub

To enable GitHub Auth for your project, you need to set up a GitHub OAuth application and add the application credentials to your Supabase Dashboard.


### Overview#
Setting up GitHub logins for your application consists of 3 parts:

- Create and configure a GitHub OAuth App onGitHub
- Add your GitHub OAuth keys to yourSupabase Project
- Add the login code to yourSupabase JS Client App


### Find your callback URL#
The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onGitHubfrom the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.


### Register a new OAuth application on GitHub#
- Navigate to theOAuth apps page
- ClickRegister a new application. If you've created an app before, clickNew OAuth Apphere.
- InApplication name, type the name of your app.
- InHomepage URL, type the full URL to your app's website.
- InAuthorization callback URL, type the callback URL of your app.
- LeaveEnable Device Flowunchecked.
- ClickRegister Application.

Copy your new OAuth credentials

- Copy and save yourClient ID.
- ClickGenerate a new client secret.
- Copy and save yourClient secret.


### Enter your GitHub credentials into your Supabase project#
- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onGitHubfrom the accordion list to expand and turnGitHub Enabledto ON
- Enter yourGitHub Client IDandGitHub Client Secretsaved in the previous step
- ClickSave

You can also configure the GitHub auth provider using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Configure GitHub auth provider6curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  -H "Content-Type: application/json" \9  -d '{10    "external_github_enabled": true,11    "external_github_client_id": "your-github-client-id",12    "external_github_secret": "your-github-client-secret"13  }'
```


### Add login code to your client app#
Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withgithubas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'github',4  })5}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()3}
```


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- GitHub Developer Settings


================================================================================


# Login with GitLab
Source: https://supabase.com/docs/guides/auth/social-login/auth-gitlab

Login with GitLab

To enable GitLab Auth for your project, you need to set up a GitLab OAuth application and add the application credentials to your Supabase Dashboard.


### Overview#
Setting up GitLab logins for your application consists of 3 parts:

- Create and configure a GitLab Application onGitLab
- Add your GitLab Application keys to yourSupabase Project
- Add the login code to yourSupabase JS Client App


### Access your GitLab account#
- Go togitlab.com.
- Click onLoginat the top right to log in.




### Find your callback URL#
The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onGitLabfrom the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.


### Create your GitLab application#
- Click on yourprofile logo(avatar) in the top-right corner.
- SelectEdit profile.
- In the left sidebar, select Applications.
- Enter the name of the application.
- In theRedirect URIbox, type the callback URL of your app.
- Check the box next toConfidential(make sure it is checked).
- Check the scope namedread_user(this is the only required scope).
- ClickSave Applicationat the bottom.
- Copy and save yourApplication ID(client_id) andSecret(client_secret) which you'll need later.


### Add your GitLab credentials into your Supabase project#
- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onGitLabfrom the accordion list to expand and turnGitLab Enabledto ON
- Enter yourGitLab Client IDandGitLab Client Secretsaved in the previous step
- ClickSave


### Add login code to your client app#
Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withgitlabas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'gitlab',4  })5}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()3}
```


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- GitLab Account


================================================================================


# Login with Google
Source: https://supabase.com/docs/guides/auth/social-login/auth-google

Login with Google

Supabase Auth supportsSign in with Google for the web, native applications (Android,macOS and iOS), andChrome extensions.

You can use Sign in with Google in two ways:

- By writing application codefor the web, native applications or Chrome extensions
- By using Google's pre-built solutionssuch aspersonalized sign-in buttons,One Taporautomatic sign-in


### Prerequisites#
You need to do some setup to get started with Sign in with Google:

- Prepare a Google Cloud project. Go to theGoogle Cloud Platformand create a new project if necessary.
- Use theGoogle Auth Platform consoleto register and set up your application's:Audienceby configuring which Google users are allowed to sign in to your application.Data Access (Scopes)define what your application can do with your user's Google data and APIs, such as access profile information or more.BrandingandVerificationshow a logo and name instead of the Supabase project ID in the consent screen, improving user retention. Brand verification may take a few business days.

- Audienceby configuring which Google users are allowed to sign in to your application.
- Data Access (Scopes)define what your application can do with your user's Google data and APIs, such as access profile information or more.
- BrandingandVerificationshow a logo and name instead of the Supabase project ID in the consent screen, improving user retention. Brand verification may take a few business days.


### Setup required scopes#
Supabase Auth needs a few scopes granting access to profile data of your end users, which you have to configure in theData Access (Scopes)screen:

- openid(add manually)
- .../auth/userinfo.email(added by default)
- ...auth/userinfo.profile(added by default)

If you add more scopes, especially those on the sensitive or restricted list your application might be subject to verification which may take a long time.


### Setup consent screen branding#
It's strongly recommended you set up a custom domain and optionally verify your brand information with Google, as this makes phishing attempts easier to spot by your users.

Google's consent screen is shown to users when they sign in. Optionally configure one of the following to improve the appearance of the screen, increasing the perception of trust by your users:

- A good approach is to useauth.example.comorapi.example.com, if your application is hosted onexample.com.
- If you don't set this up, users will see<project-id>.supabase.cowhich does not inspire trust and can make your application more susceptible to successful phishing attempts.


### Project setup#
To support Sign In with Google, you need to configure the Google provider for your Supabase project.

Regardless of whether you use application code or Google's pre-built solutions to implement the sign in flow, you need to configure your project by obtaining a Client ID and Client Secret in theClientssection of the Google Auth Platform console:

- If your app is hosted onhttps://example.com/appaddhttps://example.com.
- Addhttp://localhost:<port>while developing locally. Remember to remove this when your applicationgoes into production.

- Access it from theGoogle provider page on the Dashboard.
- For local development, usehttp://127.0.0.1:54321/auth/v1/callback.

- Add these values to theGoogle provider page on the Dashboard.


### Local development#
To use the Google provider in local development:

```javascript
1SUPABASE_AUTH_EXTERNAL_GOOGLE_CLIENT_SECRET="<client-secret>"
```

```javascript
1[auth.external.google]2enabled = true3client_id = "<client-id>"4secret = "env(SUPABASE_AUTH_EXTERNAL_GOOGLE_CLIENT_SECRET)"5skip_nonce_check = false
```

If you have multiple client IDs, such as one for Web, iOS and Android, concatenate all of the client IDs with a comma but make sure the web's client ID is first in the list.


### Using the management API#
Use thePATCH/v1/projects/{ref}/config/authManagement API endpointto configure the project's Auth settings programmatically. For configuring the Google provider send these options:

```javascript
1{2  "external_google_enabled": true,3  "external_google_client_id": "your-google-client-id",4  "external_google_secret": "your-google-client-secret"5}
```


### Signing users in#

### Application code#
To use your own application code for the signin button, call thesignInWithOAuthmethod (or the equivalent for your language).

Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

```javascript
1..({2  : 'google',3})
```

For an implicit flow, that's all you need to do. The user will be taken to Google's consent screen, and finally redirected to your app with an access and refresh token pair representing their session.

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

After a successful code exchange, the user's session will be saved to cookies.


### Saving Google tokens#
The tokens saved by your application are the Supabase Auth tokens. Your app might additionally need the Google OAuth 2.0 tokens to access Google services on the user's behalf.

On initial login, you can extract theprovider_tokenfrom the session and store it in a secure storage medium. The session is available in the returned data fromsignInWithOAuth(implicit flow) andexchangeCodeForSession(PKCE flow).

Google does not send out a refresh token by default, so you will need to pass parameters like these tosignInWithOAuth()in order to extract theprovider_refresh_token:

```javascript
1const { ,  } = await ..({2  : 'google',3  : {4    : {5      : 'offline',6      : 'consent',7    },8  },9})
```


### Google pre-built#
Most web apps and websites can utilize Google'spersonalized sign-in buttons,One Taporautomatic sign-infor the best user experience.

Load the Google client library in your app by including the third-party script:

```javascript
1<script src="https://accounts.google.com/gsi/client" async></script>
```

Use theHTML Code Generatorto customize the look, feel, features and behavior of the Sign in with Google button.

Pick theSwap to JavaScript callbackoption, and input the name of your callback function. This function will receive aCredentialResponsewhen sign in completes.

To make your app compatible with Chrome's third-party-cookie phase-out, make sure to setdata-use_fedcm_for_prompttotrue.

Your final HTML code might look something like this:

```javascript
1<div2  id="g_id_onload"3  data-client_id="<client ID>"4  data-context="signin"5  data-ux_mode="popup"6  data-callback="handleSignInWithGoogle"7  data-nonce=""8  data-auto_select="true"9  data-itp_support="true"10  data-use_fedcm_for_prompt="true"11></div>1213<div14  class="g_id_signin"15  data-type="standard"16  data-shape="pill"17  data-theme="outline"18  data-text="signin_with"19  data-size="large"20  data-logo_alignment="left"21></div>
```

Create ahandleSignInWithGooglefunction that takes theCredentialResponseand passes the included token to Supabase. The function needs to be available in the global scope for Google's code to find it.

```javascript
1async function handleSignInWithGoogle(response) {2  const { data, error } = await supabase.auth.signInWithIdToken({3    provider: 'google',4    token: response.credential,5  })6}
```

(Optional)Configure a nonce. The use of a nonce is recommended for extra security, but optional. The nonce should be generated randomly each time, and it must be provided in both thedata-nonceattribute of the HTML code and the options of the callback function.

```javascript
1async function handleSignInWithGoogle(response) {2  const { data, error } = await supabase.auth.signInWithIdToken({3    provider: 'google',4    token: response.credential,5    nonce: '<NONCE>',6  })7}
```

Note that the nonce should be the same in both places, but because Supabase Auth expects the provider to hash it (SHA-256, hexadecimal representation), you need to provide a hashed version to Google and a non-hashed version tosignInWithIdToken.

You can get both versions by using the in-builtcryptolibrary:

```javascript
1// Adapted from https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto/digest#converting_a_digest_to_a_hex_string23const  = (.(....(new (32))))4const  = new ()5const  = .()6..('SHA-256', ).(() => {7  const  = .(new ())8  const  = .(() => .(16).(2, '0')).('')9})1011// Use 'hashedNonce' when making the authentication request to Google12// Use 'nonce' when invoking the supabase.auth.signInWithIdToken() method
```


### One-tap with Next.js#
If you're integrating Google One-Tap with your Next.js application, you can refer to the example below to get started:

```javascript
1'use client'23import Script from 'next/script'4import { createClient } from '@/utils/supabase/client'5import type { accounts, CredentialResponse } from 'google-one-tap'6import { useRouter } from 'next/navigation'78declare const google: { accounts: accounts }910// generate nonce to use for google id token sign-in11const generateNonce = async (): Promise<string[]> => {12  const nonce = btoa(String.fromCharCode(...crypto.getRandomValues(new Uint8Array(32))))13  const encoder = new TextEncoder()14  const encodedNonce = encoder.encode(nonce)15  const hashBuffer = await crypto.subtle.digest('SHA-256', encodedNonce)16  const hashArray = Array.from(new Uint8Array(hashBuffer))17  const hashedNonce = hashArray.map((b) => b.toString(16).padStart(2, '0')).join('')1819  return [nonce, hashedNonce]20}2122const OneTapComponent = () => {23  const supabase = createClient()24  const router = useRouter()2526  const initializeGoogleOneTap = async () => {27    console.log('Initializing Google One Tap')28    const [nonce, hashedNonce] = await generateNonce()29    console.log('Nonce: ', nonce, hashedNonce)3031    // check if there's already an existing session before initializing the one-tap UI32    const { data, error } = await supabase.auth.getSession()33    if (error) {34      console.error('Error getting session', error)35    }36    if (data.session) {37      router.push('/')38      return39    }4041    /* global google */42    google.accounts.id.initialize({43      client_id: process.env.NEXT_PUBLIC_GOOGLE_CLIENT_ID,44      callback: async (response: CredentialResponse) => {45        try {46          // send id token returned in response.credential to supabase47          const { data, error } = await supabase.auth.signInWithIdToken({48            provider: 'google',49            token: response.credential,50            nonce,51          })5253          if (error) throw error54          console.log('Session data: ', data)55          console.log('Successfully logged in with Google One Tap')5657          // redirect to protected page58          router.push('/')59        } catch (error) {60          console.error('Error logging in with Google One Tap', error)61        }62      },63      nonce: hashedNonce,64      // with chrome's removal of third-party cookies, we need to use FedCM instead (https://developers.google.com/identity/gsi/web/guides/fedcm-migration)65      use_fedcm_for_prompt: true,66    })67    google.accounts.id.prompt() // Display the One Tap UI68  }6970  return <Script onReady={initializeGoogleOneTap} src="https://accounts.google.com/gsi/client" />71}7273export default OneTapComponent
```


================================================================================


# Login with Kakao
Source: https://supabase.com/docs/guides/auth/social-login/auth-kakao

Login with Kakao

To enable Kakao Auth for your project, you need to set up a Kakao OAuth application and add the application credentials to your Supabase Dashboard.


### Overview#
Kakao OAuth consists of six broad steps:

- Create and configure your app in theKakao Developers Portal.
- Obtain aREST API key- this serves as theclient_id.
- Obtain aKakao Login Client Secret code- this serves as theclient_secret.
- Configure additional settings in the Kakao Developers Portal.
- Add yourclient idandclient secretkeys to yourSupabase Project.
- Add the login code to yourSupabase JS Client App.


### Access your Kakao Developer account#
- Go toKakao Developers Portal.
- Click onLoginat the top right to log in.




### Create and configure your app#
- Go toApp.
- Click onCreate appat the top.
- Fill out your app information:App icon.App name.Company name.Category.App primary domain.
- ClickSaveat the bottom right.

- App icon.
- App name.
- Company name.
- Category.
- App primary domain.


### Obtain a REST API key#
This serves as theclient_idwhen you make API calls to authenticate the user.

- Go toApp.
- Click on your app.
- Go toApp Settings>App>Platform Key.
- In thePlatform Keysection isREST API key. This will become yourclient_idlater.


### Find your callback URL#
The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onKakaofrom the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.

- To add a callback URL on Kakao, go toApp Settings>App>Platform Key.
- Click on the REST API key you want to use.
- In the edit page, enter your callback URL in theKakao Login Redirect URIfield.
- ClickSavein the bottom right.


### Obtain a client secret#
- Go toApp Settings>App>Platform Key.
- Click on the REST API key you want to use.
- Note theKakao Login Client Secret code. This serves as aclient_secretfor your Supabase project.
- Make sure you activateKakao Login Client Secret.


### Additional configurations on Kakao Developers portal#
- Go toProduct Settings>Kakao Login>General.
- SetStateto "ON" in theUsage settingssection to enable Kakao Login.
- Go toProduct Settings>Kakao Login>Consent Items.
- Set the following scopes under theConsent Items:account_emailprofile_imageprofile_nickname

- account_email
- profile_image
- profile_nickname



In the Kakao Developers Portal, the "account_email" consent item is only available for apps that are registered as "Biz App". To convert your app to a "Biz App", go toApp Settings>App>General, and complete the required fields in theBusiness Informationsection.


### Add your OAuth credentials to Supabase#
- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onKakaofrom the accordion list to expand and turnKakao Enabledto ON
- Enter yourKakao Client IDandKakao Client Secretsaved in the previous step
- ClickSave


### Add login code to your client app#
Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withkakaoas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'kakao',4  })5}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function signOut() {2  const { error } = await supabase.auth.signOut()3}
```


### Using Kakao Login JS SDK#
Kakao Login JS SDKis an official Kakao SDK for authenticating Kakao users on websites.

Exchange theauthorization code returned by Kakao APIfor anID Token.

For example, this code shows a how to get ID Token:

```javascript
1const requestUrl = new URL(request.url);2const code = requestUrl.searchParams.get('code');34if (code) {5  const res = await fetch('https://kauth.kakao.com/oauth/token', {6    method: 'POST',7    headers: {8      'Content-Type': 'application/x-www-form-urlencoded;charset=utf-8',9    },10    body: new URLSearchParams({11      grant_type: 'authorization_code',12      client_id: '<CLIENT_ID>',13      redirect_uri: '<url>/api/auth/kakao/oidc',14      code,15      client_secret: '<CLIENT_SECRET>',16    }),17  });1819  const {id_token} = await res.json();20}
```

Use the ID Token to sign in:

```javascript
1const res = await auth.signInWithIdToken({2  provider: 'kakao',3  token: id_token,4});
```


### Configuration#

### Resources#
- Kakao Developers Portal.


================================================================================


# Login with Keycloak
Source: https://supabase.com/docs/guides/auth/social-login/auth-keycloak

Login with Keycloak

To enable Keycloak Auth for your project, you need to set up an Keycloak OAuth application and add the application credentials to your Supabase Dashboard.


### Overview#
To get started with Keycloak, you can run it in a docker container with:docker run -p 8080:8080 -e KEYCLOAK_ADMIN=admin -e KEYCLOAK_ADMIN_PASSWORD=admin quay.io/keycloak/keycloak:latest start-dev

This guide will be assuming that you are running Keycloak in a docker container as described in the command above.

Keycloak OAuth consists of five broad steps:

- Create a new client in your specified Keycloak realm.
- Obtain theissuerfrom the "OpenID Endpoint Configuration". This will be used as theKeycloak URL.
- Ensure that the new client has the "Client Protocol" set toopenid-connectand the "Access Type" is set to "confidential".
- TheClient IDof the client created will be used as theclient id.
- Obtain theSecretfrom the credentials tab which will be used as theclient secret.
- Add the callback URL of your application to your allowlist.


### Access your Keycloak admin console#
- Login by visitinghttp://localhost:8080and clicking on "Administration Console".


### Create a Keycloak realm#
- Once you've logged in to the Keycloak console, you can add a realm from the side panel. The default realm should be named "Master".
- After you've added a new realm, you can retrieve theissuerfrom the "OpenID Endpoint Configuration" endpoint. Theissuerwill be used as theKeycloak URL.
- You can find this endpoint from the realm settings under the "General Tab" or visithttp://localhost:8080/realms/my_realm_name/.well-known/openid-configuration




### Create a Keycloak client#
The "Client ID" of the created client will serve as theclient_idwhen you make API calls to authenticate the user.




### Client settings#
After you've created the client successfully, ensure that you set the following settings:




### Obtain the client secret#
This will serve as theclient_secretwhen you make API calls to authenticate the user.
Under the "Credentials" tab, theSecretvalue will be used as theclient secret.




### Add login code to your client app#
Since Keycloak version 22, theopenidscope must be passed. Add this to thesupabase.auth.signInWithOAuth()method.

Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withkeycloakas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'keycloak',4    : {5      : 'openid',6    },7  })8}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function signOut() {2  const { error } = await supabase.auth.signOut()3}
```


### Resources#
- You can find the Keycloak OpenID endpoint configuration under the realm settings.


================================================================================


# Login with LinkedIn
Source: https://supabase.com/docs/guides/auth/social-login/auth-linkedin

Login with LinkedIn

To enable LinkedIn Auth for your project, you need to set up a LinkedIn OAuth application and add the application credentials to your Supabase Dashboard.


### Overview#
Setting up LinkedIn logins for your application consists of 3 parts:

- Create and configure a LinkedIn Project and App on theLinkedIn Developer Dashboard.
- Add yourLinkedIn (OIDC)client_idandclient_secretto yourSupabase Project.
- Add the login code to yourSupabase JS Client App.


### Access your LinkedIn Developer account#
- Go toLinkedIn Developer Dashboard.
- Log in (if necessary.)




### Find your callback URL#
The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onLinkedInfrom the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.


### Create a LinkedIn OAuth app#
- Go toLinkedIn Developer Dashboard.
- Click onCreate Appat the top right
- Enter yourLinkedIn PageandApp Logo
- Save your app
- ClickProductsfrom the top menu
- Look forSign In with LinkedIn using OpenID Connectand click on Request Access
- ClickAuthfrom the top menu
- Add yourRedirect URLto theAuthorized Redirect URLs for your appsection
- Copy and save your newly-generatedClient ID
- Copy and save your newly-generatedClient Secret

Ensure that the appropriate scopes have been added under OAuth 2.0 Scopes at the bottom of theAuthscreen.




### Enter your LinkedIn (OIDC) credentials into your Supabase project#
- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onLinkedIn (OIDC)from the accordion list to expand and turnLinkedIn (OIDC) Enabledto ON
- Enter yourLinkedIn (OIDC) Client IDandLinkedIn (OIDC) Client Secretsaved in the previous step
- ClickSave

You can also configure the LinkedIn (OIDC) auth provider using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Configure LinkedIn (OIDC) auth provider6curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  -H "Content-Type: application/json" \9  -d '{10    "external_linkedin_oidc_enabled": true,11    "external_linkedin_oidc_client_id": "your-linkedin-client-id",12    "external_linkedin_oidc_secret": "your-linkedin-client-secret"13  }'
```


### Add login code to your client app#
Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withlinkedin_oidcas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'linkedin_oidc',4  })5}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()3}
```


### LinkedIn Open ID Connect (OIDC)#
We will be replacing theLinkedInprovider with a newLinkedIn (OIDC)provider to support recent changes to the LinkedInOAuth APIs. The new provider utilizes theOpen ID Connect standard. In view of this change, we have disabled edits on theLinkedInprovider and will be removing it effective 4th January 2024. Developers with LinkedIn OAuth Applications created prior to 1st August 2023 should create a new OAuth applicationvia the steps outlined aboveand migrate their credentials from theLinkedInprovider to theLinkedIn (OIDC)provider. Alternatively, you can also head to theProductssection and add the newly releaseSign In with LinkedIn using OpenID Connectto your existing OAuth application.

Developers using the Supabase CLI to test their LinkedIn OAuth application should also update theirconfig.tomlto make use of the new provider:

```javascript
1[auth.external.linkedin_oidc]2enabled = true3client_id = ...4secret = ...
```

Do reach out to support if you have any concerns around this change.


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- LinkedIn Developer Dashboard


================================================================================


# Login with Notion
Source: https://supabase.com/docs/guides/auth/social-login/auth-notion

Login with Notion

To enable Notion Auth for your project, you need to set up a Notion Application and add the Application OAuth credentials to your Supabase Dashboard.


### Overview#
Setting up Notion logins for your application consists of 3 parts:

- Create and configure a Notion ApplicationNotion Developer Portal
- Retrieve your OAuth client ID and OAuth client secret and add them to yourSupabase Project
- Add the login code to yourSupabase JS Client App


### Create your notion integration#
- Go todevelopers.notion.com.
- Click "View my integrations" and login.
- Once logged in, go tonotion.so/my-integrationsand create a new integration.
- When creating your integration, ensure that you select "Public integration" under "Integration type" and "Read user information including email addresses" under "Capabilities".
- You will need to add a redirect URI, seeAdd the redirect URI
- Once you've filled in the necessary fields, click "Submit" to finish creating the integration.

Go todevelopers.notion.com.

Click "View my integrations" and login.

Once logged in, go tonotion.so/my-integrationsand create a new integration.

When creating your integration, ensure that you select "Public integration" under "Integration type" and "Read user information including email addresses" under "Capabilities".

You will need to add a redirect URI, seeAdd the redirect URI

Once you've filled in the necessary fields, click "Submit" to finish creating the integration.




### Add the redirect URI#
- After selecting "Public integration", you should see an option to add "Redirect URIs".



The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onNotionfrom the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.


### Add your Notion credentials into your Supabase project#
- Once you've created your notion integration, you should be able to retrieve the "OAuth client ID" and "OAuth client secret" from the "OAuth Domain and URIs" tab.



- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onNotionfrom the accordion list to expand and turnNotion Enabledto ON
- Enter yourNotion Client IDandNotion Client Secretsaved in the previous step
- ClickSave


### Add login code to your client app#
Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withnotionas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'notion',4  })5}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()3}
```


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- Notion Account
- Notion Developer Portal


================================================================================


# Login with Slack
Source: https://supabase.com/docs/guides/auth/social-login/auth-slack

Login with Slack

To enable Slack Auth for your project, you need to set up a Slack OAuth application and add the application credentials to your Supabase Dashboard.


### Overview#
We will be replacing the existing Slack provider with a new Slack (OIDC) provider. Developers with Slack OAuth Applications created prior to 24th June 2024 should create a new application and migrate their credentials from the Slack provider to the Slack (OIDC) provider. Existing OAuth Applications built with the old Slack provider will continue to work up till 10th October. You can refer to thelist of supported scopesfor the new Slack (OIDC) User.

Setting up Slack logins for your application consists of 3 parts:

- Create and configure a Slack Project and App on theSlack Developer Dashboard.
- Add your SlackAPI KeyandAPI Secret Keyto yourSupabase Project.
- Add the login code to yourSupabase JS Client App.


### Access your Slack Developer account#
- Go toapi.slack.com.
- Click onYour Appsat the top right to log in.




### Find your callback URL#
The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onSlackfrom the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.


### Create a Slack OAuth app#
- Go toapi.slack.com.
- Click onCreate New App

UnderCreate an app...:

- ClickFrom scratch
- Type the name of your app
- Select yourSlack Workspace
- ClickCreate App

UnderApp Credentials:

- Copy and save your newly-generatedClient ID
- Copy and save your newly-generatedClient Secret

Under the sidebar, selectOAuth & Permissionsand look forRedirect URLs:

- ClickAdd New Redirect URL
- Paste yourCallback URLthen clickAdd
- ClickSave URLs

UnderScopes:

- Add the following scopes under theUser Token Scopes:profile,email,openid. These scopes are the default scopes that Supabase Auth uses to request for user information. Do not add other scopes asSign In With Slack only supportsprofile,email,openid.


### Enter your Slack credentials into your Supabase project#
- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onSlackfrom the accordion list to expand and turnSlack Enabledto ON
- Enter yourSlack Client IDandSlack Client Secretsaved in the previous step
- ClickSave

You can also configure the Slack (OIDC) auth provider using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Configure Slack (OIDC) auth provider6curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  -H "Content-Type: application/json" \9  -d '{10    "external_slack_oidc_enabled": true,11    "external_slack_oidc_client_id": "your-slack-client-id",12    "external_slack_oidc_secret": "your-slack-client-secret"13  }'
```


### Add login code to your client app#
Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withslack_oidcas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'slack_oidc',4  })5}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()3}
```


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- Slack Developer Dashboard


================================================================================


# Login with Spotify
Source: https://supabase.com/docs/guides/auth/social-login/auth-spotify

Login with Spotify

To enable Spotify Auth for your project, you need to set up a Spotify OAuth application and add the application credentials to your Supabase Dashboard.


### Overview#
Setting up Spotify logins for your application consists of 3 parts:

- Create and configure a Spotify Project and App on theSpotify Developer Dashboard.
- Add your SpotifyAPI KeyandAPI Secret Keyto yourSupabase Project.
- Add the login code to yourSupabase JS Client App.


### Access your Spotify Developer account#
- Log intoSpotify
- Access theSpotify Developer Dashboard




### Find your callback URL#
The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onSpotifyfrom the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.


### Create a Spotify OAuth app#
- Log intoSpotify.
- Go to theSpotify Developer Dashboard
- ClickCreate an App
- Type yourApp name
- Type yourApp description
- Check the box to agree with theDeveloper TOS and Branding Guidelines
- ClickCreate
- Save yourClient ID
- Save yourClient Secret
- ClickEdit Settings

UnderRedirect URIs:

- Paste your Supabase Callback URL in the box
- ClickAdd
- ClickSaveat the bottom


### Enter your Spotify credentials into your Supabase project#
- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onSpotifyfrom the accordion list to expand and turnSpotify Enabledto ON
- Enter yourSpotify Client IDandSpotify Client Secretsaved in the previous step
- ClickSave

You can also configure the Spotify auth provider using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Configure Spotify auth provider6curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  -H "Content-Type: application/json" \9  -d '{10    "external_spotify_enabled": true,11    "external_spotify_client_id": "your-spotify-client-id",12    "external_spotify_secret": "your-spotify-client-secret"13  }'
```


### Add login code to your client app#
The following outlines the steps to sign in using Spotify with Supabase Auth.

Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withspotifyas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'spotify',4  })5}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()3}
```


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- Spotify Developer Dashboard


================================================================================


# Login with Twitch
Source: https://supabase.com/docs/guides/auth/social-login/auth-twitch

Login with Twitch

To enable Twitch Auth for your project, you need to set up a Twitch Application and add the Application OAuth credentials to your Supabase Dashboard.


### Overview#
Setting up Twitch logins for your application consists of 3 parts:

- Create and configure a Twitch ApplicationTwitch Developer Console
- Add your Twitch OAuth Consumer keys to yourSupabase Project
- Add the login code to yourSupabase JS Client App


### Access your Twitch Developer account#
- Go todev.twitch.tv.
- Click onLog in with Twitchat the top right to log in.
- If you have not already enabled 2-Factor Authentication for your Twitch Account, you will need to do that atTwitch Security Settingsbefore you can continue.



- Once logged in, go to theTwitch Developer Console.




### Find your callback URL#
The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onTwitchfrom the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.


### Create a Twitch application#


- Click on+ Register Your Applicationat the top right.



- Enter the name of your application.
- Type or paste yourOAuth Redirect URL(the callback URL from the previous step.)
- Select a category for your app.
- Check the CAPTCHA box and clickCreate.


### Retrieve your Twitch OAuth client ID and client secret#
- ClickManageat the right of your application entry in the list.



- Copy your Client ID.
- ClickNew Secretto create a new Client Secret.
- Copy your Client Secret.




### Add your Twitch credentials into your Supabase project#
- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onTwitchfrom the accordion list to expand and turnTwitch Enabledto ON
- Enter yourTwitch Client IDandTwitch Client Secretsaved in the previous step
- ClickSave


### Add login code to your client app#
Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withtwitchas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'twitch',4  })5}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()3}
```


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- Twitch Account
- Twitch Developer Console


================================================================================


# Login with X / Twitter
Source: https://supabase.com/docs/guides/auth/social-login/auth-twitter

Login with X / Twitter

To enable X / Twitter Auth for your project, you need to set up an X OAuth 2.0 application and add the application credentials in the Supabase Dashboard.


### Overview#
We recommend using theX / Twitter (OAuth 2.0)provider. The legacyTwitter (OAuth 1.0a)provider will be deprecated in future releases.

Setting up X / Twitter logins for your application consists of 3 parts:

- Create and configure an X Project and App on theX Developer Dashboard.
- Add your X OAuth 2.0Client IDandClient Secretto yourSupabase Project.
- Add the login code to yourSupabase JS Client App.


### Access your X developer account#
- Go todeveloper.x.com.
- Click onSign inat the top right to log in.


### Find your callback URL#
The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onX / Twitter (OAuth 2.0)from the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.


### Create an X OAuth app#
- Click+ Create Project.Enter your project name, clickNext.Select your use case, clickNext.Enter a description for your project, clickNext.Enter a name for your app, clickNext.Copy and save yourAPI KeyandAPI Secret Key(these are used for OAuth 1.0a, which is being deprecated).Click onApp settingsto proceed to next steps.
- At the bottom, you will findUser authentication settings. Click onSet up.
- UnderUser authentication settings, you can configureApp permissions.
- Make sure you turn ONRequest email from users.
- SelectWeb App...as theType of App.
- UnderApp infoconfigure the following.Enter yourCallback URL. Check theFind your callback URLsection above to learn how to obtain your callback URL.Enter yourWebsite URL(tip: tryhttp://127.0.0.1:portorhttp://www.localhost:portduring development)Enter yourTerms of service URL.Enter yourPrivacy policy URL.
- ClickSave.
- After saving, navigate toKeys and tokenson your App page.Scroll to the bottom of the page and copy yourClient ID.Click theRegeneratebutton next toClient Secret.In the confirmation modal, clickYes, regenerate.Copy and save yourClient Secret.

- Enter your project name, clickNext.
- Select your use case, clickNext.
- Enter a description for your project, clickNext.
- Enter a name for your app, clickNext.
- Copy and save yourAPI KeyandAPI Secret Key(these are used for OAuth 1.0a, which is being deprecated).
- Click onApp settingsto proceed to next steps.

- Enter yourCallback URL. Check theFind your callback URLsection above to learn how to obtain your callback URL.
- Enter yourWebsite URL(tip: tryhttp://127.0.0.1:portorhttp://www.localhost:portduring development)
- Enter yourTerms of service URL.
- Enter yourPrivacy policy URL.

- Scroll to the bottom of the page and copy yourClient ID.
- Click theRegeneratebutton next toClient Secret.
- In the confirmation modal, clickYes, regenerate.
- Copy and save yourClient Secret.


### Enter your X credentials into your Supabase project#
- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onX / Twitter (OAuth 2.0)from the accordion list to expand and turnX / Twitter (OAuth 2.0) Enabledto ON
- Enter yourX / Twitter (OAuth 2.0) Client IDandX / Twitter (OAuth 2.0) Client Secretsaved in the previous step
- ClickSave

You can also configure the X / Twitter (OAuth 2.0) auth provider using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Configure X / Twitter (OAuth 2.0) auth provider6curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  -H "Content-Type: application/json" \9  -d '{10    "external_x_enabled": true,11    "external_x_client_id": "your-x-client-id",12    "external_x_secret": "your-x-client-secret"13  }'
```


### Add login code to your client app#
Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withxas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'x',4  })5}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()3}
```


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- X Developer Dashboard


================================================================================


# SSO and Social Login with WorkOS
Source: https://supabase.com/docs/guides/auth/social-login/auth-workos

SSO and Social Login with WorkOS


### Use Social Login with WorkOS#

### Step 1. Create a WorkOS organization#
Log in to the WorkOS dashboard and visit the Organizations tab to create an organization.

Alternatively, you cancreate an organization via the WorkOS API.


### Step 2. Obtain yourClient IDandWORKOS_API_KEYvalues#


Visit the getting started page of theWorkOS Dashboard. Copy the following values from the Quickstart panel:

- WORKOS_CLIENT_ID
- WORKOS_API_KEY

You must be signed in to see these values.


### Step 3. Add your WorkOS credentials to your Supabase project#


You can also configure the WorkOS auth provider using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Configure WorkOS auth provider6curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  -H "Content-Type: application/json" \9  -d '{10    "external_workos_enabled": true,11    "external_workos_url": "https://api.workos.com",12    "external_workos_client_id": "your-workos-client-id",13    "external_workos_secret": "your-workos-client-secret"14  }'
```


### Step 4. Set your Supabase redirect URI in the WorkOS Dashboard#
Visit the WorkOS dashboard and click the redirects button in the left navigation panel.

On the redirects page, enter your Supabase project'sCallback URL (for OAuth)which you saved in the previous step, as shown below:




### Step 5. Add login code to your client app#
When a user signs in, callsignInWithOAuthwithworkosas the provider.

```javascript
1async function () {2  const { ,  } = await ..({3    : 'workos',4    : {5      : 'http://example.com/auth/v1/callback', // Make sure your redirect URL is configured in the Supabase Dashboard Auth settings6      : {7        : '<connection_id>',8      },9    },10  })1112  if (.) {13    (.) // use the redirect API for your server or framework14  }15}
```

You can find yourconnection_idin the WorkOS dashboard under the Organizations tab. Select your organization and then click View connection.

Within your specified callback URL, you'll exchange the code for a logged-in user profile:

```javascript
1import { NextResponse } from 'next/server'2import { createClient } from '@/utils/supabase/server'34export async function GET(request: Request) {5  const { searchParams, origin } = new URL(request.url)6  const code = searchParams.get('code')7  // if "next" is in param, use it as the redirect URL8  let next = searchParams.get('next') ?? '/'9  if (!next.startsWith('/')) {10    // if "next" is not a relative URL, use the default11    next = '/'12  }1314  if (code) {15    const supabase = await createClient()16    const { error } = await supabase.auth.exchangeCodeForSession(code)17    if (!error) {18      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer19      const isLocalEnv = process.env.NODE_ENV === 'development'20      if (isLocalEnv) {21        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host22        return NextResponse.redirect(`${origin}${next}`)23      } else if (forwardedHost) {24        return NextResponse.redirect(`https://${forwardedHost}${next}`)25      } else {26        return NextResponse.redirect(`${origin}${next}`)27      }28    }29  }3031  // return the user to an error page with instructions32  return NextResponse.redirect(`${origin}/auth/auth-code-error`)33}
```


### Resources#
- WorkOS Documentation


================================================================================


# Login with Zoom
Source: https://supabase.com/docs/guides/auth/social-login/auth-zoom

Login with Zoom

To enable Zoom Auth for your project, you need to set up a Zoom OAuth application and add the application credentials to your Supabase Dashboard.


### Overview#
Setting up Zoom logins for your application consists of 3 parts:

- Create and configure a Zoom OAuth App onZoom App Marketplace
- Add your Zoom OAuth keys to yourSupabase Project
- Add the login code to yourSupabase JS Client App


### Access your Zoom Developer account#
- Go tomarketplace.zoom.us.
- Click onSign Inat the top right to log in.




### Find your callback URL#
The next step requires a callback URL, which looks like this:https://<project-ref>.supabase.co/auth/v1/callback

- Go to yourSupabase Project Dashboard
- Click on theAuthenticationicon in the left sidebar
- Click onSign In / Providersunder the Configuration section
- Click onZoomfrom the accordion list to expand and you'll find yourCallback URL, you can clickCopyto copy it to the clipboard


### Local development#
When testing OAuth locally with the Supabase CLI, ensure your OAuth provider
is configured with the local Supabase Auth callback URL:

http://localhost:54321/auth/v1/callback

If this callback URL is missing or misconfigured, OAuth sign-in may fail or not redirect correctly during local development.

See thelocal development docsfor more details.

For testing OAuth locally with the Supabase CLI see thelocal development docs.


### Create a Zoom OAuth app#
- Go tomarketplace.zoom.us.
- Click onSign Inat the top right to log in.
- ClickBuild App(from the dropdown Develop)
- In the OAuth card, clickCreate
- Type the name of your app
- Choose app type
- ClickCreate

UnderApp credentials

- Copy and save yourClient ID.
- Copy and save yourClient secret.
- Add yourCallback URLin the OAuth allow list.

UnderRedirect URL for OAuth

- Paste yourCallback URL

UnderScopes

- Click onAdd scopes
- Click onUser
- Chooseuser:read
- ClickDone
- ClickContinue


### Enter your Zoom credentials into your Supabase project#
- Go to yourSupabase Project Dashboard
- In the left sidebar, click theAuthenticationicon (near the top)
- Click onProvidersunder the Configuration section
- Click onZoomfrom the accordion list to expand and turnZoom Enabledto ON
- Enter yourZoom Client IDandZoom Client Secretsaved in the previous step
- ClickSave

You can also configure the Zoom auth provider using the Management API:

```javascript
1# Get your access token from https://supabase.com/dashboard/account/tokens2export SUPABASE_ACCESS_TOKEN="your-access-token"3export PROJECT_REF="your-project-ref"45# Configure Zoom auth provider6curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \7  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \8  -H "Content-Type: application/json" \9  -d '{10    "external_zoom_enabled": true,11    "external_zoom_client_id": "your-zoom-client-id",12    "external_zoom_secret": "your-zoom-client-secret"13  }'
```


### Add login code to your client app#
Make sure you're using the rightsupabaseclient in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use thecreateClientfrom@supabase/supabase-js. If you're using Server-Side Rendering, see theServer-Side Auth guidefor instructions on creating your Supabase client.

When your user signs in, callsignInWithOAuth()withzoomas theprovider:

```javascript
1async function () {2  const { ,  } = await ..({3    : 'zoom',4  })5}
```

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When callingsignInWithOAuth, provide aredirectToURL which points to a callback route. This redirect URL should be added to yourredirect allow list.

In the browser,signInWithOAuthautomatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

```javascript
1await ..({2  ,3  : {4    : `http://example.com/auth/callback`,5  },6})
```

At the callback endpoint, handle the code exchange to save the user session.

Create a new file atapp/auth/callback/route.tsand populate with the following:

```javascript
1import { NextResponse } from 'next/server'2// The client you created from the Server-Side Auth instructions3import { createClient } from '@/utils/supabase/server'45export async function GET(request: Request) {6  const { searchParams, origin } = new URL(request.url)7  const code = searchParams.get('code')8  // if "next" is in param, use it as the redirect URL9  let next = searchParams.get('next') ?? '/'10  if (!next.startsWith('/')) {11    // if "next" is not a relative URL, use the default12    next = '/'13  }1415  if (code) {16    const supabase = await createClient()17    const { error } = await supabase.auth.exchangeCodeForSession(code)18    if (!error) {19      const forwardedHost = request.headers.get('x-forwarded-host') // original origin before load balancer20      const isLocalEnv = process.env.NODE_ENV === 'development'21      if (isLocalEnv) {22        // we can be sure that there is no load balancer in between, so no need to watch for X-Forwarded-Host23        return NextResponse.redirect(`${origin}${next}`)24      } else if (forwardedHost) {25        return NextResponse.redirect(`https://${forwardedHost}${next}`)26      } else {27        return NextResponse.redirect(`${origin}${next}`)28      }29    }30  }3132  // return the user to an error page with instructions33  return NextResponse.redirect(`${origin}/auth/auth-code-error`)34}
```

When your user signs out, callsignOut()to remove them from the browser session and any objects from localStorage:

```javascript
1async function () {2  const {  } = await ..()3}
```


### Resources#
- Supabase - Get started for free
- Supabase JS Client
- Zoom App Marketplace


================================================================================


# Auth0
Source: https://supabase.com/docs/guides/auth/third-party/auth0

Auth0


### Use Auth0 with your Supabase project
Use Auth0 with your Supabase project

Auth0 can be used as a third-party authentication provider alongside Supabase Auth, or standalone, with your Supabase project.


### Getting started#

### Setup the Supabase client library#
```javascript
1import { createClient } from '@supabase/supabase-js'2import Auth0Client from '@auth0/auth0-spa-js'34const auth0 = new Auth0Client({5  domain: '<AUTH0_DOMAIN>',6  clientId: '<AUTH0_CLIENT_ID>',7  authorizationParams: {8    redirect_uri: '<MY_CALLBACK_URL>',9  },10})1112const supabase = createClient(13  'https://<supabase-project>.supabase.co',14  'SUPABASE_PUBLISHABLE_KEY',15  {16    accessToken: async () => {17      const accessToken = await auth0.getTokenSilently()1819      // Alternatively you can use (await auth0.getIdTokenClaims()).__raw to20      // use an ID token instead.2122      return accessToken23    },24  }25)
```


### Add a new Third-Party Auth integration to your project#
In the dashboard navigate to your project'sAuthentication settingsand find the Third-Party Auth section to add a new integration.

In the CLI add the following config to yoursupabase/config.tomlfile:

```javascript
1[auth.third_party.auth0]2enabled = true3tenant = "<id>"4tenant_region = "<region>" # if your tenant has a region
```


### Use an Auth0 Action to assign the authenticated role#
Your Supabase project inspects theroleclaim present in all JWTs sent to it, to assign the correct Postgres role when using the Data API, Storage or Realtime authorization.

By default, Auth0 JWTs (both access token and ID token) do not contain aroleclaim in them. If you were to send such a JWT to your Supabase project, theanonrole would be assigned when executing the Postgres query. Most of your app's logic will be accessible by theauthenticatedrole.

A recommended approach to do this is to configure theonExecutePostLoginAuth0 Actionwhich will add the custom claim:

```javascript
1exports.onExecutePostLogin = async (event, api) => {2  api.accessToken.setCustomClaim('role', 'authenticated')3}
```


### Limitations#
At this time, Auth0 tenants with the followingsigning algorithmsare not supported:

- HS256 (HMAC with SHA-256) -- also known as symmetric JWTs
- PS256 (RSA-PSS with SHA-256)


================================================================================


# Amazon Cognito (Amplify)
Source: https://supabase.com/docs/guides/auth/third-party/aws-cognito

Amazon Cognito (Amplify)


### Use Amazon Cognito via Amplify or standalone with your Supabase project
Use Amazon Cognito via Amplify or standalone with your Supabase project

Amazon Cognito User Pools (via AWS Amplify or on its own) can be used as a third-party authentication provider alongside Supabase Auth, or standalone, with your Supabase project.


### Getting started#

### Setup the Supabase client library#
```javascript
1import { fetchAuthSession, Hub } from 'aws-amplify/auth'23const supabase = createClient(4  'https://<supabase-project>.supabase.co',5  'SUPABASE_PUBLISHABLE_KEY',6  {7    accessToken: async () => {8      const tokens = await fetchAuthSession()910      // Alternatively you can use tokens?.idToken instead.11      return tokens?.accessToken12    },13  }14)1516// if you're using Realtime you also need to set up a listener for Cognito auth changes17Hub.listen('auth', () => {18  fetchAuthSession().then((tokens) => supabase.realtime.setAuth(tokens?.accessToken))19})
```


### Add a new Third-Party Auth integration to your project#
In the dashboard navigate to your project'sAuthentication settingsand find the Third-Party Auth section to add a new integration.

In the CLI add the following config to yoursupabase/config.tomlfile:

```javascript
1[auth.third_party.aws_cognito]2enabled = true3user_pool_id = "<id>"4user_pool_region = "<region>"
```


### Use a pre-token generation trigger to assign the authenticated role#
Your Supabase project inspects theroleclaim present in all JWTs sent to it, to assign the correct Postgres role when using the Data API, Storage or Realtime authorization.

By default, Amazon Cognito JWTs (both ID token and access tokens) do not contain aroleclaim in them. If you were to send such a JWT to your Supabase project, theanonrole would be assigned when executing the Postgres query. Most of your app's logic will be accessible by theauthenticatedrole.

A recommended approach to do this is to configure aPre-Token Generation TriggereitherV1_0(ID token only) orV2_0(both access and ID token). To do this you will need to create a new Lambda function (in any language and runtime) and assign it to theAmazon Cognito User Pool's Lambda Triggers configuration. For example, the Lambda function should look similar to this:

```javascript
1export const handler = async (event) => {2  event.response = {3    claimsOverrideDetails: {4      claimsToAddOrOverride: {5        role: 'authenticated',6      },7    },8  }910  return event11}
```


================================================================================


# Clerk
Source: https://supabase.com/docs/guides/auth/third-party/clerk

Clerk


### Use Clerk with your Supabase project
Use Clerk with your Supabase project

Clerk can be used as a third-party authentication provider alongside Supabase Auth, or standalone, with your Supabase project.


### Getting started#
Getting started is incredibly easy. Start off by visitingClerk's Connect with Supabase pageto configure your Clerk instance for Supabase compatibility.

Finally add anew Third-Party Auth integration with Clerkin the Supabase dashboard.


### Configure for local development or self-hosting#
When developing locally or self-hosting with the Supabase CLI, add the following config to yoursupabase/config.tomlfile:

```javascript
1[auth.third_party.clerk]2enabled = true3domain = "example.clerk.accounts.dev"
```

You will still need to configure your Clerk instance for Supabase compatibility.


### Manually configuring your Clerk instance#
If you are not able to useClerk's Connect with Supabase pageto configure your Clerk instance for working with Supabase, follow these steps.


### Setup the Supabase client library#
```javascript
1const supabaseClient = createClient(2    process.env.NEXT_PUBLIC_SUPABASE_URL!,3    process.env.NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY!,4    {5      // Session accessed from Clerk SDK, either as Clerk.session (vanilla6      // JavaScript) or useSession (React)7      accessToken: async () => session?.getToken() ?? null,8    }9  )
```


### Using RLS policies#
Once you've configured the Supabase client library to use Clerk session tokens, you can use RLS policies to secure access to your project's database, Storage objects and Realtime channels.

The recommended way to design RLS policies with Clerk is to use claims present in your Clerk session token to allow or reject access to your project's data. CheckClerk's docson the available JWT claims and their values.


### Example: Check user organization role#
```javascript
1create policy "Only organization admins can insert in table"2on secured_table3for insert4to authenticated5with check (6  (((select auth.jwt()->>'org_role') = 'org:admin') or ((select auth.jwt()->'o'->>'rol') = 'admin'))7    and8  (organization_id = (select coalesce(auth.jwt()->>'org_id', auth.jwt()->'o'->>'id')))9);
```

This RLS policy checks that the newly inserted row in the table has the user's declared organization ID in theorganization_idcolumn. Additionally it ensures that they're anorg:admin.

This way only organization admins can add rows to the table, for organizations they're a member of.


### Example: Check user has passed second factor verification#
```javascript
1create policy "Only users that have passed second factor verification can read from table"2on secured_table3as restrictive4for select5to authenticated6using (7  ((select auth.jwt()->'fva'->>1) != '-1')8);
```

This example uses a restrictive RLS policy checks that thesecond factor verificationage element in thefvaclaim is not'-1'indicating the user has passed through second factor verification.


### Deprecated integration with JWT templates#
As of 1st April 2025 the previously availableClerk Integration with Supabaseis considered deprecated and is no longer recommended for use. All projects using the deprecated integration will be excluded from Third-Party Monthly Active User (TP-MAU) charges until at least 1st January 2026.

This integration used low-level primitives that are still available in Supabase and Clerk, such as aconfigurable JWT secretandJWT templates from Clerk. This enables you to keep using it in an unofficial manner, though only limited support will be provided from Supabase.

Deprecation is done for the following reasons:

- Sharing your project's JWT secret with a third-party is a problematic security practice
- Rotating the project's JWT secret in this case almost always results in significant downtime for your application
- Additional latency togenerate a new JWTfor use with Supabase, instead of using the Clerksession tokens


================================================================================


# Firebase Auth
Source: https://supabase.com/docs/guides/auth/third-party/firebase-auth

Firebase Auth


### Use Firebase Auth with your Supabase project
Use Firebase Auth with your Supabase project

Firebase Auth can be used as a third-party authentication provider alongside Supabase Auth, or standalone, with your Supabase project.


### Getting started#

### Setup the Supabase client library#
Creating a client for the Web is as easy as passing theaccessTokenasync function. This function shouldreturn the Firebase Auth JWT of the current user(or null if no such user) is found.

```javascript
1import { createClient } from '@supabase/supabase-js'23const supabase = createClient(4  'https://<supabase-project>.supabase.co',5  'SUPABASE_PUBLISHABLE_KEY',6  {7    accessToken: async () => {8      return (await firebase.auth().currentUser?.getIdToken(/* forceRefresh */ false)) ?? null9    },10  }11)
```

Make sure the all users in your application have therole: 'authenticated'custom claimset. If you're using theonCreateCloud Function to add this custom claim to newly signed up users, you will need to callgetIdToken(/* forceRefresh */ true)immediately after sign up as theonCreatefunction does not run synchronously.


### Add a new Third-Party Auth integration to your project#
In the dashboard navigate to your project'sAuthentication settingsand find the Third-Party Auth section to add a new integration.

In the CLI add the following config to yoursupabase/config.tomlfile:

```javascript
1[auth.third_party.firebase]2enabled = true3project_id = "<id>"
```


### Adding an extra layer of security to your project's RLS policies (self-hosting only)#
Follow this section carefully to prevent unauthorized access to your project's data when self-hosting.

When using the Supabase hosted platform, following this step is optional.

Firebase Auth uses a single set of JWT signing keys for all projects. This means that JWTs issued from an unrelated Firebase project to yours could access data in your Supabase project.

When using the Supabase hosted platform, JWTs coming from Firebase project IDs you have not registered will be rejected before they reach your database. When self-hosting implementing this mechanism is your responsibility. An easy way to guard against this is to create and maintain the following RLS policies forall of your tables in thepublicschema. You should also attach this policy toStoragebuckets orRealtimechannels.

It's recommended you use arestrictive Postgres Row-Level Security policy.

Restrictive RLS policies differ from regular (or permissive) policies in that they use theas restrictiveclause when being defined. They do not grant permissions, but rather restrict any existing or future permissions. They're great for cases like this where the technical limitations of Firebase Auth remain separate from your app's logic.

Postgres has two types of policies: permissive and restrictive. This example uses restrictive policies so make sure you don't omit theas restrictiveclause.

This is an example of such an RLS policy that will restrict access to only your project's (denoted with<firebase-project-id>) users, and not any other Firebase project.

```javascript
1create policy "Restrict access to Supabase Auth and Firebase Auth for project ID <firebase-project-id>"2  on table_name3  as restrictive4  to authenticated5  using (6    (auth.jwt()->>'iss' = 'https://<project-ref>.supabase.co/auth/v1')7    or8    (9        auth.jwt()->>'iss' = 'https://securetoken.google.com/<firebase-project-id>'10        and11        auth.jwt()->>'aud' = '<firebase-project-id>'12     )13  );
```

If you have a lot of tables in your app, or need to manage complex RLS policies forStorageorRealtimeit can be useful to define astable Postgres functionthat performs the check to cut down on duplicate code. For example:

```javascript
1create function public.is_supabase_or_firebase_project_jwt()2  returns bool3  language sql4  stable5  returns null on null input6  return (7    (auth.jwt()->>'iss' = 'https://<project-ref>.supabase.co/auth/v1')8    or9    (10        auth.jwt()->>'iss' = concat('https://securetoken.google.com/<firebase-project-id>')11        and12        auth.jwt()->>'aud' = '<firebase-project-id>'13     )14  );
```

Make sure you substitute<project-ref>with your Supabase project's ID and the<firebase-project-id>to your Firebase Project ID. Then the restrictive policies on all your tables, buckets and channels can be simplified to be:

```javascript
1create policy "Restrict access to correct Supabase and Firebase projects"2  on table_name3  as restrictive4  to authenticated5  using ((select public.is_supabase_or_firebase_project_jwt()) is true);
```


### Assign the "role" custom claim#
Your Supabase project inspects theroleclaim present in all JWTs sent to it, to assign the correct Postgres role when using the Data API, Storage or Realtime authorization.

By default, Firebase JWTs do not contain aroleclaim in them. If you were to send such a JWT to your Supabase project, theanonrole would be assigned when executing the Postgres query. Most of your app's logic will be accessible by theauthenticatedrole.


### Use Firebase Authentication functions to assign the authenticated role#
You have two choices to set up a Firebase Authentication function depending on your Firebase project's configuration:

```javascript
1import { beforeUserCreated, beforeUserSignedIn } from 'firebase-functions/v2/identity'23export const beforecreated = beforeUserCreated((event) => {4  return {5    customClaims: {6      // The Supabase project will use this role to assign the `authenticated`7      // Postgres role.8      role: 'authenticated',9    },10  }11})1213export const beforesignedin = beforeUserSignedIn((event) => {14  return {15    customClaims: {16      // The Supabase project will use this role to assign the `authenticated`17      // Postgres role.18      role: 'authenticated',19    },20  }21})
```

Note that instead of usingcustomClaimsyou can instead usesessionClaims. The difference is thatsession_claimsare not saved in the Firebase user profile, but remain valid for as long as the user is signed in.

Finally deploy your functions for the changes to take effect:

```javascript
1firebase deploy --only functions
```

Note that these functions are only called on new sign-ups and sign-ins. Existing users will not have these claims in their ID tokens. You will need to use the admin SDK to assign the role custom claim to all users. Make sure you do this after the blocking Firebase Authentication functions as described above are deployed.


### Use the admin SDK to assign the role custom claim to all users#
You need to run a script that will assign therole: 'authenticated'custom claim to all of your existing Firebase Authentication users. You can do this by combining thelist usersandset custom user claimsadmin APIs. An example script is provided below:

```javascript
1'use strict';2const { initializeApp } = require('firebase-admin/app');3const { getAuth } = require('firebase-admin/auth');4initializeApp();56async function setRoleCustomClaim() => {7  let nextPageToken = undefined89  do {10    const listUsersResult = await getAuth().listUsers(1000, nextPageToken)1112    nextPageToken = listUsersResult.pageToken1314    await Promise.all(listUsersResult.users.map(async (userRecord) => {15      try {16        await getAuth().setCustomUserClaims(userRecord.id, {17          role: 'authenticated'18        })19      } catch (error) {20        console.error('Failed to set custom role for user', userRecord.id)21      }22    })23  } while (nextPageToken);24};2526setRoleCustomClaim().then(() => process.exit(0))
```

After all users have received therole: 'authenticated'claim, it will appear in all newly issued ID tokens for the user.


================================================================================


# Third-party auth
Source: https://supabase.com/docs/guides/auth/third-party/overview

Third-party auth


### First-class support for authentication providers
First-class support for authentication providers

Supabase has first-class support for these third-party authentication providers:

- Clerk
- Firebase Auth
- Auth0
- AWS Cognito (with or without AWS Amplify)
- WorkOS

You can use these providers alongside Supabase Auth, or on their own, to access theData API (REST and GraphQL),Storage,RealtimeandFunctionsfrom your existing apps.

If you already have production apps using one of these authentication providers, and would like to use a Supabase feature, you no longer need to migrate your users to Supabase Auth or use workarounds like translating JWTs into the Supabase Auth format and using your project's signing secret.


### How does it work?#
To use Supabase products like Data APIs for your Postgres database, Storage or Realtime, you often need to send access tokens or JWTs via the Supabase client libraries or via the REST API. Third-party auth support means that when you add a new integration with one of these providers, the API will trust JWTs issued by the provider similar to how it trusts JWTs issued by Supabase Auth.

This is made possible if the providers are using JWTs signed with asymmetric keys, which means that the Supabase APIs will be able to only verify but not create JWTs.


### Limitations#
There are some limitations you should be aware of when using third-party authentication providers with Supabase.


### Pricing#
$0.00325per Third-Party MAU. You are only charged for usage exceeding your subscription
plan's quota.

For a detailed breakdown of how charges are calculated, refer toManage Monthly Active Third-Party Users usage.


================================================================================


# WorkOS
Source: https://supabase.com/docs/guides/auth/third-party/workos

WorkOS


### Use WorkOS with your Supabase project
Use WorkOS with your Supabase project

WorkOS can be used as a third-party authentication provider alongside Supabase Auth, or standalone, with your Supabase project.


### Getting started#

### Setup the Supabase client library#
```javascript
1import { createClient } from '@supabase/supabase-js'2import { createClient as createAuthKitClient } from '@workos-inc/authkit-js'34const authkit = await createAuthKitClient('WORKOS_CLIENT_ID', {5  apiHostname: '<WORKOS_AUTH_DOMAIN>',6})78const supabase = createClient(9  'https://<supabase-project>.supabase.co',10  'SUPABASE_PUBLISHABLE_KEY',11  {12    accessToken: async () => {13      return authkit.getAccessToken()14    },15  }16)
```


### Add a new Third-Party Auth integration to your project#
In the dashboard navigate to your project'sAuthentication settingsand find the Third-Party Auth section to add a new integration.


### Set up a JWT template to add the authenticated role.#
Your Supabase project inspects theroleclaim present in all JWTs sent to it, to assign the correct Postgres role when using the Data API, Storage or Realtime authorization.

WorkOS JWTs already contain aroleclaim that corresponds to the user's role in their organization. It is necessary to adjust theroleclaim to be"authenticated"like Supabase expects. This can be done using JWT templates (navigate to Authentication -> Sessions -> JWT Template in the WorkOS Dashboard).

This template overrides theroleclaim to meet Supabase's expectations, and adds the WorkOS role in a newuser_roleclaim:

```javascript
1{2  "role": "authenticated",3  "user_role": {{organization_membership.role}}4}
```


================================================================================


# Users
Source: https://supabase.com/docs/guides/auth/users

Users

Auserin Supabase Auth is someone with a user ID, stored in the Auth schema. Once someone is a user, they can be issued an Access Token, which can be used to access Supabase endpoints. The token is tied to the user, so you can restrict access to resources viaRLS policies.


### Permanent and anonymous users#
Supabase distinguishes between permanent and anonymous users.

- Permanent usersare tied to a piece of Personally Identifiable Information (PII), such as an email address, a phone number, or a third-party identity. They can use these identities to sign back into their account after signing out.
- Anonymous usersaren't tied to any identities. They have a user ID and a personalized Access Token, but they have no way of signing back in as the same user if they are signed out.

Anonymous users are useful for:

- E-commerce applications, to create shopping carts before checkout
- Full-feature demos without collecting personal information
- Temporary or throw-away accounts

See theAnonymous Signins guideto learn more about anonymous users.

Just like permanent users, anonymous users use theauthenticatedrole for database access.

Theanonrole is for those who aren't signed in at all and are not tied to any user ID. We refer to these as unauthenticated or public users.


### The user object#
The user object stores all the information related to a user in your application. The user object can be retrieved using one of these methods:

A user can sign in with one of the following methods:

- Password-based method (with email or phone)
- Passwordless method (with email or phone)
- OAuth
- SAML SSO

An identity describes the authentication method that a user can use to sign in. A user can have multiple identities. These are the types of identities supported:

- Email
- Phone
- OAuth
- SAML

A user with an email or phone identity will be able to sign in with either a password or passwordless method (e.g. use a one-time password (OTP) or magic link). By default, a user with an unverified email or phone number will not be able to sign in.

The user object contains the following attributes:


### Resources#
- User Management guide


================================================================================


# Working With Arrays
Source: https://supabase.com/docs/guides/database/arrays

Working With Arrays

Postgres supports flexiblearray types. These arrays are also supported in the Supabase Dashboard and in the JavaScript API.


### Create a table with an array column#
Create a test table with a text array (an array of strings):


### Insert a record with an array value#

### View the results#
You should see:

```javascript
1| id  | textarray               |2| --- | ----------------------- |3| 1   | ["Harry","Larry","Moe"] |
```


### Query array data#
Postgres uses 1-based indexing (e.g.,textarray[1]is the first item in the array).

To select the first item from the array and get the total length of the array:

```javascript
1SELECT textarray[1], array_length(textarray, 1) FROM arraytest;
```

returns:

```javascript
1| textarray | array_length |2| --------- | ------------ |3| Harry     | 3            |
```


### Resources#
- Supabase JS Client
- Supabase - Get started for free
- Postgres Arrays


================================================================================


# Connecting with Beekeeper Studio
Source: https://supabase.com/docs/guides/database/beekeeper-studio

Connecting with Beekeeper Studio

Beekeeper Studio Communityis a free GUI tool for interacting with databases.


### Create a new connection
In Beekeeper, create a new Postgres connection.




### Get your connection credentials
Get your connection credentials from theConnectpanel. You will need:

- host
- username
- password
- port

Add your credentials to Beekeeper's connection form




### Download your SSL Certificate
Download your SSL certificate from the Dashboard'sDatabase Settings

Add your SSL to the connection form


### Test and connect
Test your connection and then connect




================================================================================


# Connect to your database
Source: https://supabase.com/docs/guides/database/connecting-to-postgres

Connect to your database


### Supabase provides multiple methods to connect to your Postgres database, whether you’re working on the frontend, backend, or utilizing serverless functions.
Supabase provides multiple methods to connect to your Postgres database, whether you’re working on the frontend, backend, or utilizing serverless functions.


### How to connect to your Postgres databases#
How you connect to your database depends on where you're connecting from:

- For frontend applications, use theData API
- For Postgres clients, use a connection stringFor single sessions (for example, database GUIs) or Postgres native commands (for example, using client applications likepg_dump,migrations,backup-restore, or specifying connections forreplication) use thedirect connection stringif your environment supports IPv6. IPv4 available asAdd-on.For application traffic from persistent clients, and support for both IPv4 and IPv6, usepooler session modeFor application traffic from temporary clients (for example, serverless or edge functions) usepooler transaction mode

- For single sessions (for example, database GUIs) or Postgres native commands (for example, using client applications likepg_dump,migrations,backup-restore, or specifying connections forreplication) use thedirect connection stringif your environment supports IPv6. IPv4 available asAdd-on.
- For application traffic from persistent clients, and support for both IPv4 and IPv6, usepooler session mode
- For application traffic from temporary clients (for example, serverless or edge functions) usepooler transaction mode


### Quickstarts#
Prisma

Drizzle

Postgres.js

pgAdmin

PSQL

DBeaver

Metabase

Beekeeper Studio


### Data APIs and client libraries#
The Data APIs allow you to interact with your database using REST or GraphQL requests. You can use these APIs to fetch and insert data from the frontend, as long as you haveRLSenabled.

- REST
- GraphQL

For convenience, you can also use theSupabase client libraries, which wrap the Data APIs with a developer-friendly interface and automatically handle authentication:

- JavaScript
- Flutter
- Swift
- Python
- C#
- Kotlin


### Direct connection#
The direct connection string connects directly to your Postgres instance. It is ideal for persistent servers, such as virtual machines (VMs) and long-lasting containers. Examples include AWS EC2 machines, Fly.io VMs, and DigitalOcean Droplets.

Direct connections use IPv6 by default. If your environment doesn't support IPv6, useSupavisor session modeor get theIPv4 add-on.

The connection string looks like this:

```javascript
1postgresql://postgres:[YOUR-PASSWORD]@db.abcdefghijklmnopqrst.supabase.co:5432/postgres
```

Get your project's direct connection string from your project dashboard by clickingConnect.


### Poolers#
Every Supabase project includes a connection pooler. This is ideal for persistent servers when IPv6 is not supported.


### Pooler session mode#
The session mode connection string connects to your Postgres instance via a proxy. This is only recommended as an alternative to a Direct Connection, when connecting via an IPv4 network.

The connection string looks like this:

```javascript
1postgres://postgres.apbkobhfnmcqqzqeeqss:[YOUR-PASSWORD]@aws-0-[REGION].pooler.supabase.com:5432/postgres
```

Get your project's Session pooler connection string from your project dashboard by clickingConnect.


### Pooler transaction mode#
The transaction mode connection string connects to your Postgres instance via a proxy which serves as a connection pooler. This is ideal for serverless or edge functions, which require many transient connections.

Transaction mode does not supportprepared statements. To avoid errors,turn off prepared statementsfor your connection library.

The connection string looks like this:

```javascript
1postgres://postgres:[YOUR-PASSWORD]@db.abcdefghijklmnopqrst.supabase.co:6543/postgres
```

Get your project's Transaction pooler connection string from your project dashboard by clickingConnect.


### Dedicated pooler#
For paying customers, we provision a Dedicated Pooler (PgBouncer) that's co-located with your Postgres database. This will require you to connect with IPv6 or, if that's not an option, you can use theIPv4 add-on.

The Dedicated Pooler ensures best performance and latency, while using up more of your project's compute resources. If your network supports IPv6 or you have the IPv4 add-on, we encourage you to use the Dedicated Pooler over the Shared Pooler.

Get your project's Dedicated pooler connection string from your project dashboard by clickingConnect.

PgBouncer always runs in Transaction mode and the current version does not support prepared statement (will be added in a few weeks).


### More about connection pooling#
Connection pooling improves database performance by reusing existing connections between queries. This reduces the overhead of establishing connections and improves scalability.

You can use an application-side pooler or a server-side pooler (Supabase automatically provides one called Supavisor), depending on whether your backend is persistent or serverless.


### Application-side poolers#
Application-side poolers are built into connection libraries and API servers, such as Prisma, SQLAlchemy, and PostgREST. They maintain several active connections with Postgres or a server-side pooler, reducing the overhead of establishing connections between queries. When deploying to static architecture, such as long-standing containers or VMs, application-side poolers are satisfactory on their own.


### Serverside poolers#
Postgres connections are like a WebSocket. Once established, they are preserved until the client (application server) disconnects. A server might only make a single 10 ms query, but needlessly reserve its database connection for seconds or longer.

Serverside-poolers, such as Supabase'sSupavisorin transaction mode, sit between clients and the database and can be thought of as load balancers for Postgres connections.

They maintain hot connections with the database and intelligently share them with clients only when needed, maximizing the amount of queries a single connection can service. They're best used to manage queries from auto-scaling systems, such as edge and serverless functions.


### Connecting with SSL#
You should connect to your database using SSL wherever possible, to prevent snooping and man-in-the-middle attacks.

You can obtain your connection info and Server root certificate from your application's dashboard:




### Resources#
- Connection management
- Connecting with psql
- Importing data into Supabase


### Troubleshooting and Postgres connection string FAQs#
Below are answers to common challenges and queries.


### What is a “connection refused” error?#
A “Connection refused” error typically means your database isn’t reachable. Ensure your Supabase project is running, confirm your database’s connection string, check firewall settings, and validate network permissions.


### What is the “FATAL: Password authentication failed” error?#
This error occurs when your credentials are incorrect. Double-check your username and password from the Supabase dashboard. If the problem persists, reset your database password from the project settings.


### How do you connect using IPv4?#
Supabase’s default direct connection supports IPv6 only. To connect over IPv4, consider using the Supavisor session or transaction modes, or a connection pooler (shared or dedicated), which support both IPv4 and IPv6.


### Where is the Postgres connection string in Supabase?#
Your connection string is located in the Supabase Dashboard. Click theConnectbutton at the top of the page.


### Can you use Supavisor and PgBouncer together?#
You can technically use both, but it’s not recommended unless you’re specifically trying to increase the total number of concurrent client connections. In most cases, it is better to choose either PgBouncer or Supavisor for pooled or transaction-based traffic. Direct connections remain the best choice for long-lived sessions, and, if IPv4 is required for those sessions, Supavisor session mode can be used as an alternative. Running both poolers simultaneously increases the risk of hitting your database’s maximum connection limit on smaller compute tiers.


### How does the default pool size work?#
Supavisor and PgBouncer work independently, but both reference the same pool size setting. For example, If you set the pool size to 30, Supavisor can open up to 30 server side connections to Postgres. These connections are shared between the session mode port (5432) and the transaction mode port (6543). Each mode can use up to 30 connections independently, or split them between both, but the total combined connections across both modes cannot exceed 30. PgBouncer can also open up to 30 connections under the same limit. If both poolers are active and reach their roles/modes limits at the same time, you could have as many as 60 backend connections hitting your database, in addition to any direct connections. You can adjust the pool size inDatabase settingsin the dashboard.


### What is the difference between client connections and backend connections?#
There are two different limits to understand when working with poolers. The first is client connections, which refers to how many clients can connect to a pooler at the same time. This number is capped by yourcompute tier’s “max pooler clients” limit, and it applies independently to Supavisor and PgBouncer. The second is backend connections, which is the number of active connections a pooler opens to Postgres. This number is set by the pool size for that pooler.

```javascript
1Total backend load on Postgres =2 Direct connections +3 Supavisor backend connections (≤ supavisor_pool_size) +4 PgBouncer backend connections (≤ pgbouncer_pool_size)5≤ Postgres max connections for your compute instance
```


### What is the max pooler clients limit?#
The “max pooler clients” limit for your compute tier applies separately to Supavisor and PgBouncer. One pooler reaching its client limit does not affect the other. When a pooler reaches this limit, it stops accepting new client connections until existing ones are closed, but the other pooler remains unaffected. You can check your tier’s connection limits in thecompute and disk limits documentation.


### Where can you see current connection usage?#
You can track connection usage from theObservabilitysection in your project dashboard. There are three key reports:

- Database Connections:shows total active connections by role (this includes direct and pooled connections).
- Dedicated Pooler Client Connections:shows the number of active client connections to PgBouncer.
- Shared Pooler (Supavisor) Client Connections:shows the number of active client connections to Supavisor.

Keep in mind that the Roles page is not real-time, it shows the connection count from the last refresh. If you need up-to-the-second data, set up Grafana or run the query againstpg_stat_activitydirectly in SQL Editor. We have a few helpful queries for checking connections.

```javascript
1-- Count connections by application and user name2select3  count(usename),4  count(application_name),5  application_name,6  usename7from8  pg_stat_ssl9  join pg_stat_activity on pg_stat_ssl.pid = pg_stat_activity.pid10group by usename, application_name;
```

```javascript
1-- View all connections2 SELECT3   pg_stat_activity.pid,4   ssl AS ssl_connection,5   datname AS database,6   usename AS connected_role,7   application_name,8   client_addr,9   query,10   query_start,11   state,12   backend_start13FROM pg_stat_ssl14JOIN pg_stat_activity15 ON pg_stat_ssl.pid = pg_stat_activity.pid;
```


### Why are there active connections when the app is idle?#
Even if your application isn’t making queries, some Supabase services keep persistent connections to your database. For example, Storage, PostgREST, and our health checker all maintain long-lived connections. You usually see a small baseline of active connections from these services.


### Why do connection strings have different ports?#
Different modes use different ports:

- Direct connection:5432(database server)
- PgBouncer:6543(database server)
- Supavisor transaction mode:6543(separate server)
- Supavisor session mode:5432(separate server)

The port helps route the connection to the right pooler/mode.


### Does connection pooling affect latency?#
Because the dedicated pooler is hosted on the same machine as your database, it connects with lower latency than the shared pooler, which is hosted on a separate server. Direct connections have no pooler overhead but require IPv6 unless you have the IPv4 add-on.


### How to choose the right connection method?#
Direct connection:

- Best for: persistent backend services
- Use for migrations, pg_dump, backup and management tools
- Limitation: IPv6 only by default. IPv4 available asAdd-on.

Shared pooler:

- Best for: general-purpose connections (supports IPv4 and IPv6)Supavisor session mode → persistent backend that require IPv4Supavisor transaction mode → serverless functions or short-lived tasks
- Use for application runtime traffic (queries, writes)

- Supavisor session mode → persistent backend that require IPv4
- Supavisor transaction mode → serverless functions or short-lived tasks

Dedicated pooler (paid tier):

- Best for: high-performance apps that need dedicated resources
- Use for application runtime traffic (queries, writes)
- Uses PgBouncer

You can follow the decision flow in the connection method diagram to quickly choose the right option for your environment.


================================================================================


# Connect to your database
Source: https://supabase.com/docs/guides/database/connecting-to-postgres/serverless-drivers

Connect to your database


### Supabase provides multiple methods to connect to your Postgres database, whether you’re working on the frontend, backend, or utilizing serverless functions.
Supabase provides multiple methods to connect to your Postgres database, whether you’re working on the frontend, backend, or utilizing serverless functions.


### How to connect to your Postgres databases#
How you connect to your database depends on where you're connecting from:

- For frontend applications, use theData API
- For Postgres clients, use a connection stringFor single sessions (for example, database GUIs) or Postgres native commands (for example, using client applications likepg_dump,migrations,backup-restore, or specifying connections forreplication) use thedirect connection stringif your environment supports IPv6. IPv4 available asAdd-on.For application traffic from persistent clients, and support for both IPv4 and IPv6, usepooler session modeFor application traffic from temporary clients (for example, serverless or edge functions) usepooler transaction mode

- For single sessions (for example, database GUIs) or Postgres native commands (for example, using client applications likepg_dump,migrations,backup-restore, or specifying connections forreplication) use thedirect connection stringif your environment supports IPv6. IPv4 available asAdd-on.
- For application traffic from persistent clients, and support for both IPv4 and IPv6, usepooler session mode
- For application traffic from temporary clients (for example, serverless or edge functions) usepooler transaction mode


### Quickstarts#
Prisma

Drizzle

Postgres.js

pgAdmin

PSQL

DBeaver

Metabase

Beekeeper Studio


### Data APIs and client libraries#
The Data APIs allow you to interact with your database using REST or GraphQL requests. You can use these APIs to fetch and insert data from the frontend, as long as you haveRLSenabled.

- REST
- GraphQL

For convenience, you can also use theSupabase client libraries, which wrap the Data APIs with a developer-friendly interface and automatically handle authentication:

- JavaScript
- Flutter
- Swift
- Python
- C#
- Kotlin


### Direct connection#
The direct connection string connects directly to your Postgres instance. It is ideal for persistent servers, such as virtual machines (VMs) and long-lasting containers. Examples include AWS EC2 machines, Fly.io VMs, and DigitalOcean Droplets.

Direct connections use IPv6 by default. If your environment doesn't support IPv6, useSupavisor session modeor get theIPv4 add-on.

The connection string looks like this:

```javascript
1postgresql://postgres:[YOUR-PASSWORD]@db.abcdefghijklmnopqrst.supabase.co:5432/postgres
```

Get your project's direct connection string from your project dashboard by clickingConnect.


### Poolers#
Every Supabase project includes a connection pooler. This is ideal for persistent servers when IPv6 is not supported.


### Pooler session mode#
The session mode connection string connects to your Postgres instance via a proxy. This is only recommended as an alternative to a Direct Connection, when connecting via an IPv4 network.

The connection string looks like this:

```javascript
1postgres://postgres.apbkobhfnmcqqzqeeqss:[YOUR-PASSWORD]@aws-0-[REGION].pooler.supabase.com:5432/postgres
```

Get your project's Session pooler connection string from your project dashboard by clickingConnect.


### Pooler transaction mode#
The transaction mode connection string connects to your Postgres instance via a proxy which serves as a connection pooler. This is ideal for serverless or edge functions, which require many transient connections.

Transaction mode does not supportprepared statements. To avoid errors,turn off prepared statementsfor your connection library.

The connection string looks like this:

```javascript
1postgres://postgres:[YOUR-PASSWORD]@db.abcdefghijklmnopqrst.supabase.co:6543/postgres
```

Get your project's Transaction pooler connection string from your project dashboard by clickingConnect.


### Dedicated pooler#
For paying customers, we provision a Dedicated Pooler (PgBouncer) that's co-located with your Postgres database. This will require you to connect with IPv6 or, if that's not an option, you can use theIPv4 add-on.

The Dedicated Pooler ensures best performance and latency, while using up more of your project's compute resources. If your network supports IPv6 or you have the IPv4 add-on, we encourage you to use the Dedicated Pooler over the Shared Pooler.

Get your project's Dedicated pooler connection string from your project dashboard by clickingConnect.

PgBouncer always runs in Transaction mode and the current version does not support prepared statement (will be added in a few weeks).


### More about connection pooling#
Connection pooling improves database performance by reusing existing connections between queries. This reduces the overhead of establishing connections and improves scalability.

You can use an application-side pooler or a server-side pooler (Supabase automatically provides one called Supavisor), depending on whether your backend is persistent or serverless.


### Application-side poolers#
Application-side poolers are built into connection libraries and API servers, such as Prisma, SQLAlchemy, and PostgREST. They maintain several active connections with Postgres or a server-side pooler, reducing the overhead of establishing connections between queries. When deploying to static architecture, such as long-standing containers or VMs, application-side poolers are satisfactory on their own.


### Serverside poolers#
Postgres connections are like a WebSocket. Once established, they are preserved until the client (application server) disconnects. A server might only make a single 10 ms query, but needlessly reserve its database connection for seconds or longer.

Serverside-poolers, such as Supabase'sSupavisorin transaction mode, sit between clients and the database and can be thought of as load balancers for Postgres connections.

They maintain hot connections with the database and intelligently share them with clients only when needed, maximizing the amount of queries a single connection can service. They're best used to manage queries from auto-scaling systems, such as edge and serverless functions.


### Connecting with SSL#
You should connect to your database using SSL wherever possible, to prevent snooping and man-in-the-middle attacks.

You can obtain your connection info and Server root certificate from your application's dashboard:




### Resources#
- Connection management
- Connecting with psql
- Importing data into Supabase


### Troubleshooting and Postgres connection string FAQs#
Below are answers to common challenges and queries.


### What is a “connection refused” error?#
A “Connection refused” error typically means your database isn’t reachable. Ensure your Supabase project is running, confirm your database’s connection string, check firewall settings, and validate network permissions.


### What is the “FATAL: Password authentication failed” error?#
This error occurs when your credentials are incorrect. Double-check your username and password from the Supabase dashboard. If the problem persists, reset your database password from the project settings.


### How do you connect using IPv4?#
Supabase’s default direct connection supports IPv6 only. To connect over IPv4, consider using the Supavisor session or transaction modes, or a connection pooler (shared or dedicated), which support both IPv4 and IPv6.


### Where is the Postgres connection string in Supabase?#
Your connection string is located in the Supabase Dashboard. Click theConnectbutton at the top of the page.


### Can you use Supavisor and PgBouncer together?#
You can technically use both, but it’s not recommended unless you’re specifically trying to increase the total number of concurrent client connections. In most cases, it is better to choose either PgBouncer or Supavisor for pooled or transaction-based traffic. Direct connections remain the best choice for long-lived sessions, and, if IPv4 is required for those sessions, Supavisor session mode can be used as an alternative. Running both poolers simultaneously increases the risk of hitting your database’s maximum connection limit on smaller compute tiers.


### How does the default pool size work?#
Supavisor and PgBouncer work independently, but both reference the same pool size setting. For example, If you set the pool size to 30, Supavisor can open up to 30 server side connections to Postgres. These connections are shared between the session mode port (5432) and the transaction mode port (6543). Each mode can use up to 30 connections independently, or split them between both, but the total combined connections across both modes cannot exceed 30. PgBouncer can also open up to 30 connections under the same limit. If both poolers are active and reach their roles/modes limits at the same time, you could have as many as 60 backend connections hitting your database, in addition to any direct connections. You can adjust the pool size inDatabase settingsin the dashboard.


### What is the difference between client connections and backend connections?#
There are two different limits to understand when working with poolers. The first is client connections, which refers to how many clients can connect to a pooler at the same time. This number is capped by yourcompute tier’s “max pooler clients” limit, and it applies independently to Supavisor and PgBouncer. The second is backend connections, which is the number of active connections a pooler opens to Postgres. This number is set by the pool size for that pooler.

```javascript
1Total backend load on Postgres =2 Direct connections +3 Supavisor backend connections (≤ supavisor_pool_size) +4 PgBouncer backend connections (≤ pgbouncer_pool_size)5≤ Postgres max connections for your compute instance
```


### What is the max pooler clients limit?#
The “max pooler clients” limit for your compute tier applies separately to Supavisor and PgBouncer. One pooler reaching its client limit does not affect the other. When a pooler reaches this limit, it stops accepting new client connections until existing ones are closed, but the other pooler remains unaffected. You can check your tier’s connection limits in thecompute and disk limits documentation.


### Where can you see current connection usage?#
You can track connection usage from theObservabilitysection in your project dashboard. There are three key reports:

- Database Connections:shows total active connections by role (this includes direct and pooled connections).
- Dedicated Pooler Client Connections:shows the number of active client connections to PgBouncer.
- Shared Pooler (Supavisor) Client Connections:shows the number of active client connections to Supavisor.

Keep in mind that the Roles page is not real-time, it shows the connection count from the last refresh. If you need up-to-the-second data, set up Grafana or run the query againstpg_stat_activitydirectly in SQL Editor. We have a few helpful queries for checking connections.

```javascript
1-- Count connections by application and user name2select3  count(usename),4  count(application_name),5  application_name,6  usename7from8  pg_stat_ssl9  join pg_stat_activity on pg_stat_ssl.pid = pg_stat_activity.pid10group by usename, application_name;
```

```javascript
1-- View all connections2 SELECT3   pg_stat_activity.pid,4   ssl AS ssl_connection,5   datname AS database,6   usename AS connected_role,7   application_name,8   client_addr,9   query,10   query_start,11   state,12   backend_start13FROM pg_stat_ssl14JOIN pg_stat_activity15 ON pg_stat_ssl.pid = pg_stat_activity.pid;
```


### Why are there active connections when the app is idle?#
Even if your application isn’t making queries, some Supabase services keep persistent connections to your database. For example, Storage, PostgREST, and our health checker all maintain long-lived connections. You usually see a small baseline of active connections from these services.


### Why do connection strings have different ports?#
Different modes use different ports:

- Direct connection:5432(database server)
- PgBouncer:6543(database server)
- Supavisor transaction mode:6543(separate server)
- Supavisor session mode:5432(separate server)

The port helps route the connection to the right pooler/mode.


### Does connection pooling affect latency?#
Because the dedicated pooler is hosted on the same machine as your database, it connects with lower latency than the shared pooler, which is hosted on a separate server. Direct connections have no pooler overhead but require IPv6 unless you have the IPv4 add-on.


### How to choose the right connection method?#
Direct connection:

- Best for: persistent backend services
- Use for migrations, pg_dump, backup and management tools
- Limitation: IPv6 only by default. IPv4 available asAdd-on.

Shared pooler:

- Best for: general-purpose connections (supports IPv4 and IPv6)Supavisor session mode → persistent backend that require IPv4Supavisor transaction mode → serverless functions or short-lived tasks
- Use for application runtime traffic (queries, writes)

- Supavisor session mode → persistent backend that require IPv4
- Supavisor transaction mode → serverless functions or short-lived tasks

Dedicated pooler (paid tier):

- Best for: high-performance apps that need dedicated resources
- Use for application runtime traffic (queries, writes)
- Uses PgBouncer

You can follow the decision flow in the connection method diagram to quickly choose the right option for your environment.


================================================================================


# Connection management
Source: https://supabase.com/docs/guides/database/connection-management

Connection management


### Using your connections resourcefully
Using your connections resourcefully


### Connections#
EveryCompute Add-Onhas a pre-configured direct connection count and Supavisor pool size. This guide discusses ways to observe and manage them resourcefully.


### Configuring Supavisor's pool size#
You can change how many database connections Supavisor can manage by altering the pool size in the "Connection pooling configuration" section of theDatabase Settings:



The general rule is that if you are heavily using the PostgREST database API, you should be conscientious about raising your pool size past 40% of the Database Max Connections. Otherwise, you can commit 80% to the pool. This leaves adequate room for the Authentication server and other utilities.

These numbers are generalizations and depends on other Supabase products that you use and the extent of their usage. The actual values depend on your concurrent peak connection usage. For instance, if you were only using 80 connections in a week period and your database max connections is set to 500, then realistically you could allocate the difference of 420 (minus a reasonable buffer) to service more demand.


### Monitoring connections#

### Capturing historical usage#

### Dashboard monitoring charts#
For Teams and Enterprise plans, Supabase provides Advanced Telemetry charts directly within the Dashboard. TheDatabase client connectionschart displays historical connection data broken down by connection type:

- Postgres: Direct connections from your application
- PostgREST: Connections from the PostgREST API layer
- Reserved: Administrative connections for Supabase services
- Auth: Connections from Supabase Auth service
- Storage: Connections from Supabase Storage service
- Other roles: Miscellaneous database connections

This chart helps you monitor connection pool usage, identify connection leaks, and plan capacity. It also shows a reference line for your compute size's maximum connection limit.

For more details on using these monitoring charts, see theReports guide.


### Grafana Dashboard#
Supabase offers a Grafana Dashboard that records and visualizes over 200 project metrics, including connections. For setup instructions, check themetrics docs.

Its "Client Connections" graph displays connections for both Supavisor and Postgres


### Observing live connections#
pg_stat_activityis a special view that keeps track of processes being run by your database, including live connections. It's particularly useful for determining if idle clients are hogging connection slots.

Query to get all live connections:

```javascript
1SELECT2  pg_stat_activity.pid as connection_id,3  ssl,4  datname as database,5  usename as connected_role,6  application_name,7  client_addr as IP,8  query,9  query_start,10  state,11  backend_start12FROM pg_stat_ssl13JOIN pg_stat_activity14ON pg_stat_ssl.pid = pg_stat_activity.pid;
```

Interpreting the query:

The username can be used to identify the source:


================================================================================


# Customizing Postgres configs
Source: https://supabase.com/docs/guides/database/custom-postgres-config

Customizing Postgres configs

Each Supabase project is a pre-configured Postgres cluster. You can override some configuration settings to suit your needs. This is an advanced topic, and we don't recommend touching these settings unless it is necessary.

Customizing Postgres configurations providesadvancedcontrol over your database, but inappropriate settings can lead to severe performance degradation or project instability.


### Viewing settings#
To list all Postgres settings and their descriptions, run:

```javascript
1select * from pg_settings;
```


### Configurable settings#

### User-context settings#
Thepg_settingstable'scontextcolumn specifies the requirements for changing a setting. By default, those with ausercontext can be changed at theroleordatabaselevel withSQL.

To list all user-context settings, run:

```javascript
1select * from pg_settings where context = 'user';
```

As an example, thestatement_timeoutsetting can be altered:

```javascript
1alter database "postgres" set "statement_timeout" TO '60s';
```

To verify the change, execute:

```javascript
1show "statement_timeout";
```


### Superuser settings#
Some settings can only be modified by a superuser. Supabase pre-enables thesupautilsextension, which allows thepostgresrole to retain certain superuser privileges. It enables modification of the below reserved configurations at therolelevel:

For example, to enablelog_nested_statementsfor thepostgresrole, execute:

```javascript
1alter role "postgres" set "auto_explain.log_nested_statements" to 'on';
```

To view the change:

```javascript
1select2  rolname,3  rolconfig4from pg_roles5where rolname = 'postgres';
```


### CLI configurable settings#
While many Postgres parameters are configurable directly, some configurations can be changed with the Supabase CLI at thesystemlevel.

CLI changes permanently overwrite default settings, soreset allandset to defaultcommands won't revert to the original values.

In order to overwrite the default settings, you must haveOwnerorAdministratorprivileges within your organizations.


### CLI supported parameters#
If a setting you need is not yet configurable,share your use case with us! Let us know what setting you'd like to control, and we'll consider adding support in future updates.

The following parameters are available for overrides:

Parameters marked withRestart: Yescause the CLI to automatically restart your database (and any read replicas) to apply the change. This may cause a brief interruption to active connections. You can use the--no-restartflag to defer the restart.

Use the examples below withsupabase --experimental --project-ref <project-ref> postgres-config update:


### Managing Postgres configuration with the CLI#
To start:

To update Postgres configurations, use thepostgres configcommand:

```javascript
1supabase --experimental \2--project-ref <project-ref> \3postgres-config update --config shared_buffers=250MB
```

By default, the CLI will merge any provided config overrides with any existing ones. The--replace-existing-overridesflag can be used to instead force all existing overrides to be replaced with the ones being provided:

```javascript
1supabase --experimental \2--project-ref <project-ref> \3postgres-config update --config max_parallel_workers=3 \4--replace-existing-overrides
```

To delete specific configuration overrides, use thepostgres-config deletecommand:

```javascript
1supabase --experimental \2--project-ref <project-ref> \3postgres-config delete --config shared_buffers,work_mem
```

By default, CLI v2 (≥ 2.0.0) checks the parameter’s context and requests the correct action (reload or restart):

- If the setting can be reloaded (pg_settings.context = 'sighup'), then the Management API will detect this and apply the change with a configuration reload.
- If the setting requires a restart (pg_settings.context = 'postmaster'), then both the primary and any read replicas will restart to apply the change.

To check whether a parameter can be reloaded without a restart, see thePostgres docs.

You can verify whether changes have been applied with the following checks:

```javascript
1supabase --version;
```

```javascript
1-- Check whether the parameters were updated (and if a restart is pending):2select name, setting, context, pending_restart3from pg_settings4where name in ('max_slot_wal_keep_size', 'shared_buffers', 'max_connections');
```

```javascript
1-- If the timestamp hasn’t changed, no restart occurred2select pg_postmaster_start_time();
```

You can also pass the--no-restartflag to attempt a reload-only apply. If the parameter cannot be reloaded, the change stays pending until the next restart.

Postgres requires several parameters to be synchronized between the Primary cluster andRead Replicas.

By default, Supabase ensures that this propagation is executed correctly. However, if the--no-restartbehavior is used in conjunction with parameters that cannot be reloaded without a restart, the user is responsible for ensuring that both the primaries and the read replicas get restarted in a timely manner to ensure a stable running state. Leaving the configuration updated, but not utilized (via a restart) in such a case can result in read replica failure if the primary, or a read replica, restarts in isolation (e.g. due to an out-of-memory event, or hardware failure).

```javascript
1supabase --experimental \2--project-ref <project-ref> \3postgres-config delete --config shared_buffers --no-restart
```


### Resetting to default config#
To reset a setting to its default value at the database level:

```javascript
1-- reset a single setting at the database level2alter database "postgres" set "<setting_name>" to default;34-- reset all settings at the database level5alter database "postgres" reset all;
```

Forrolelevel configurations, you can run:

```javascript
1alter role "<role_name>" set "<setting_name>" to default;
```


### Considerations#

================================================================================


# Connecting with DBeaver
Source: https://supabase.com/docs/guides/database/dbeaver

Connecting with DBeaver

If you do not have DBeaver, you can download it from itswebsite.


### Create a new database connection
Create a new database connection




### Select PostgreSQL



### Get Your Credentials
On your project dashboard, clickConnect, note your session pooler's:

- host
- username

You will also need your database's password. If you forgot it, you can generate a new one in the settings.

If you're in anIPv6 environmentor have the IPv4 Add-On, you can use the direct connection string instead of Supavisor in Session mode.




### Fill out credentials
In DBeaver's Main menu, add your host, username, and password




### Download certificate
In theDatabase Settings, download your SSL certificate.




### Secure your connection
In DBeaver's SSL tab, add your SSL certificate




### Connect
Test your connection and then click finish. You should now be able to interact with your database with DBeaver




================================================================================


# Debugging performance issues
Source: https://supabase.com/docs/guides/database/debugging-performance

Debugging performance issues


### Debug slow-running queries using the Postgres execution planner.
Debug slow-running queries using the Postgres execution planner.

explain()is a method that provides the PostgresEXPLAINexecution plan of a query. It is a powerful tool for debugging slow queries and understanding how Postgres will execute a given query. This feature is applicable to any query, including those made throughrpc()or write operations.


### Enablingexplain()#
explain()is disabled by default to protect sensitive information about your database structure and operations. We recommend usingexplain()in a non-production environment. Run the following SQL to enableexplain():

```javascript
1-- enable explain2alter role authenticator3set pgrst.db_plan_enabled to 'true';45-- reload the config6notify pgrst, 'reload config';
```


### Usingexplain()#
To get the execution plan of a query, you can chain theexplain()method to a Supabase query:

```javascript
1const { data, error } = await supabase2  .from('instruments')3  .select()4  .explain()
```


### Example data#
To illustrate, consider the following setup of ainstrumentstable:

```javascript
1create table instruments (2  id int8 primary key,3  name text4);56insert into books7  (id, name)8values9  (1, 'violin'),10  (2, 'viola'),11  (3, 'cello');
```


### Expected response#
The response would typically look like this:

```javascript
1Aggregate  (cost=33.34..33.36 rows=1 width=112)2  ->  Limit  (cost=0.00..18.33 rows=1000 width=40)3        ->  Seq Scan on instruments  (cost=0.00..22.00 rows=1200 width=40)
```

By default, the execution plan is returned in TEXT format. However, you can also retrieve it as JSON by specifying theformatparameter.


### Production use with pre-request protection#
If you need to enableexplain()in a production environment, ensure you protect your database by restricting access to theexplain()feature. You can do so by using a pre-request function that filters requests based on the IP address:

```javascript
1create or replace function filter_plan_requests()2returns void as $$3declare4  headers   json := current_setting('request.headers', true)::json;5  client_ip text := coalesce(headers->>'cf-connecting-ip', '');6  accept    text := coalesce(headers->>'accept', '');7  your_ip   text := '123.123.123.123'; -- replace this with your IP8begin9  if accept like 'application/vnd.pgrst.plan%' and client_ip != your_ip then10    raise insufficient_privilege using11      message = 'Not allowed to use application/vnd.pgrst.plan';12  end if;13end; $$ language plpgsql;14alter role authenticator set pgrst.db_pre_request to 'filter_plan_requests';15notify pgrst, 'reload config';
```

Thepgrst.db_pre_requestconfiguration only works with theData API(PostgREST). It does not work with Realtime, Storage, or other Supabase products.

If you're usingdb_pre_requestto call a function (likeset_information()) that sets up context or performs checks on every request, and you need similar behavior for other Supabase products, you must call the function directly in your Row Level Security (RLS) policies instead.

Example:

If you have adb_pre_requestfunction that callsset_information()that returnstrueto set up context or perform checks, and you have an RLS policy like:

```javascript
1create policy "Individuals can view their own todos."2on todos for select3using ( (select auth.uid()) = user_id );
```

To achieve the same behavior with other Supabase products, you need to call the function directly in your RLS policy:

```javascript
1create policy "Individuals can view their own todos."2on todos for select3using ( set_information() AND (select auth.uid()) = user_id );
```

This ensures the function is called when evaluating RLS policies for all products, not just Data API requests.

Performance consideration:

Be aware that calling functions directly in RLS policies can impact database performance, as the function is evaluated for each row when the policy is checked. Consider optimizing your function or using caching strategies if performance becomes an issue.

Replace'123.123.123.123'with your actual IP address.


### Disabling explain#
To disable theexplain()method after use, execute the following SQL commands:

```javascript
1-- disable explain2alter role authenticator3set pgrst.db_plan_enabled to 'false';45-- if you used the above pre-request6alter role authenticator7set pgrst.db_pre_request to '';89-- reload the config10notify pgrst, 'reload config';
```


================================================================================


# Drizzle
Source: https://supabase.com/docs/guides/database/drizzle

Drizzle


### Connecting with Drizzle#
Drizzle ORMis a TypeScript ORM for SQL databases designed with maximum type safety in mind. You can use their ORM to connect to your database.

If you plan on solely using Drizzle instead of the Supabase Data API (PostgREST), you can turn off the latter in theAPI Settings.


### Install
Install Drizzle and related dependencies.

```javascript
1npm i drizzle-orm postgres2npm i -D drizzle-kit
```


### Create your models
Create aschema.tsfile and define your models.

```javascript
1import { pgTable, serial, text, varchar } from "drizzle-orm/pg-core";23export const users = pgTable('users', {4  id: serial('id').primaryKey(),5  fullName: text('full_name'),6  phone: varchar('phone', { length: 256 }),7});
```


### Connect
Connect to your database using the Connection Pooler.

From the projectConnectpanel, copy the URI from the "Shared Pooler" option and save it as theDATABASE_URLenvironment variable. Remember to replace the password placeholder with your actual database password.

In local SUPABASE_DB_URL require to be adapted to work with Docker resolver

```javascript
1import 'dotenv/config'23import { drizzle } from 'drizzle-orm/postgres-js'4import postgres from 'postgres'56let connectionString = process.env.DATABASE_URL7if (host.includes('postgres:postgres@supabase_db_')) {8  const url = URL.parse(host)!9  url.hostname = url.hostname.split('_')[1]10  connectionString = url.href11}1213// Disable prefetch as it is not supported for "Transaction" pool mode14export const client = postgres(connectionString, { prepare: false })15export const db = drizzle(client);
```


================================================================================


# Postgres Extensions Overview
Source: https://supabase.com/docs/guides/database/extensions

Postgres Extensions Overview

Extensions are exactly as they sound - they "extend" the database with functionality which isn't part of the Postgres core.
Supabase has pre-installed some of the most useful open source extensions.


### Enable and disable extensions#
Most extensions are installed under theextensionsschema, which is accessible to public by default. To avoid namespace pollution, we do not recommend creating other entities in theextensionsschema.

If you need to restrict user access to tables managed by extensions, we recommend creating a separate schema for installing that specific extension.

Some extensions can only be created under a specific schema, for example,postgis_tiger_geocoderextension creates a schema namedtiger. Before enabling such extensions, make sure you have not created a conflicting schema with the same name.

In addition to the pre-configured extensions, you can also install your own SQL extensions directly in the database using Supabase's SQL editor. The SQL code for the extensions, including plpgsql extensions, can be added through the SQL editor.


### Upgrade extensions#
If a new version of an extension becomes available on Supabase, you need to initiate a software upgrade in theInfrastructure Settingsto access it. Software upgrades can also be initiated by restarting your server in theGeneral Settings.


### Full list of extensions#
Supabase is pre-configured with over 50 extensions and you can install additional extensions through thedatabase.devpackage manager.

You can install pure SQL extensions directly in the database using the SQL editor or any Postgres client.

If you would like to request an extension, add (or upvote) it in theGitHub Discussion.


================================================================================


# http: RESTful Client
Source: https://supabase.com/docs/guides/database/extensions/http

http: RESTful Client

Thehttpextension allows you to call RESTful endpoints within Postgres.


### Quick demo#

### Overview#
Let's cover some basic concepts:

- REST: stands for REpresentational State Transfer. It's a way to request data from external services.
- RESTful APIs are servers which accept HTTP "calls". The calls are typically:GET− Read only access to a resource.POST− Creates a new resource.DELETE− Removes a resource.PUT− Updates an existing resource or creates a new resource.

- GET− Read only access to a resource.
- POST− Creates a new resource.
- DELETE− Removes a resource.
- PUT− Updates an existing resource or creates a new resource.

You can use thehttpextension to make these network requests from Postgres.


### Usage#

### Enable the extension#

### Available functions#
While the main usage ishttp('http_request'), there are 5 wrapper functions for specific functionality:

- http_get()
- http_post()
- http_put()
- http_delete()
- http_head()


### Returned values#
A successful call to a web URL from thehttpextension returns a record with the following fields:

- status: integer
- content_type: character varying
- headers: http_header[]
- content: character varying. Typically you would want to cast this tojsonbusing the formatcontent::jsonb


### Examples#

### SimpleGETexample#
```javascript
1select2  "status", "content"::jsonb3from4  extensions.http_get('https://jsonplaceholder.typicode.com/todos/1');
```


### SimplePOSTexample#
```javascript
1select2  "status", "content"::jsonb3from4  extensions.http_post(5    'https://jsonplaceholder.typicode.com/posts',6    '{ "title": "foo", "body": "bar", "userId": 1 }',7    'application/json'8  );
```


### Resources#
- OfficialhttpGitHub Repository


================================================================================


# HypoPG: Hypothetical indexes
Source: https://supabase.com/docs/guides/database/extensions/hypopg

HypoPG: Hypothetical indexes

HypoPGis Postgres extension for creating hypothetical/virtual indexes. HypoPG allows users to rapidly create hypothetical/virtual indexes that have no resource cost (CPU, disk, memory) that are visible to the Postgres query planner.

The motivation for HypoPG is to allow users to quickly search for an index to improve a slow query without consuming server resources or waiting for them to build.


### Enable the extension#

### Speeding up a query#
Given the following table and a simple query to select from the table byid:

```javascript
1create table account (2  id int,3  address text4);56insert into account(id, address)7select8  id,9  id || ' main street'10from11  generate_series(1, 10000) id;
```

We can generate an explain plan for a description of how the Postgres query planner
intends to execute the query.

```javascript
1explain select * from account where id=1;23                      QUERY PLAN4-------------------------------------------------------5 Seq Scan on account  (cost=0.00..180.00 rows=1 width=13)6   Filter: (id = 1)7(2 rows)
```

Using HypoPG, we can create a hypothetical index on theaccount(id)column to check if it would be useful to the query planner and then re-run the explain plan.

Note that the virtual indexes created by HypoPG are only visible in the Postgres connection that they were created in. Supabase connects to Postgres through a connection pooler so thehypopg_create_indexstatement and theexplainstatement should be executed in a single query.

```javascript
1select * from hypopg_create_index('create index on account(id)');23explain select * from account where id=1;45                                     QUERY PLAN6------------------------------------------------------------------------------------7 Index Scan using <13504>btree_account_id on hypo  (cost=0.29..8.30 rows=1 width=13)8   Index Cond: (id = 1)9(2 rows)
```

The query plan has changed from aSeq Scanto anIndex Scanusing the newly created virtual index, so we may choose to create a real version of the index to improve performance on the target query:

```javascript
1create index on account(id);
```


### Functions#
- hypo_create_index(text): A function to create a hypothetical index.
- hypopg_list_indexes: A View that lists all hypothetical indexes that have been created.
- hypopg(): A function that lists all hypothetical indexes that have been created with the same format aspg_index.
- hypopg_get_index_def(oid): A function to display thecreate indexstatement that would create the index.
- hypopg_get_relation_size(oid): A function to estimate how large a hypothetical index would be.
- hypopg_drop_index(oid): A function to remove a given hypothetical index byoid.
- hypopg_reset(): A function to remove all hypothetical indexes.


### Resources#
- OfficialHypoPG documentation


================================================================================


# index_advisor: query optimization
Source: https://supabase.com/docs/guides/database/extensions/index_advisor

index_advisor: query optimization

Index advisoris a Postgres extension for recommending indexes to improve query performance.

Features:

- Supports generic parameters e.g.$1,$2
- Supports materialized views
- Identifies tables/columns obfuscated by views
- Skips duplicate indexes

index_advisoris accessible directly through Supabase Studio by navigating to theQuery Performance Reportand selecting a query and then the "indexes" tab.



Alternatively, you can use index_advisor directly via SQL.

For example:

```javascript
1select2    *3from4  index_advisor('select book.id from book where title = $1');56 startup_cost_before | startup_cost_after | total_cost_before | total_cost_after |                  index_statements                   | errors7---------------------+--------------------+-------------------+------------------+-----------------------------------------------------+--------8 0.00                | 1.17               | 25.88             | 6.40             | {"CREATE INDEX ON public.book USING btree (title)"},| {}9(1 row)
```


### Installation#
To get started, enable index_advisor by running

```javascript
1create extension index_advisor;
```


### API#
Index advisor exposes a single functionindex_advisor(query text)that accepts a query and searches for a set of SQL DDLcreate indexstatements that improve the query's execution time.

The function's signature is:

```javascript
1index_advisor(query text)2returns3    table  (4        startup_cost_before jsonb,5        startup_cost_after jsonb,6        total_cost_before jsonb,7        total_cost_after jsonb,8        index_statements text[],9        errors text[]10    )
```


### Usage#
As a minimal example, theindex_advisorfunction can be given a single table query with a filter on an unindexed column.

```javascript
1create extension if not exists index_advisor cascade;23create table book(4  id int primary key,5  title text not null6);78select9  *10from11  index_advisor('select book.id from book where title = $1');1213 startup_cost_before | startup_cost_after | total_cost_before | total_cost_after |                  index_statements                   | errors14---------------------+--------------------+-------------------+------------------+-----------------------------------------------------+--------15 0.00                | 1.17               | 25.88             | 6.40             | {"CREATE INDEX ON public.book USING btree (title)"},| {}16(1 row)
```

and will return a row recommending an index on the unindexed column.

More complex queries may generate additional suggested indexes:

```javascript
1create extension if not exists index_advisor cascade;23create table author(4    id serial primary key,5    name text not null6);78create table publisher(9    id serial primary key,10    name text not null,11    corporate_address text12);1314create table book(15    id serial primary key,16    author_id int not null references author(id),17    publisher_id int not null references publisher(id),18    title text19);2021create table review(22    id serial primary key,23    book_id int references book(id),24    body text not null25);2627select28    *29from30    index_advisor('31        select32            book.id,33            book.title,34            publisher.name as publisher_name,35            author.name as author_name,36            review.body review_body37        from38            book39            join publisher40                on book.publisher_id = publisher.id41            join author42                on book.author_id = author.id43            join review44                on book.id = review.book_id45        where46            author.id = $147            and publisher.id = $248    ');4950 startup_cost_before | startup_cost_after | total_cost_before | total_cost_after |                  index_statements                         | errors51---------------------+--------------------+-------------------+------------------+-----------------------------------------------------------+--------52 27.26               | 12.77              | 68.48             | 42.37            | {"CREATE INDEX ON public.book USING btree (author_id)",   | {}53                                                                                    "CREATE INDEX ON public.book USING btree (publisher_id)",54                                                                                    "CREATE INDEX ON public.review USING btree (book_id)"}55(3 rows)
```


### Limitations#
- index_advisor will only recommend single column, B-tree indexes. More complex indexes will be supported in future releases.
- when a generic argument's type is not discernible from context, an error is returned in theerrorsfield. To resolve those errors, add explicit type casting to the argument. e.g.$1::int.


### Resources#
- index_advisorrepo


================================================================================


# pg_cron: Schedule Recurring Jobs with Cron Syntax in Postgres
Source: https://supabase.com/docs/guides/database/extensions/pg_cron

pg_cron: Schedule Recurring Jobs with Cron Syntax in Postgres

See theSupabase Cron docs.


================================================================================


# pg_graphql: GraphQL for PostgreSQL
Source: https://supabase.com/docs/guides/database/extensions/pg_graphql

pg_graphql: GraphQL for PostgreSQL

pg_graphqlis Postgres extension for interacting with the database usingGraphQLinstead of SQL.

The extension reflects a GraphQL schema from the existing SQL schema and exposes it through a SQL function,graphql.resolve(...). This enables any programming language that can connect to Postgres to query the database via GraphQL with no additional servers, processes, or libraries.

Thepg_graphqlresolve method is designed to interop withPostgREST, the tool that underpins the Supabase API, such that thegraphql.resolvefunction can be called via RPC to safely and performantly expose the GraphQL API over HTTP/S.

For more information about how the SQL schema is reflected into a GraphQL schema, see thepg_graphql API docs.


### Enable the extension#

### Usage#
Given a table

```javascript
1create table "Blog"(2  id serial primary key,3  name text not null,4  description text5);67insert into "Blog"(name)8values ('My Blog');
```

The reflected GraphQL schema can be queried immediately as

```javascript
1select2  graphql.resolve($$3    {4      blogCollection(first: 1) {5        edges {6          node {7            id,8            name9          }10        }11      }12    }13  $$);
```

returning the JSON

```javascript
1{2  "data": {3    "blogCollection": {4      "edges": [5        {6          "node": {7            "id": 18            "name": "My Blog"9          }10        }11      ]12    }13  }14}
```

Note thatpg_graphqlfully supports schema introspection so you can connect any GraphQL IDE or schema inspection tool to see the full set of fields and arguments available in the API.


### API#
- graphql.resolve: A SQL function for executing GraphQL queries.


### Resources#
- Officialpg_graphqldocumentation


================================================================================


# pg_hashids: Short UIDs
Source: https://supabase.com/docs/guides/database/extensions/pg_hashids

pg_hashids: Short UIDs

pg_hashidsprovides a secure way to generate short, unique, non-sequential ids from numbers. The hashes are intended to be small, easy-to-remember identifiers that can be used to obfuscate data (optionally) with a password, alphabet, and salt. For example, you may wish to hide data like user IDs, order numbers, or tracking codes in favor ofpg_hashid's unique identifiers.


### Enable the extension#

### Usage#
Suppose we have a table that stores order information, and we want to give customers a unique identifier without exposing the sequentialidcolumn. To do this, we can usepg_hashid'sid_encodefunction.

```javascript
1create table orders (2  id serial primary key,3  description text,4  price_cents bigint5);67insert into orders (description, price_cents)8values ('a book', 9095);910select11  id,12  id_encode(id) as short_id,13  description,14  price_cents15from16  orders;1718  id | short_id | description | price_cents19----+----------+-------------+-------------20  1 | jR       | a book      |        909521(1 row)
```

To reverse theshort_idback into anid, there is an equivalent function namedid_decode.


### Resources#
- Officialpg_hashids documentation


================================================================================


# pg_jsonschema: JSON Schema Validation
Source: https://supabase.com/docs/guides/database/extensions/pg_jsonschema

pg_jsonschema: JSON Schema Validation

JSON Schemais a language for annotating and validating JSON documents.pg_jsonschemais a Postgres extension that adds the ability to validate PostgreSQL's built-injsonandjsonbdata types against JSON Schema documents.


### Enable the extension#

### Functions#
- json_matches_schema(schema json, instance json): Checks if ajsoninstanceconforms to a JSON Schemaschema.
- jsonb_matches_schema(schema json, instance jsonb): Checks if ajsonbinstanceconforms to a JSON Schemaschema.


### Usage#
Sincepg_jsonschemaexposes its utilities as functions, we can execute them with a select statement:

```javascript
1select2  extensions.json_matches_schema(3    schema := '{"type": "object"}',4    instance := '{}'5  );
```

pg_jsonschemais generally used in tandem with acheck constraintas a way to constrain the contents of a json/b column to match a JSON Schema.

```javascript
1create table customer(2    id serial primary key,3    ...4    metadata json,56    check (7        json_matches_schema(8            '{9                "type": "object",10                "properties": {11                    "tags": {12                        "type": "array",13                        "items": {14                            "type": "string",15                            "maxLength": 1616                        }17                    }18                }19            }',20            metadata21        )22    )23);2425-- Example: Valid Payload26insert into customer(metadata)27values ('{"tags": ["vip", "darkmode-ui"]}');28-- Result:29--   INSERT 0 13031-- Example: Invalid Payload32insert into customer(metadata)33values ('{"tags": [1, 3]}');34-- Result:35--   ERROR:  new row for relation "customer" violates check constraint "customer_metadata_check"36--   DETAIL:  Failing row contains (2, {"tags": [1, 3]}).
```


### Resources#
- Officialpg_jsonschemadocumentation


================================================================================


# pg_net: Async Networking
Source: https://supabase.com/docs/guides/database/extensions/pg_net

pg_net: Async Networking

The pg_net API is in beta. Functions signatures may change.

pg_netenables Postgres to make asynchronous HTTP/HTTPS requests in SQL. It differs from thehttpextension in that it is asynchronous by default. This makes it useful in blocking functions (like triggers).

It eliminates the need for servers to continuously poll for database changes and instead allows the database to proactively notify external resources about significant events.


### Enable the extension#

### http_get#
Creates an HTTP GET request returning the request's ID. HTTP requests are not started until the transaction is committed.


### Signature#
This is a PostgresSECURITY DEFINERfunction.

```javascript
1net.http_get(2    -- url for the request3    url text,4    -- key/value pairs to be url encoded and appended to the `url`5    params jsonb default '{}'::jsonb,6    -- key/values to be included in request headers7    headers jsonb default '{}'::jsonb,8    -- the maximum number of milliseconds the request may take before being canceled9    timeout_milliseconds int default 200010)11    -- request_id reference12    returns bigint1314    strict15    volatile16    parallel safe17    language plpgsql
```


### Usage#
```javascript
1select2    net.http_get('https://news.ycombinator.com')3    as request_id;4request_id5----------6         17(1 row)
```


### http_post#
Creates an HTTP POST request with a JSON body, returning the request's ID. HTTP requests are not started until the transaction is committed.

The body's character set encoding matches the database'sserver_encodingsetting.


### Signature#
This is a PostgresSECURITY DEFINERfunction

```javascript
1net.http_post(2    -- url for the request3    url text,4    -- body of the POST request5    body jsonb default '{}'::jsonb,6    -- key/value pairs to be url encoded and appended to the `url`7    params jsonb default '{}'::jsonb,8    -- key/values to be included in request headers9    headers jsonb default '{"Content-Type": "application/json"}'::jsonb,10    -- the maximum number of milliseconds the request may take before being canceled11    timeout_milliseconds int default 200012)13    -- request_id reference14    returns bigint1516    volatile17    parallel safe18    language plpgsql
```


### Usage#
```javascript
1select2    net.http_post(3        url:='https://httpbin.org/post',4        body:='{"hello": "world"}'::jsonb5    ) as request_id;6request_id7----------8         19(1 row)
```


### http_delete#
Creates an HTTP DELETE request, returning the request's ID. HTTP requests are not started until the transaction is committed.


### Signature#
This is a PostgresSECURITY DEFINERfunction

```javascript
1net.http_delete(2    -- url for the request3    url text,4    -- key/value pairs to be url encoded and appended to the `url`5    params jsonb default '{}'::jsonb,6    -- key/values to be included in request headers7    headers jsonb default '{}'::jsonb,8    -- the maximum number of milliseconds the request may take before being canceled9    timeout_milliseconds int default 200010)11    -- request_id reference12    returns bigint1314    strict15    volatile16    parallel safe17    language plpgsql18    security definer
```


### Usage#
```javascript
1select2    net.http_delete(3        'https://dummy.restapiexample.com/api/v1/delete/2'4    ) as request_id;5----------6         17(1 row)
```


### Analyzing responses#
Waiting requests are stored in thenet.http_request_queuetable. Upon execution, they are deleted.

```javascript
1CREATE UNLOGGED TABLE2    net.http_request_queue (3        id bigint NOT NULL DEFAULT nextval('net.http_request_queue_id_seq'::regclass),4        method text NOT NULL,5        url text NOT NULL,6        headers jsonb NOT NULL,7        body bytea NULL,8        timeout_milliseconds integer NOT NULL9    )
```

Once a response is returned, by default, it is stored for 6 hours in thenet._http_responsetable.

```javascript
1CREATE UNLOGGED TABLE2    net._http_response (3        id bigint NULL,4        status_code integer NULL,5        content_type text NULL,6        headers jsonb NULL,7        content text NULL,8        timed_out boolean NULL,9        error_msg text NULL,10        created timestamp with time zone NOT NULL DEFAULT now()11    )
```

The responses can be observed with the following query:

```javascript
1select * from net._http_response;
```

The data can also be observed in thenetschema with theSupabase Dashboard's SQL Editor


### Debugging requests#

### Inspecting request data#
ThePostman Echo APIreturns a response with the same body and content
as the request. It can be used to inspect the data being sent.

Sending a post request to the echo API

```javascript
1select2    net.http_post(3        url := 'https://postman-echo.com/post',4        body := '{"key1": "value", "key2": 5}'::jsonb5    ) as request_id;
```

Inspecting the echo API response content to ensure it contains the right body

```javascript
1select2    "content"3from net._http_response4where id = <request_id>5-- returns information about the request6-- including the body sent: {"key": "value", "key": 5}
```

Alternatively, by wrapping a request in adatabase function, sent row data can be logged or returned for inspection and debugging.

```javascript
1create or replace function debugging_example (row_id int)2returns jsonb as $$3declare4    -- Store payload data5    row_data_var jsonb;6begin7    -- Retrieve row data and convert to JSON8    select to_jsonb("<example_table>".*) into row_data_var9    from "<example_table>"10    where "<example_table>".id = row_id;1112    -- Initiate HTTP POST request to URL13    perform14        net.http_post(15            url := 'https://postman-echo.com/post',16            -- Use row data as payload17            body := row_data_var18        ) as request_id;1920    -- Optionally Log row data or other data for inspection in Supabase Dashboard's Postgres Logs21    raise log 'Logging an entire row as JSON (%)', row_data_var;2223    -- return row data to inspect24    return row_data_var;2526-- Handle exceptions here if needed27exception28    when others then29        raise exception 'An error occurred: %', SQLERRM;30end;31$$ language plpgsql;3233-- calling function34select debugging_example(<row_id>);
```


### Inspecting failed requests#
Finds all failed requests

```javascript
1select2  *3from net._http_response4where "status_code" >= 400 or "error_msg" is not null5order by "created" desc;
```


### Configuration#
Supabase supports reconfiguring pg*net starting from v0.12.0+. For the latest release, initiate a Postgres upgrade in theInfrastructure Settings.

The extension is configured to reliably execute up to 200 requests per second. The response messages are stored for only 6 hours to prevent needless buildup. The default behavior can be modified by rewriting config variables.


### Get current settings#
```javascript
1select2  "name",3  "setting"4from pg_settings5where "name" like 'pg_net%';
```


### Alter settings#
Change variables:

```javascript
1alter role "postgres" set pg_net.ttl to '24 hours';2alter role "postgres" set pg_net.batch_size to 500;
```

Then reload the settings and restart thepg_netbackground worker with:

```javascript
1select net.worker_restart();
```


### Examples#

### Invoke a Supabase Edge Function#
Make a POST request to a Supabase Edge Function with auth header and JSON body payload:

```javascript
1select2    net.http_post(3        url:='https://project-ref.supabase.co/functions/v1/function-name',4        headers:='{"Content-Type": "application/json", "Authorization": "Bearer <YOUR_ANON_KEY>"}'::jsonb,5        body:='{"name": "pg_net"}'::jsonb6    ) as request_id;
```


### Call an endpoint every minute withpg_cron#
The pg_cron extension enables Postgres to become its own cron server. With it you can schedule regular calls with up to a minute precision to endpoints.

```javascript
1select cron.schedule(2	'cron-job-name',3	'* * * * *', -- Executes every minute (cron syntax)4	$$5	    -- SQL query6	    select "net"."http_post"(7            -- URL of Edge function8            url:='https://project-ref.supabase.co/functions/v1/function-name',9            headers:='{"Authorization": "Bearer <YOUR_ANON_KEY>"}'::jsonb,10            body:='{"name": "pg_net"}'::jsonb11	    ) as "request_id";12	$$13);
```


### Execute pg_net in a trigger#
Make a call to an external endpoint when a trigger event occurs.

```javascript
1-- function called by trigger2create or replace function <function_name>()3    returns trigger4    language plpgSQL5as $$6begin7    -- calls pg_net function net.http_post8    -- sends request to postman API9    perform "net"."http_post"(10      'https://postman-echo.com/post'::text,11      jsonb_build_object(12        'old_row', to_jsonb(old.*),13        'new_row', to_jsonb(new.*)14      ),15      headers:='{"Content-Type": "application/json"}'::jsonb16    ) as request_id;17    return new;18END $$;1920-- trigger for table update21create trigger <trigger_name>22    after update on <table_name>23    for each row24    execute function <function_name>();
```


### Send multiple table rows in one request#
```javascript
1with "selected_table_rows" as (2    select3        -- Converts all the rows into a JSONB array4        jsonb_agg(to_jsonb(<table_name>.*)) as JSON_payload5    from <table_name>6    -- good practice to LIMIT the max amount of rows7)8select9    net.http_post(10        url := 'https://postman-echo.com/post'::text,11        body := JSON_payload12    ) AS request_id13FROM "selected_table_rows";
```

More examples can be seen on theExtension's GitHub page


### Limitations#
- To improve speed and performance, the requests and responses are stored inunlogged tables, which are not preserved during a crash or unclean shutdown.
- By default, response data is saved for only 6 hours
- Can only make POST requests with JSON data. No other data formats are supported
- Intended to handle at most 200 requests per second. Increasing the rate can introduce instability
- Does not have support for PATCH/PUT requests
- Can only work with one database at a time. It defaults to thepostgresdatabase.


### Resources#
- Source code:github.com/supabase/pg_net
- Official Docs:github.com/supabase/pg_net


================================================================================


# pg_plan_filter: Restrict Total Cost
Source: https://supabase.com/docs/guides/database/extensions/pg_plan_filter

pg_plan_filter: Restrict Total Cost

pg_plan_filteris Postgres extension to block execution of statements where query planner's estimate of the total cost exceeds a threshold. This is intended to give database administrators a way to restrict the contribution an individual query has on database load.


### Enable the extension#
The extension is already enabled by default viashared_preload_librariessetting.

You can follow the instructions below.


### API#
plan_filter.statement_cost_limit: restricts the maximum total cost for executed statementsplan_filter.limit_select_only: restricts toselectstatements

Note thatlimit_select_only = trueis not the same as read-only becauseselectstatements may modify data, for example, through a function call.


### Example#
To demonstrate total cost filtering, we'll compare howplan_filter.statement_cost_limittreats queries that are under and over its cost limit. First, we set up a table with some data:

```javascript
1create table book(2  id int primary key3);4-- CREATE TABLE56insert into book(id) select * from generate_series(1, 10000);7-- INSERT 0 10000
```

Next, we can review the explain plans for a single record select, and a whole table select.

```javascript
1explain select * from book where id =1;2                                QUERY PLAN3---------------------------------------------------------------------------4 Index Only Scan using book_pkey on book  (cost=0.28..2.49 rows=1 width=4)5   Index Cond: (id = 1)6(2 rows)78explain select * from book;9                       QUERY PLAN10---------------------------------------------------------11 Seq Scan on book  (cost=0.00..135.00 rows=10000 width=4)12(1 row)
```

Now we can choose astatement_cost_filtervalue between the total cost for the single select (2.49) and the whole table select (135.0) so one statement will succeed and one will fail.

```javascript
1set plan_filter.statement_cost_limit = 50; -- between 2.49 and 135.023select * from book where id = 1;4 id5----6  17(1 row)8-- SUCCESS
```

```javascript
1select * from book;23ERROR:  plan cost limit exceeded4HINT:  The plan for your query shows that it would probably have an excessive run time. This may be due to a logic error in the SQL, or it maybe just a very costly query. Rewrite your query or increase the configuration parameter "plan_filter.statement_cost_limit".5-- FAILURE
```


### Resources#
- Officialpg_plan_filterdocumentation


================================================================================


# pg_repack: Physical storage optimization and maintenance
Source: https://supabase.com/docs/guides/database/extensions/pg_repack

pg_repack: Physical storage optimization and maintenance

pg_repackis a Postgres extension to remove bloat from tables and indexes, and optionally restore the physical order of clustered indexes. Unlike CLUSTER and VACUUM FULL, pg_repack runs "online" and does not hold a exclusive locks on the processed tables that could prevent ongoing database operations. pg_repack's efficiency is comparable to using CLUSTER directly.

pg_repack provides the following methods to optimize physical storage:

- Online CLUSTER: ordering table data by cluster index in a non-blocking way
- Ordering table data by specified columns
- Online VACUUM FULL: packing rows only in a non-blocking way
- Rebuild or relocate only the indexes of a table

pg_repack has 2 components, the database extension and a client-side CLI to control it.


### Requirements#
- A target table must have a PRIMARY KEY, or a UNIQUE total index on a NOT NULL column.
- Performing a full-table repack requires free disk space about twice as large as the target table and its indexes.

pg_repack requires the Postgres superuser role by default. That role is not available to users on the Supabase platform. To avoid that requirement, use the-kor--no-superuser-checkflags on everypg_repackCLI command.

The first version of pg_repack with full support for non-superuser repacking is 1.5.2. You can check the version installed on your Supabase instance using

```javascript
1select default_version2from pg_available_extensions3where name = 'pg_repack';
```

If pg_repack is not present, or the version is < 1.5.2,upgrade to the latest versionof Supabase to gain access.


### Usage#

### Enable the extension#
Get started with pg_repack by enabling the extension in the Supabase Dashboard.


### Install the CLI#
Select an option from the pg_repack docs toinstall the client CLI.


### Syntax#
All pg_repack commands should include the-kflag to skip the client-side superuser check.

```javascript
1pg_repack -k [OPTION]... [DBNAME]
```


### Example#
Perform an onlineVACUUM FULLon the tablespublic.fooandpublic.barin the databasepostgres:

```javascript
1pg_repack -k -h db.<PROJECT_REF>.supabase.co -p 5432 -U postgres -d postgres --no-order --table public.foo --table public.bar
```

See theofficial pg_repack documentationfor the full list of options.


### Limitations#
- pg_repack cannot reorganize temporary tables.
- pg_repack cannot cluster tables by GiST indexes.
- You cannot perform DDL commands of the target tables except VACUUM or ANALYZE while pg_repack is working.
pg_repack holds an ACCESS SHARE lock on the target table to enforce this restriction.


### Resources#
- Official pg_repack documentation


================================================================================


# pg_stat_statements: Query Performance Monitoring
Source: https://supabase.com/docs/guides/database/extensions/pg_stat_statements

pg_stat_statements: Query Performance Monitoring

pg_stat_statementsis a database extension that exposes a view, of the same name, to track statistics about SQL statements executed on the database. The following table shows some of the available statistics and metadata:

A full list of statistics is available in thepg_stat_statements docs.

For more information on query optimization, check out thequery performance guide.


### Enable the extension#

### Inspecting activity#
A common use forpg_stat_statementsis to track down expensive or slow queries. Thepg_stat_statementsview contains a row for each executed query with statistics inlined. For example, you can leverage the statistics to identify frequently executed and slow queries against a given table.

```javascript
1select2	calls,3	mean_exec_time,4	max_exec_time,5	total_exec_time,6	stddev_exec_time,7	query8from9	pg_stat_statements10where11    calls > 50                   -- at least 50 calls12    and mean_exec_time > 2.0     -- averaging at least 2ms/call13    and total_exec_time > 60000  -- at least one minute total server time spent14    and query ilike '%user_in_organization%' -- filter to queries that touch the user_in_organization table15order by16	calls desc
```

From the results, we can make an informed decision about which queries to optimize or index.


### Resources#
- Officialpg_stat_statements documentation


================================================================================


# PGAudit: Postgres Auditing
Source: https://supabase.com/docs/guides/database/extensions/pgaudit

PGAudit: Postgres Auditing

PGAuditextends Postgres's built-in logging abilities. It can be used to selectively track activities within your database.

This helps you with:

- Compliance: Meeting audit requirements for regulations
- Security: Detecting suspicious database activity
- Troubleshooting: Identifying and fixing database issues


### Enable the extension#

### Configure the extension#
PGAudit can be configured with different levels of precision.

PGAudit logging precision:

- Session:Logs activity within a connection, such as apsqlconnection.
- User:Logs activity by a particular database user (for example,anonorpostgres).
- Global:Logs activity across the entire database.
- Object:Logs events related to specific database objects (for example, the auth.users table).

Although Session, User, and Global modes differ in their precision, they're all considered variants ofSession Modeand are configured with the same input categories.


### Session mode categories#
These modes can monitor predefined categories of database operations:

Below is a limited example of how to assign PGAudit to monitor specific categories.

```javascript
1-- log all CREATE, ALTER, and DROP events2... pgaudit.log = 'ddl';34-- log all CREATE, ALTER, DROP, and SELECT events5... pgaudit.log = 'read, ddl';67-- log nothing8... pgaudit.log = 'none';
```


### Session logging#
When you are connecting in a session environment, such as apsqlconnection, you can configure PGAudit to record events initiated within the session.

TheDashboardis a transactional environment and won't sustain a session.

Inside a session, by default, PGAudit will log nothing:

```javascript
1-- returns 'none'2show pgaudit.log;
```

In the session, you cansetthepgaudit.logvariable to record events:

```javascript
1-- log CREATE, ALTER, and DROP events2set pgaudit.log = 'ddl';34-- log all CREATE, ALTER, DROP, and SELECT events5set pgaudit.log = 'read, ddl';67-- log nothing8set pgaudit.log = 'none';
```


### User logging#
There are some cases where you may want to monitor a database user's actions. For instance, let's say you connected your database toZapierand created a custom role for it to use:

```javascript
1create user "zapier" with password '<new password>';
```

You may want to log all actions initiated byzapier, which can be done with the following command:

```javascript
1alter role "zapier" set pgaudit.log to 'all';
```

To remove the settings, execute the following code:

```javascript
1-- disables role's log2alter role "zapier" set pgaudit.log to 'none';34-- check to make sure the changes are finalized:5select6  rolname,7  rolconfig8from pg_roles9where rolname = 'zapier';10-- should return a rolconfig path with "pgaudit.log=none" present
```


### Global logging#
Use global logging cautiously. It can generate many logs and make it difficult to find important events. Consider limiting the scope of what is logged by using session, user, or object logging where possible.

The below SQL configures PGAudit to record all events associated with thepostgresrole. Since it has extensive privileges, this effectively monitors all database activity.

```javascript
1alter role "postgres" set pgaudit.log to 'all';
```

To check if thepostgresrole is auditing, execute the following command:

```javascript
1select2  rolname,3  rolconfig4from pg_roles5where rolname = 'postgres';6-- should return a rolconfig path with "pgaudit.log=all" present
```

To remove the settings, execute the following code:

```javascript
1alter role "postgres" set pgaudit.log to 'none';
```


### Object logging#
To fine-tune what object events PGAudit will record, you must create a custom database role with limited permissions:

```javascript
1create role "some_audit_role" noinherit;
```

No other Postgres user can assume or login via this role. It solely exists to securely define what PGAudit will record.

Once the role is created, you can direct PGAudit to log by assigning it to thepgaudit.rolevariable:

```javascript
1alter role "postgres" set pgaudit.role to 'some_audit_role';
```

You can then assign the role to monitor only approved object events, such asselectstatements that include a specific table:

```javascript
1grant select on random_table to "some_audit_role";
```

With this privilege granted, PGAudit will record all select statements that reference therandom_table, regardless ofwhoorwhatactually initiated the event. All assignable privileges can be viewed in thePostgres documentation.

If you would no longer like to use object logging, you will need to unassign thepgaudit.rolevariable:

```javascript
1-- change pgaudit.role to no longer reference some_audit_role2alter role "postgres" set pgaudit.role to '';34-- view if pgaudit.role changed with the following command:5select6  rolname,7  rolconfig8from pg_roles9where rolname = 'postgres';10-- should return a rolconfig path with "pgaudit.role="
```


### Interpreting Audit Logs#
PGAudit was designed for storing logs as CSV files with the following headers:

Referenced from thePGAudit official docs

A log made from the following create statement:

```javascript
1create table account (2  id int primary key,3  name text,4  description text5);
```

Generates the following log in theDashboard's Postgres Logs:

```javascript
1AUDIT: SESSION,1,1,DDL,CREATE TABLE,TABLE,public.account,create table account(2  id int,3  name text,4  description text5); <not logged>
```


### Finding and filtering audit logs#
Logs generated by PGAudit can be found inPostgres Logs. To find a specific log, you can use the log explorer. Below is a basic example to extract logs referencingCREATE TABLEevents

```javascript
1select2  cast(t.timestamp as datetime) as timestamp,3  event_message4from5  postgres_logs as t6  cross join unnest(metadata) as m7  cross join unnest(m.parsed) as p8where event_message like 'AUDIT%CREATE TABLE%'9order by timestamp desc10limit 100;
```


### Practical examples#

### Monitoring API events#
API requests are already recorded in theAPI Edge Networklogs.

To monitor all writes initiated by the PostgREST API roles:

```javascript
1alter role "authenticator" set pgaudit.log to 'write';23-- the above is the practical equivalent to:4-- alter role "anon" set pgaudit.log TO 'write';5-- alter role "authenticated" set pgaudit.log TO 'write';6-- alter role "service_role" set pgaudit.log TO 'write';
```


### Monitoring theauth.userstable#
In the worst case scenario, where a privileged roles' password is exposed, you can use PGAudit to monitor if theauth.userstable was targeted. It should be stated that API requests are already monitored in theAPI Edge Networkand this is more about providing greater clarity about what is happening at the database level.

Loggingauth.usershould be done in Object Mode and requires a custom role:

```javascript
1-- create logging role2create role "auth_auditor" noinherit;34-- give role permission to observe relevant table events5grant select on auth.users to "auth_auditor";6grant delete on auth.users to "auth_auditor";78-- assign auth_auditor to pgaudit.role9alter role "postgres" set pgaudit.role to 'auth_auditor';
```

With the above code, any query involving reading or deleting from the auth.users table will be logged.


### Best practices#

### Disabling excess logging#
PGAudit, if not configured mindfully, can log all database events, including background tasks. This can generate an undesirably large amount of logs in a few hours.

The first step to solve this problem is to identify which database users PGAudit is observing:

```javascript
1-- find all users monitored by pgaudit2select3  rolname,4  rolconfig5from pg_roles6where7  exists (8    select9      110    from UNNEST(rolconfig) as c11    where c like '%pgaudit.role%' or c like '%pgaudit.log%'12  );
```

To prevent PGAudit from monitoring the problematic roles, you'll want to change theirpgaudit.logvalues tononeandpgaudit.rolevalues toempty quotes ''

```javascript
1-- Use to disable object level logging2  alter role "<role name>" set pgaudit.role to '';34  -- Use to disable global and user level logging5  alter role "<role name>" set pgaudit.log to 'none';
```


### FAQ#

### Using PGAudit to debug database functions#
Technically yes, but it is not the best approach. It is better to check out ourfunction debugging guideinstead.


### Downloading database logs#
In theLogs Dashboardyou can download logs as CSVs.


### Logging observed table rows#
By default, PGAudit records queries, but not the returned rows. You can modify this behavior with thepgaudit.log_rowsvariable:

```javascript
1--enable2alter role "postgres" set pgaudit.log_rows to 'on';34-- disable5alter role "postgres" set pgaudit.log_rows to 'off';
```

You should not do this unless you areabsolutelycertain it is necessary for your use case. It can expose sensitive values to your logs that ideally should not be preserved. Furthermore, if done in excess, it can noticeably reduce database performance.


### Logging function parameters#
We don't currently support configuringpgaudit.log_parameterbecause it may log secrets in encrypted columns if you are usingpgsodiumorVault.

You can upvote thisfeature requestwith your use-case if you'd like this restriction lifted.


### Does PGAudit support system wide configurations?#
PGAudit allows settings to be applied to 3 different database scopes:

Supabase limits full privileges for file system and database variables, meaning PGAudit modifications can only occur at the role level. Assigning PGAudit to thepostgresrole grants it nearly complete visibility into the database, making role-level adjustments a practical alternative to configuring at the database or system level.

PGAudit'sofficial documentationfocuses on system and database level configs, but its docs officially supports role level configs, too.


### Resources#
- OfficialPGAuditdocumentation
- Database Function Logging
- Supabase Logging
- Self-Hosting Logs


================================================================================


# pgjwt: JSON Web Tokens
Source: https://supabase.com/docs/guides/database/extensions/pgjwt

pgjwt: JSON Web Tokens

Supabase creates and handles JWT for you. It is built into the platform.If you use Postgres version 15 or earlier, you don't need the pgjwt extension, and it is safe to disable. For more information on how Supabase handles JWTs, read theSupabase and JWTs documentation

Thepgjwtextension is deprecated in projects using Postgres 17. It continues to be supported in projects using Postgres 15, but will need to dropped before those projects are upgraded to Postgres 17. See theUpgrading to Postgres 17 notesfor more information.

Thepgjwt(Postgres JSON Web Token) extension allows you to create and parseJSON Web Tokens (JWTs)within a Postgres database. JWTs are commonly used for authentication and authorization in web applications and services.


### Enable the extension#

### API#
- sign(payload json, secret text, algorithm text default 'HSA256'): Signs a JWT containingpayloadwithsecretusingalgorithm.
- verify(token text, secret text, algorithm text default 'HSA256'): Decodes a JWTtokenthat was signed withsecretusingalgorithm.

Where:

- payloadis an encrypted JWT represented as a string.
- secretis the private/secret passcode which is used to sign the JWT and verify its integrity.
- algorithmis the method used to sign the JWT using the secret.
- tokenis an encrypted JWT represented as a string.


### Usage#
Once the extension is installed, you can use its functions to create and parse JWTs. Here's an example of how you can use thesignfunction to create a JWT:

```javascript
1select2  extensions.sign(3    payload   := '{"sub":"1234567890","name":"John Doe","iat":1516239022}',4    secret    := 'secret',5    algorithm := 'HS256'6  );
```

Thepgjwt_encodefunction returns a string that represents the JWT, which can then be safely transmitted between parties.

```javascript
1sign2---------------------------------3 eyJhbGciOiJIUzI1NiIsInR5cCI6IkpX4 VCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiw5 ibmFtZSI6IkpvaG4gRG9lIiwiaWF0Ijo6 xNTE2MjM5MDIyfQ.XbPfbIHMI6arZ3Y97 22BhjWgQzWXcXNrz0ogtVhfEd2o8(1 row)
```

To parse a JWT and extract its claims, you can use theverifyfunction. Here's an example:

```javascript
1select2  extensions.verify(3    token := 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoiRm9vIn0.Q8hKjuadCEhnCPuqIj9bfLhTh_9QSxshTRsA5Aq4IuM',4    secret    := 'secret',5    algorithm := 'HS256'6  );
```

Which returns the decoded contents and some associated metadata.

```javascript
1header            |    payload     | valid2-----------------------------+----------------+-------3 {"alg":"HS256","typ":"JWT"} | {"name":"Foo"} | t4(1 row)
```


### Resources#
- Officialpgjwtdocumentation


================================================================================


# pgmq: Queues
Source: https://supabase.com/docs/guides/database/extensions/pgmq

pgmq: Queues

See theSupabase Queues docs.


================================================================================


# PGroonga: Multilingual Full Text Search
Source: https://supabase.com/docs/guides/database/extensions/pgroonga

PGroonga: Multilingual Full Text Search

PGroongais a Postgres extension adding a full text search indexing method based onGroonga. While native Postgres supports full text indexing, it is limited to alphabet and digit based languages.PGroongaoffers a wider range of character support making it viable for a superset of languages supported by Postgres including Japanese, Chinese, etc.


### Enable the extension#

### Creating a full text search index#
Given a table with atextcolumn:

```javascript
1create table memos (2  id serial primary key,3  content text4);
```

We can index the column for full text search with apgroongaindex:

```javascript
1create index ix_memos_content ON memos USING pgroonga(content);
```

To test the full text index, we'll add some data.

```javascript
1insert into memos(content)2values3  ('PostgreSQL is a relational database management system.'),4  ('Groonga is a fast full text search engine that supports all languages.'),5  ('PGroonga is a PostgreSQL extension that uses Groonga as index.'),6  ('There is groonga command.');
```

The Postgres query planner is smart enough to know that, for extremely small tables, it's faster to scan the whole table rather than loading an index. To force the index to be used, we can disable sequential scans:

```javascript
1-- For testing only. Don't do this in production2set enable_seqscan = off;
```

Now if we run an explain plan on a query filtering onmemos.content:

```javascript
1explain select * from memos where content like '%engine%';23                               QUERY PLAN4-----------------------------------------------------------------------------5Index Scan using ix_memos_content on memos  (cost=0.00..1.11 rows=1 width=36)6  Index Cond: (content ~~ '%engine%'::text)7(2 rows)
```

Thepgroongaindex is used to retrieve the result set:

```javascript
1| id  | content                                                                  |2| --- | ------------------------------------------------------------------------ |3| 2   | 'Groonga is a fast full text search engine that supports all languages.' |
```


### Full text search#
The&@~operator performs full text search. It returns any matching results. UnlikeLIKEoperator,pgroongacan search any text that contains the keyword case insensitive.

Take the following example:

```javascript
1select * from memos where content &@~ 'groonga';
```

And the result:

```javascript
1id | content  2----+------------------------------------------------------------------------32 | Groonga is a fast full text search engine that supports all languages.43 | PGroonga is a PostgreSQL extension that uses Groonga as index.54 | There is groonga command.6(3 rows)
```


### Match all search words#
To find all memos where content contains BOTH of the wordspostgresandpgroonga, we can just use space to separate each words:

```javascript
1select * from memos where content &@~ 'postgres pgroonga';
```

And the result:

```javascript
1id | content  2----+----------------------------------------------------------------33 | PGroonga is a PostgreSQL extension that uses Groonga as index.4(1 row)
```


### Match any search words#
To find all memos where content contain ANY of the wordspostgresorpgroonga, use the upper caseOR:

```javascript
1select * from memos where content &@~ 'postgres OR pgroonga';
```

And the result:

```javascript
1id | content  2----+----------------------------------------------------------------31 | PostgreSQL is a relational database management system.43 | PGroonga is a PostgreSQL extension that uses Groonga as index.5(2 rows)
```


### Search that matches words with negation#
To find all memos where content contain the wordpostgresbut notpgroonga, use-symbol:

```javascript
1select * from memos where content &@~ 'postgres -pgroonga';
```

And the result:

```javascript
1id | content  2----+--------------------------------------------------------31 | PostgreSQL is a relational database management system.4(1 row)
```


### Resources#
- OfficialPGroonga documentation


================================================================================


# pgrouting: Geospatial Routing
Source: https://supabase.com/docs/guides/database/extensions/pgrouting

pgrouting: Geospatial Routing

pgRoutingis Postgres andPostGISextension adding geospatial routing functionality.

The core functionality ofpgRoutingis a set of path finding algorithms including:

- All Pairs Shortest Path, Johnson’s Algorithm
- All Pairs Shortest Path, Floyd-Warshall Algorithm
- Shortest Path A*
- Bi-directional Dijkstra Shortest Path
- Bi-directional A* Shortest Path
- Shortest Path Dijkstra
- Driving Distance
- K-Shortest Path, Multiple Alternative Paths
- K-Dijkstra, One to Many Shortest Path
- Traveling Sales Person
- Turn Restriction Shortest Path (TRSP)


### Enable the extension#

### Example#
As an example, we'll solve thetraveling salesperson problemusing thepgRouting'spgr_TSPeuclideanfunction from some PostGIS coordinates.

A summary of the traveling salesperson problem is, given a set of city coordinates, solve for a path that goes through each city and minimizes the total distance traveled.

First we populate a table with some X, Y coordinates

```javascript
1create table wi29 (2  id bigint,3  x float,4  y float,5  geom gis.geometry6);78insert into wi29 (id, x, y)9values10  (1,20833.3333,17100.0000),11  (2,20900.0000,17066.6667),12  (3,21300.0000,13016.6667),13  (4,21600.0000,14150.0000),14  (5,21600.0000,14966.6667),15  (6,21600.0000,16500.0000),16  (7,22183.3333,13133.3333),17  (8,22583.3333,14300.0000),18  (9,22683.3333,12716.6667),19  (10,23616.6667,15866.6667),20  (11,23700.0000,15933.3333),21  (12,23883.3333,14533.3333),22  (13,24166.6667,13250.0000),23  (14,25149.1667,12365.8333),24  (15,26133.3333,14500.0000),25  (16,26150.0000,10550.0000),26  (17,26283.3333,12766.6667),27  (18,26433.3333,13433.3333),28  (19,26550.0000,13850.0000),29  (20,26733.3333,11683.3333),30  (21,27026.1111,13051.9444),31  (22,27096.1111,13415.8333),32  (23,27153.6111,13203.3333),33  (24,27166.6667,9833.3333),34  (25,27233.3333,10450.0000),35  (26,27233.3333,11783.3333),36  (27,27266.6667,10383.3333),37  (28,27433.3333,12400.0000),38  (29,27462.5000,12992.2222);
```

Next we use thepgr_TSPeuclideanfunction to find the best path.

```javascript
1select2    *3from4     pgr_TSPeuclidean($$select * from wi29$$)
```

```javascript
1seq | node |       cost       |     agg_cost     2-----+------+------------------+------------------3   1 |    1 |                0 |                04   2 |    2 |  74.535614157127 |  74.5356141571275   3 |    6 | 900.617093380362 | 975.1527075374896   4 |   10 | 2113.77757765045 | 3088.930285187937   5 |   11 | 106.718669615254 | 3195.648954803198   6 |   12 | 1411.95293791574 | 4607.601892718939   7 |   13 | 1314.23824873744 | 5921.8401414563710   8 |   14 | 1321.76283931305 | 7243.6029807694211   9 |   17 | 1202.91366735569 |  8446.516648125112  10 |   18 | 683.333268292684 | 9129.8499164177913  11 |   15 | 1108.05137466134 | 10237.901291079114  12 |   19 | 772.082339448903 |  11009.98363052815  13 |   22 | 697.666150054665 | 11707.649780582716  14 |   23 | 220.141999627513 | 11927.791780210217  15 |   21 | 197.926372783442 | 12125.718152993718  16 |   29 | 440.456596290771 | 12566.174749284419  17 |   28 | 592.939989005405 | 13159.114738289820  18 |   26 | 648.288376333318 | 13807.403114623121  19 |   20 | 509.901951359278 | 14317.305065982422  20 |   25 | 1330.83095428717 | 15648.136020269623  21 |   27 |  74.535658878487 | 15722.671679148124  22 |   24 | 559.016994374947 |  16281.68867352325  23 |   16 | 1243.87392358622 | 17525.562597109226  24 |    9 |  4088.0585364911 | 21613.621133600427  25 |    7 |  650.85409697993 | 22264.475230580328  26 |    3 | 891.004385199336 | 23155.479615779629  27 |    4 | 1172.36699411442 |  24327.84660989430  28 |    8 | 994.708187806297 | 25322.554797700331  29 |    5 | 1188.01888359478 | 26510.573681295132  30 |    1 | 2266.91173136004 | 28777.4854126552
```


### Resources#
- OfficialpgRoutingdocumentation


================================================================================


# pgsodium (pending deprecation): Encryption Features
Source: https://supabase.com/docs/guides/database/extensions/pgsodium

pgsodium (pending deprecation): Encryption Features

Supabase DOES NOT RECOMMEND any new usage ofpgsodium.

Thepgsodiumextension is expected to go through a deprecation cycle in the near future. We will reach out to owners of impacted projects to assist with migrations away frompgsodiumonce the deprecation process begins.

TheVault extensionwon’t be impacted. Its internal implementation will shift away from pgsodium, but the interface and API will remain unchanged.

pgsodiumis a Postgres extension which provides SQL access tolibsodium'shigh-level cryptographic algorithms.

Supabase previously documented two features derived from pgsodium. NamelyServer Key ManagementandTransparent Column Encryption. At this time, we do not recommend using either on the Supabase platform due to their high level of operational complexity and misconfiguration risk.

Note that Supabase projects are encrypted at rest by default which likely is sufficient for your compliance needs e.g. SOC2 & HIPAA.


### Get the root encryption key for your Supabase project#
Encryption requires keys. Keeping the keys in the same database as the encrypted data would be unsafe. For more information about managing thepgsodiumroot encryption key on your Supabase project seeencryption key location. This key is required to decrypt values stored inSupabase Vaultand data encrypted with Transparent Column Encryption.


### Resources#
- Supabase Vault
- Read more about Supabase Vault in theblog post
- Supabase Vault on GitHub


### Resources#
- Officialpgsodiumdocumentation


================================================================================


# pgTAP: Unit Testing
Source: https://supabase.com/docs/guides/database/extensions/pgtap

pgTAP: Unit Testing

pgTAPis a unit testing extension for Postgres.


### Overview#
Let's cover some basic concepts:

- Unit tests: allow you to test small parts of a system (like a database table!).
- TAP: stands forTest Anything Protocol. It is an framework which aims to simplify the error reporting during testing.


### Enable the extension#

### Testing tables#
```javascript
1begin;2select plan( 1 );34select has_table( 'profiles' );56select * from finish();7rollback;
```

API:

- has_table(): Tests whether or not a table exists in the database
- has_index(): Checks for the existence of a named index associated with the named table.
- has_relation(): Tests whether or not a relation exists in the database.


### Testing columns#
```javascript
1begin;2select plan( 2 );34select has_column( 'profiles', 'id' ); -- test that the "id" column exists in the "profiles" table5select col_is_pk( 'profiles', 'id' ); -- test that the "id" column is a primary key67select * from finish();8rollback;
```

API:

- has_column(): Tests whether or not a column exists in a given table, view, materialized view or composite type.
- col_is_pk(): Tests whether the specified column or columns in a table is/are the primary key for that table.


### Testing RLS policies#
```javascript
1begin;2select plan( 1 );34select policies_are(5  'public',6  'profiles',7  ARRAY [8    'Profiles are public', -- Test that there is a policy called  "Profiles are public" on the "profiles" table.9    'Profiles can only be updated by the owner'  -- Test that there is a policy called  "Profiles can only be updated by the owner" on the "profiles" table.10  ]11);1213select * from finish();14rollback;
```

API:

- policies_are(): Tests that all of the policies on the named table are only the policies that should be on that table.
- policy_roles_are(): Tests whether the roles to which policy applies are only the roles that should be on that policy.
- policy_cmd_is(): Tests whether the command to which policy applies is same as command that is given in function arguments.

You can also use theresults_eq()method to test that a Policy returns the correct data:

```javascript
1begin;2select plan( 1 );34select results_eq(5    'select * from profiles()',6    $$VALUES ( 1, 'Anna'), (2, 'Bruce'), (3, 'Caryn')$$,7    'profiles() should return all users'8);91011select * from finish();12rollback;
```

API:

- results_eq()
- results_ne()


### Testing functions#
```javascript
1prepare hello_expr as select 'hello'23begin;4select plan(3);5-- You'll need to create a hello_world and is_even function6select function_returns( 'hello_world', 'text' );                   -- test if the function "hello_world" returns text7select function_returns( 'is_even', ARRAY['integer'], 'boolean' );  -- test if the function "is_even" returns a boolean8select results_eq('select * from hello_world()', 'hello_expr');          -- test if the function "hello_world" returns "hello"910select * from finish();11rollback;
```

API:

- function_returns(): Tests that a particular function returns a particular data type
- is_definer(): Tests that a function is a security definer (that is, asetuidfunction).


### Resources#
- OfficialpgTAPdocumentation


================================================================================


# pgvector: Embeddings and vector similarity
Source: https://supabase.com/docs/guides/database/extensions/pgvector

pgvector: Embeddings and vector similarity

pgvectoris a Postgres extension for vector similarity search. It can also be used for storingembeddings.

The name of pgvector's Postgres extension isvector.

Learn more about Supabase'sAI & Vectoroffering.


### Concepts#

### Vector similarity#
Vector similarity refers to a measure of the similarity between two related items. For example, if you have a list of products, you can use vector similarity to find similar products. To do this, you need to convert each product into a "vector" of numbers, using a mathematical model. You can use a similar model for text, images, and other types of data. Once all of these vectors are stored in the database, you can use vector similarity to find similar items.


### Embeddings#
This is particularly useful if you're building AI applications with large language models. You can create and storeembeddingsfor retrieval augmented generation (RAG).


### Usage#

### Enable the extension#

### Usage#

### Create a table to store vectors#
```javascript
1create table posts (2  id serial primary key,3  title text not null,4  body text not null,5  embedding extensions.vector(384)6);
```


### Storing a vector / embedding#
In this example we'll generate a vector using Transformer.js, then store it in the database using the Supabase client.

```javascript
1import { pipeline } from '@xenova/transformers'2const generateEmbedding = await pipeline('feature-extraction', 'Supabase/gte-small')34const title = 'First post!'5const body = 'Hello world!'67// Generate a vector using Transformers.js8const output = await generateEmbedding(body, {9  pooling: 'mean',10  normalize: true,11})1213// Extract the embedding output14const embedding = Array.from(output.data)1516// Store the vector in Postgres17const { data, error } = await supabase.from('posts').insert({18  title,19  body,20  embedding,21})
```


### Specific usage cases#

### Queries with filtering#
If you use an IVFFlat or HNSW index and naively filter the results based on the value of another column, you may get fewer rows returned than requested.

For example, the following query may return fewer than 5 rows, even if 5 corresponding rows exist in the database. This is because the embedding index may not return 5 rows matching the filter.

```javascript
1SELECT * FROM items WHERE category_id = 123 ORDER BY embedding <-> '[3,1,2]' LIMIT 5;
```

To get the exact number of requested rows, useiterative searchto continue scanning the index until enough results are found.


### More pgvector and Supabase resources#
- Supabase Clippy: ChatGPT for Supabase Docs
- Storing OpenAI embeddings in Postgres with pgvector
- A ChatGPT Plugins Template built with Supabase Edge Runtime
- Template for building your own custom ChatGPT style doc search


================================================================================


# plpgsql_check: PL/pgSQL Linter
Source: https://supabase.com/docs/guides/database/extensions/plpgsql_check

plpgsql_check: PL/pgSQL Linter

plpgsql_checkis a Postgres extension that lints plpgsql for syntax, semantic and other related issues. The tool helps developers to identify and correct errors before executing the code. plpgsql_check is most useful for developers who are working with large or complex SQL codebases, as it can help identify and resolve issues early in the development cycle.


### Enable the extension#

### API#
- plpgsql_check_function( ... ): Scans a function for errors.

plpgsql_check_functionis highly customizable. For a complete list of available arguments seethe docs


### Usage#
To demonstrateplpgsql_checkwe can create a function with a known error. In this case we create a functionsome_func, that references a non-existent columnplace.created_at.

```javascript
1create table place(2  x float,3  y float4);56create or replace function public.some_func()7  returns void8  language plpgsql9as $$10declare11  rec record;12begin13  for rec in select * from place14  loop15    -- Bug: There is no column `created_at` on table `place`16    raise notice '%', rec.created_at;17  end loop;18end;19$$;
```

Note that executing the function would not catch the invalid reference error because theloopdoes not execute if no rows are present in the table.

```javascript
1select public.some_func();2  some_func3 ───────────45 (1 row)
```

Now we can use plpgsql_check'splpgsql_check_functionfunction to identify the known error.

```javascript
1select plpgsql_check_function('public.some_func()');23                   plpgsql_check_function4------------------------------------------------------------5 error:42703:8:RAISE:record "rec" has no field "created_at"6 Context: SQL expression "rec.created_at"
```


### Resources#
- Officialplpgsql_checkdocumentation


================================================================================


# plv8: JavaScript Language
Source: https://supabase.com/docs/guides/database/extensions/plv8

plv8: JavaScript Language

Theplv8extension is deprecated in projects using Postgres 17. It continues to be supported in projects using Postgres 15, but will need to dropped before those projects are upgraded to Postgres 17. See theUpgrading to Postgres 17 notesfor more information.

Theplv8extension allows you use JavaScript within Postgres.


### Overview#
While Postgres natively runs SQL, it can also run other procedural languages.plv8allows you to run JavaScript code - specifically any code that runs on theV8 JavaScript engine.

It can be used for database functions, triggers, queries and more.


### Enable the extension#

### Createplv8functions#
Functions written inplv8are written just like any other Postgres functions, only
with thelanguageidentifier set toplv8.

```javascript
1create or replace function function_name()2returns void as $$3    // V8 JavaScript4    // code5    // here6$$ language plv8;
```

You can callplv8functions like any other Postgres function:

```javascript
1select function_name();
```


### Examples#

### Scalar functions#
Ascalar functionis anything that takes in some user input and returns a single result.

```javascript
1create or replace function hello_world(name text)2returns text as $$34    let output = `Hello, ${name}!`;5    return output;67$$ language plv8;
```


### Executing SQL#
You can execute SQL withinplv8code using theplv8.executefunction.

```javascript
1create or replace function update_user(id bigint, first_name text)2returns smallint as $$34    var num_affected = plv8.execute(5        'update profiles set first_name = $1 where id = $2',6        [first_name, id]7    );89    return num_affected;10$$ language plv8;
```


### Set-returning functions#
Aset-returning functionis anything that returns a full set of results - for example, rows in a table.

```javascript
1create or replace function get_messages()2returns setof messages as $$34    var json_result = plv8.execute(5        'select * from messages'6    );78    return json_result;9$$ language plv8;1011select * from get_messages();
```


### Resources#
- Officialplv8documentation
- plv8 GitHub Repository


================================================================================


# PostGIS: Geo queries
Source: https://supabase.com/docs/guides/database/extensions/postgis

PostGIS: Geo queries

PostGISis a Postgres extension that allows you to interact with Geo data within Postgres. You can sort your data by geographic location, get data within certain geographic boundaries, and do much more with it.


### Overview#
While you may be able to store simple lat/long geographic coordinates as a set of decimals, it does not scale very well when you try to query through a large data set. PostGIS comes with special data types that are efficient, and indexable for high scalability.

The additional data types that PostGIS provides includePoint,Polygon,LineString, and many more to represent different types of geographical data. In this guide, we will mainly focus on how to interact withPointtype, which represents a single set of latitude and longitude. If you are interested in digging deeper, you can learn more about different data types on thedata management section of PostGIS docs.


### Enable the extension#
You can get started with PostGIS by enabling the PostGIS extension in your Supabase dashboard.


### Examples#
Now that we are ready to get started with PostGIS, let’s create a table and see how we can utilize PostGIS for some typical use cases. Let’s imagine we are creating a simple restaurant-searching app.

Let’s create our table. Each row represents a restaurant with its location stored inlocationcolumn as aPointtype.

```javascript
1create table if not exists public.restaurants (2	id int generated by default as identity primary key,3	name text not null,4	location extensions.geography(POINT) not null5);
```

We can then set aspatial indexon thelocationcolumn of this table.

```javascript
1create index restaurants_geo_index2  on public.restaurants3  using GIST (location);
```


### Inserting data#
You can insert geographical data through SQL or through our API.


### Restaurants
Notice the order in which you pass the latitude and longitude. Longitude comes first, and is because longitude represents the x-axis of the location. Another thing to watch for is when inserting data from the client library, there is no comma between the two values, just a single space.

At this point, if you go into your Supabase dashboard and look at the data, you will notice that the value of thelocationcolumn looks something like this.

```javascript
10101000020E6100000A4DFBE0E9C91614044FAEDEBC0494240
```

We can query therestaurantstable directly, but it will return thelocationcolumn in the format you see above.
We will createdatabase functionsso that we can use thest_y()andst_x()function to convert it back to lat and long floating values.


### Order by distance#
Sorting datasets from closest to farthest, sometimes called nearest-neighbor sort, is a very common use case in Geo-queries. PostGIS can handle it with the use of the<->operator.<->operator returns the two-dimensional distance between two geometries and will utilize the spatial index when used withinorder byclause. You can create the following database function to sort the restaurants from closest to farthest by passing the current locations as parameters.

```javascript
1create or replace function nearby_restaurants(lat float, long float)2returns table (id public.restaurants.id%TYPE, name public.restaurants.name%TYPE, lat float, long float, dist_meters float)3set search_path = ''4language sql5as $$6  select id, name, extensions.st_y(location::extensions.geometry) as lat, extensions.st_x(location::extensions.geometry) as long, extensions.st_distance(location, extensions.st_point(long, lat)::extensions.geography) as dist_meters7  from public.restaurants8  order by location operator(extensions.<->) extensions.st_point(long, lat)::extensions.geography;9$$;
```

Now you can call this function from your client usingrpc()like this:

```javascript
1const { data, error } = await supabase.rpc('nearby_restaurants', {2  lat: 40.807313,3  long: -73.946713,4})
```


### Finding all data points within a bounding box#


When you are working on a map-based application where the user scrolls through your map, you might want to load the data that lies within the bounding box of the map every time your users scroll. PostGIS can return the rows that are within the bounding box just by supplying the bottom left and the top right coordinates. Let’s look at what the function would look like:

```javascript
1create or replace function restaurants_in_view(min_lat float, min_long float, max_lat float, max_long float)2returns table (id public.restaurants.id%TYPE, name public.restaurants.name%TYPE, lat float, long float)3set search_path to ''4language sql5as $$6	select id, name, extensions.st_y(location::extensions.geometry) as lat, extensions.st_x(location::extensions.geometry) as long7	from public.restaurants8	where location operator(extensions.&&) extensions.ST_SetSRID(extensions.ST_MakeBox2D(extensions.ST_Point(min_long, min_lat), extensions.ST_Point(max_long, max_lat)), 4326)9$$;
```

The&&operator used in thewherestatement here returns a boolean of whether the bounding box of the two geometries intersect or not. We are basically creating a bounding box from the two points and finding those points that fall under the bounding box. We are also utilizing a few different PostGIS functions:

- ST_MakeBox2D: Creates a 2-dimensional box from two points.
- ST_SetSRID: Sets theSRID, which is an identifier of what coordinate system to use for the geometry. 4326 is the standard longitude and latitude coordinate system.

You can call this function from your client usingrpc()like this:

```javascript
1const { data, error } = await supabase.rpc('restaurants_in_view', {2  min_lat: 40.807,3  min_long: -73.946,4  max_lat: 40.808,5  max_long: -73.945,6})
```


### Troubleshooting#
As of PostGIS 2.3 or newer, the PostGIS extension is no longer relocatable from one schema to another. If you need to move it from one schema to another for any reason (e.g. from the public schema to the extensions schema for security reasons), you would normally run a ALTER EXTENSION to relocate the schema. However, you will now to do the following steps:

Backup your Database to prevent data loss - You can do this through theCLIor Postgres backup tools such aspg_dumpall

Drop all dependencies you created and the PostGIS extension -DROP EXTENSION postgis CASCADE;

Enable PostGIS extension in the new schema -CREATE EXTENSION postgis SCHEMA extensions;

Restore dropped data via the Backup if necessary from step 1 with your tool of choice.

Alternatively, you can contact theSupabase Support Teamand ask them to run the following SQL on your instance:

```javascript
1BEGIN;2	UPDATE pg_extension3	  SET extrelocatable = true4	WHERE extname = 'postgis';56	ALTER EXTENSION postgis7	  SET SCHEMA extensions;89	ALTER EXTENSION postgis10	  UPDATE TO "<POSTGIS_VERSION>next";1112	ALTER EXTENSION postgis UPDATE;1314	UPDATE pg_extension15	  SET extrelocatable = false16	WHERE extname = 'postgis';17COMMIT;
```


### Resources#
- Official PostGIS documentation


================================================================================


# postgres_fdw
Source: https://supabase.com/docs/guides/database/extensions/postgres_fdw

postgres_fdw

The extension enables Postgres to query tables and views on a remote Postgres server.


### Enable the extension#

### Create a connection to another database#

### Create a foreign server
Define the remote database address

```javascript
1create server "<foreign_server_name>"2    foreign data wrapper postgres_fdw3    options (4        host '<host>',5        port '<port>',6        dbname '<dbname>'7    );
```


### Create a server mapping
Set the user credentials for the remote server

```javascript
1create user mapping for "<dbname>"2server "<foreign_server_name>"3options (4    user '<db_user>',5    password '<password>'6);
```


### Import tables
Import tables from the foreign database

Example: Import all tables from a schema

```javascript
1import foreign schema "<foreign_schema>"2from server "<foreign_server>"3into "<host_schema>";
```

Example: Import specific tables

```javascript
1import foreign schema "<foreign_schema>"2limit to (3    "<table_name1>",4    "<table_name2>"5)6from server "<foreign_server>"7into "<host_schema>";
```


### Query foreign table
```javascript
1select * from "<foreign_table>"
```


### Configuring execution options#

### Fetch_size#
Maximum rows fetched per operation. For example, fetching 200 rows withfetch_sizeset to 100 requires 2 requests.

```javascript
1alter server "<foreign_server_name>"2options (fetch_size '10000');
```


### Batch_size#
Maximum rows inserted per cycle. For example, inserting 200 rows withbatch_sizeset to 100 requires 2 requests.

```javascript
1alter server "<foreign_server_name>"2options (batch_size '1000');
```


### Extensions#
Lists shared extensions. Without them, queries involving unlisted extension functions or operators may fail or omit references.

```javascript
1alter server "<foreign_server_name>"2options (extensions 'vector, postgis');
```

For more server options, check the extension'sofficial documentation


### Resources#
- Officialpostgres_fdwdocumentation


================================================================================


# RUM: improved inverted index for full-text search based on GIN index
Source: https://supabase.com/docs/guides/database/extensions/rum

RUM: improved inverted index for full-text search based on GIN index

RUMis an extension which adds a RUM index to Postgres.

RUM index is based on GIN that stores additional per-entry information in a posting tree. For example, positional information of lexemes or timestamps. In comparison to GIN it can use this information to make faster index-only scans for:

- Phrase search
- Text search with ranking by text distance operator
- TextSELECTs with ordering by some non-indexed additional column e.g. by timestamp.

RUM works best in scenarios when the possible keys are highly repeatable. I.e. all texts are composed of a
limited amount of words, so per-lexeme indexing gives significant speed-up in searching texts containing word
combinations or phrases.

Main operators for ordering are:

tsvector<=>tsquery|float4| Distance betweentsvectorandtsquery.
value<=>value |float8| Distance between two values.

Where value istimestamp,timestamptz,int2,int4,int8,float4,float8,moneyandoid


### Usage#

### Enable the extension#
You can get started with rum by enabling the extension in your Supabase dashboard.


### Syntax#

### For type:tsvector#
To understand the following you may need first to seeOfficial Postgres documentation on text
search

rum_tsvector_ops

```javascript
1CREATE TABLE test_rum(t text, a tsvector);23CREATE TRIGGER tsvectorupdate4BEFORE UPDATE OR INSERT ON test_rum5FOR EACH ROW EXECUTE PROCEDURE tsvector_update_trigger('a', 'pg_catalog.english', 't');67INSERT INTO test_rum(t) VALUES ('The situation is most beautiful');8INSERT INTO test_rum(t) VALUES ('It is a beautiful');9INSERT INTO test_rum(t) VALUES ('It looks like a beautiful place');1011CREATE INDEX rumidx ON test_rum USING rum (a rum_tsvector_ops);
```

And we can executetsvectorselects with ordering by text distance operator:

```javascript
1SELECT t, a `<=>` to_tsquery('english', 'beautiful | place') AS rank2    FROM test_rum3    WHERE a @@ to_tsquery('english', 'beautiful | place')4    ORDER BY a `<=>` to_tsquery('english', 'beautiful | place');5                t                |  rank6---------------------------------+---------7 It looks like a beautiful place | 8.224678 The situation is most beautiful | 16.44939 It is a beautiful               | 16.449310(3 rows)
```

rum_tsvector_addon_ops

```javascript
1CREATE TABLE tsts (id int, t tsvector, d timestamp);2CREATE INDEX tsts_idx ON tsts USING rum (t rum_tsvector_addon_ops, d)3    WITH (attach = 'd', to = 't');
```

Now we can execute the selects with ordering distance operator on attached column:

```javascript
1SELECT id, d, d `<=>` '2016-05-16 14:21:25' FROM tsts WHERE t @@ 'wr&qh' ORDER BY d `<=>` '2016-05-16 14:21:25' LIMIT 5;2 id  |                d                |   ?column?3-----+---------------------------------+---------------4 355 | Mon May 16 14:21:22.326724 2016 |      2.6732765 354 | Mon May 16 13:21:22.326724 2016 |   3602.6732766 371 | Tue May 17 06:21:22.326724 2016 |  57597.3267247 406 | Wed May 18 17:21:22.326724 2016 | 183597.3267248 415 | Thu May 19 02:21:22.326724 2016 | 215997.3267249(5 rows)
```


### For type:anyarray#
rum_anyarray_ops

This operator class storesanyarrayelements with length of the array. It supports operators&&,@>,<@,=,%operators. It also supports ordering by<=>operator.

```javascript
1CREATE TABLE test_array (i int2[]);2INSERT INTO test_array VALUES ('{}'), ('{0}'), ('{1,2,3,4}'), ('{1,2,3}'), ('{1,2}'), ('{1}');3CREATE INDEX idx_array ON test_array USING rum (i rum_anyarray_ops);
```

Now we can execute the query using index scan:

```javascript
1SELECT * FROM test_array WHERE i && '{1}' ORDER BY i `<=>` '{1}' ASC;2     i3-----------4 {1}5 {1,2}6 {1,2,3}7 {1,2,3,4}8(4 rows)
```

rum_anyarray_addon_ops

The does the same withanyarrayindex asrum_tsvector_addon_opsi.e. allows to order select results using distance
operator by attached column.


### Limitations#
RUMhas slower build and insert times thanGINdue to:


### Resources#
- Official RUM documentation


================================================================================


# timescaledb: Time-Series data
Source: https://supabase.com/docs/guides/database/extensions/timescaledb

timescaledb: Time-Series data

Thetimescaledbextension is deprecated in projects using Postgres 17. It continues to be supported in projects using Postgres 15, but will need to dropped before those projects are upgraded to Postgres 17. See theUpgrading to Postgres 17 notesfor more information.

timescaledbis a Postgres extension designed for improved handling of time-series data. It provides a scalable, high-performance solution for storing and querying time-series data on top of a standard Postgres database.

timescaledbuses a time-series-aware storage model and indexing techniques to improve performance of Postgres in working with time-series data. The extension divides data into chunks based on time intervals, allowing it to scale efficiently, especially for large data sets. The data is then compressed, optimized for write-heavy workloads, and partitioned for parallel processing.timescaledbalso includes a set of functions, operators, and indexes that work with time-series data to reduce query times, and make data easier to work with.

Supabase projects come withTimescaleDB Apache 2 Edition. Functionality only available under the Community Edition is not available.


### Enable the extension#
Even though the SQL code iscreate extension, this is the equivalent of "enabling the extension". To disable an extension you can calldrop extension.

It's good practice to create the extension within a separate schema (likeextensions) to keep yourpublicschema clean.


### Usage#
To demonstrate howtimescaledbworks, let's consider a simple example where we have a table that stores temperature data from different sensors. We will create a table named "temperatures" and store data for two sensors.

First we create a hypertable, which is a virtual table that is partitioned into chunks based on time intervals. The hypertable acts as a proxy for the actual table and makes it easy to query and manage time-series data.

```javascript
1create table temperatures (2  time timestamptz not null,3  sensor_id int not null,4  temperature double precision not null5);67select create_hypertable('temperatures', 'time');
```

Next, we can populate some values

```javascript
1insert into temperatures (time, sensor_id, temperature)2values3    ('2023-02-14 09:00:00', 1, 23.5),4    ('2023-02-14 09:00:00', 2, 21.2),5    ('2023-02-14 09:05:00', 1, 24.5),6    ('2023-02-14 09:05:00', 2, 22.3),7    ('2023-02-14 09:10:00', 1, 25.1),8    ('2023-02-14 09:10:00', 2, 23.9),9    ('2023-02-14 09:15:00', 1, 24.9),10    ('2023-02-14 09:15:00', 2, 22.7),11    ('2023-02-14 09:20:00', 1, 24.7),12    ('2023-02-14 09:20:00', 2, 23.5);
```

And finally we can query the table usingtimescaledb'stime_bucketfunction to divide the time-series into intervals of the specified size (in this case, 1 hour) averaging thetemperaturereading within each group.

```javascript
1select2    time_bucket('1 hour', time) AS hour,3    avg(temperature) AS average_temperature4from5    temperatures6where7    sensor_id = 18    and time > NOW() - interval '1 hour'9group by10    hour;
```


### Resources#
- Officialtimescaledbdocumentation


================================================================================


# uuid-ossp: Unique Identifiers
Source: https://supabase.com/docs/guides/database/extensions/uuid-ossp

uuid-ossp: Unique Identifiers

Theuuid-osspextension can be used to generate aUUID.


### Overview#
AUUIDis a "Universally Unique Identifier" and it is, for practical purposes, unique.
This makes them particularly well suited as Primary Keys. It is occasionally referred to as aGUID, which stands for "Globally Unique Identifier".


### Enable the extension#
Note:
Currentlyuuid-osspextension is enabled by default and cannot be disabled.


### Theuuidtype#
Once the extension is enabled, you now have access to auuidtype.


### uuid_generate_v1()#
Creates a UUID value based on the combination of computer’s MAC address, current timestamp, and a random value.

UUIDv1 leaks identifiable details, which might make it unsuitable for certain security-sensitive applications.


### uuid_generate_v4()#
Creates UUID values based solely on random numbers. You can also use Postgres's built-ingen_random_uuid()function to generate a UUIDv4.


### Examples#

### Within a query#
```javascript
1select uuid_generate_v4();
```


### As a primary key#
Automatically create a unique, random ID in a table:

```javascript
1create table contacts (2  id uuid default uuid_generate_v4(),3  first_name text,4  last_name text,5  primary key (id)6);
```


### Resources#
- Choosing a Postgres Primary Key
- The Basics Of PostgresUUIDData Type


================================================================================


# Foreign Data Wrappers
Source: https://supabase.com/docs/guides/database/extensions/wrappers/overview

Foreign Data Wrappers


### Connecting to external systems using Postgres Foreign Data Wrappers.
Connecting to external systems using Postgres Foreign Data Wrappers.

Foreign Data Wrappers (FDW) are a core feature of Postgres that allow you to access and query data stored in external data sources as if they were native Postgres tables.

Postgres includes several built-in foreign data wrappers, such aspostgres_fdwfor accessing other Postgres databases, andfile_fdwfor reading data from files. Supabase extends this feature to query other databases or any other external systems. We do this with our open sourceWrappersframework. In these guides we'll refer to them as "Wrappers", Foreign Data Wrappers, or FDWs. They are conceptually the same thing.


### Concepts#
Wrappers introduce some new terminology and different workflows.


### Remote servers#
A Remote Server is an external database, API, or any system containing data that you want to query from your Postgres database. Examples include:

- An external database, like Postgres or Firebase.
- A remote data warehouse, like ClickHouse, BigQuery, or Snowflake.
- An API, like Stripe or GitHub.

It's possible to connect to multiple remote servers of the same type. For example, you can connect to two different Firebase projects within the same Supabase database.


### Foreign tables#
A table in your database which maps to some data inside a Remote Server.

Examples:

- Ananalyticstable which maps to a table inside your data warehouse.
- Asubscriptionstable which maps to your Stripe subscriptions.
- Acollectionstable which maps to a Firebase collection.

Although a foreign table behaves like any other table, the data is not stored inside your database. The data remains inside the Remote Server.


### ETL with Wrappers#
ETL stands for Extract, Transform, Load. It's an established process for moving data from one system to another. For example, it's common to move data from a production database to a data warehouse.

There are many popular ETL tools, such asFivetranandAirbyte.

Wrappers provide an alternative to these tools. You can use SQL to move data from one table to another:

```javascript
1-- Copy data from your production database to your2-- data warehouse for the last 24 hours:34insert into warehouse.analytics5select * from public.analytics6where ts > (now() - interval '1 DAY');
```

This approach provides several benefits:

One disadvantage is that Wrappers are not as feature-rich as ETL tools. They also couple the ETL process to your database.


### On-demand ETL with Wrappers#
Supabase extends the ETL concept with real-time data access. Instead of moving gigabytes of data from one system to another before you can query it, you can instead query the data directly from the remote server. This additional option, "Query", extends the ETL process and is calledQETL(pronounced "kettle"): Query, Extract, Transform, Load.

```javascript
1-- Get all purchases for a user from your data warehouse:2select3  auth.users.id as user_id,4  warehouse.orders.id as order_id5from6  warehouse.orders7join 8  auth.users on auth.users.id = warehouse.orders.user_id9where 10  auth.users.id = '<some_user_id>';
```

This approach has several benefits:


### Batch ETL with Wrappers#
A common use case for Wrappers is to extract data from a production database and load it into a data warehouse. This can be done within your database usingpg_cron. For example, you can schedule a job to run every night to extract data from your production database and load it into your data warehouse.

```javascript
1-- Every day at 3am, copy data from your2-- production database to your data warehouse:3select cron.schedule(4  'nightly-etl',5  '0 3 * * *',6  $$7    insert into warehouse.analytics8    select * from public.analytics9    where ts > (now() - interval '1 DAY');10  $$11);
```

This process can be taxing on your database if you are moving large amounts of data. Often, it's better to use an external tool for batch ETL, such asFivetranorAirbyte.


### WebAssembly Wrappers#
WebAssembly (Wasm) is a binary instruction format that enables high-performance execution of code on the web. Wrappers now includes a Wasm runtime, which provides a sandboxed execution environment, to run Wasm foreign data wrappers. Combined Wrappers with Wasm, developing and distributing new FDW becomes much easier and you can even build your own Wasm FDW and use it on Supabase platform.

To learn more about Wasm FDW, visitWrappers official documentation.


### Security#
Foreign Data Wrappers do not provide Row Level Security, thus it is not advised to expose them via your API. Wrappers shouldalwaysbe stored in a private schema. For example, if you are connecting to your Stripe account, you should create astripeschema to store all of your foreign tables inside. This schema shouldnotbe added to the “Additional Schemas” setting in the API section.

If you want to expose any of the foreign table columns to your public API, you can create aDatabase Function with security definerin thepublicschema, and then you can interact with your foreign table through API. For better access control, the function should have appropriate filters on the foreign table to apply security rules based on your business needs.

As an example, go toSQL Editorand then follow below steps,

Create a Stripe Products foreign table:

```javascript
1create foreign table stripe.stripe_products (2  id text,3  name text,4  active bool,5  default_price text,6  description text,7  created timestamp,8  updated timestamp,9  attrs jsonb10)11  server stripe_fdw_server12  options (13    object 'products',14    rowid_column 'id'15  );
```

Create a security definer function that queries the foreign table and filters on the name prefix parameter:

```javascript
1create function public.get_stripe_products(name_prefix text)2returns table (3  id text,4  name text,5  active boolean,6  default_price text,7  description text8)9language plpgsql10security definer set search_path = ''11as $$12begin13  return query14  select15    t.id,16    t.name,17    t.active,18    t.default_price,19    t.description20  from21    stripe.stripe_products t22  where23    t.name like name_prefix || '%'24  ;25end;26$$;
```

Restrict the function execution to a specific role only, for example, the authenticated users:

By default, the function created can be executed by any roles likeanon, that means the
foreign table is public accessible. Always limit the function execution permission to
appropriate roles.

```javascript
1-- revoke public execute permission2revoke execute on function public.get_stripe_products from public;3revoke execute on function public.get_stripe_products from anon;45-- grant execute permission to a specific role only6grant execute on function public.get_stripe_products to authenticated;
```

Once the preceding steps are finished, the function can be invoked from Supabase client to query the foreign table:

```javascript
1const { data, error } = await supabase2  .rpc('get_stripe_products', { name_prefix: 'Test' })3  .select('*')4if (error) console.error(error)5else console.log(data)
```


### Resources#
- Officialsupabase/wrappersdocumentation


================================================================================


# Full Text Search
Source: https://supabase.com/docs/guides/database/full-text-search

Full Text Search


### How to use full text search in PostgreSQL.
How to use full text search in PostgreSQL.

Postgres has built-in functions to handleFull Text Searchqueries. This is like a "search engine" within Postgres.


### Preparation#
For this guide we'll use the following example data:


### Usage#
The functions we'll cover in this guide are:


### to_tsvector()#
Converts your data into searchable tokens.to_tsvector()stands for "to text search vector." For example:

```javascript
1select to_tsvector('green eggs and ham');2-- Returns 'egg':2 'green':1 'ham':4
```

Collectively these tokens are called a "document" which Postgres can use for comparisons.


### to_tsquery()#
Converts a query string into tokens to match.to_tsquery()stands for "to text search query."

This conversion step is important because we will want to "fuzzy match" on keywords.
For example if a user searches foreggs, and a column has the valueegg, we probably still want to return a match.

Postgres provides several functions to create tsquery objects:

- to_tsquery()- Requires manual specification of operators (&,|,!)
- plainto_tsquery()- Converts plain text to an AND query:plainto_tsquery('english', 'fat rats')→'fat' & 'rat'
- phraseto_tsquery()- Creates phrase queries:phraseto_tsquery('english', 'fat rats')→'fat' <-> 'rat'
- websearch_to_tsquery()- Supports web search syntax with quotes, "or", and negation


### Match:@@#
The@@symbol is the "match" symbol for Full Text Search. It returns any matches between ato_tsvectorresult and ato_tsqueryresult.

Take the following example:

```javascript
1select *2from books3where title = 'Harry';
```

The equality symbol above (=) is very "strict" on what it matches. In a full text search context, we might want to find all "Harry Potter" books and so we can rewrite the
example above:

```javascript
1select *2from books3where to_tsvector(title) @@ to_tsquery('Harry');
```


### Basic full text queries#

### Search a single column#
To find allbookswhere thedescriptioncontain the wordbig:

```javascript
1select2  *3from4  books5where6  to_tsvector(description)7  @@ to_tsquery('big');
```


### Search multiple columns#
Right now there is no direct way to use JavaScript or Dart to search through multiple columns but you can do it by creatingcomputed columnson the database.

To find allbookswheredescriptionortitlecontain the wordlittle:

```javascript
1select2  *3from4  books5where6  to_tsvector(description || ' ' || title) -- concat columns, but be sure to include a space to separate them!7  @@ to_tsquery('little');
```


### Match all search words#
To find allbookswheredescriptioncontains BOTH of the wordslittleandbig, we can use the&symbol:

```javascript
1select2  *3from4  books5where6  to_tsvector(description)7  @@ to_tsquery('little & big'); -- use & for AND in the search query
```


### Match any search words#
To find allbookswheredescriptioncontain ANY of the wordslittleorbig, use the|symbol:

```javascript
1select2  *3from4  books5where6  to_tsvector(description)7  @@ to_tsquery('little | big'); -- use | for OR in the search query
```

Notice how searching forbigincludes results with the wordbigger(orbiggest, etc).


### Partial search#
Partial search is particularly useful when you want to find matches on substrings within your data.


### Implementing partial search#
You can use the:*syntax withto_tsquery(). Here's an example that searches for any book titles beginning with "Lit":

```javascript
1select title from books where to_tsvector(title) @@ to_tsquery('Lit:*');
```


### Extending functionality with RPC#
To make the partial search functionality accessible through the API, you can wrap the search logic in a stored procedure.

After creating this function, you can invoke it from your application using the SDK for your platform. Here's an example:

```javascript
1create or replace function search_books_by_title_prefix(prefix text)2returns setof books AS $$3begin4  return query5  select * from books where to_tsvector('english', title) @@ to_tsquery(prefix || ':*');6end;7$$ language plpgsql;
```

This function takes a prefix parameter and returns all books where the title contains a word starting with that prefix. The:*operator is used to denote a prefix match in theto_tsquery()function.


### Handling spaces in queries#
When you want the search term to include a phrase or multiple words, you can concatenate words using a+as a placeholder for space:

```javascript
1select * from search_books_by_title_prefix('Little+Puppy');
```


### Web search syntax withwebsearch_to_tsquery()#
Thewebsearch_to_tsquery()function provides an intuitive search syntax similar to popular web search engines, making it ideal for user-facing search interfaces.


### Basic usage#
```javascript
1select *2from books3where to_tsvector(description) @@ websearch_to_tsquery('english', 'green eggs');
```


### Quoted phrases#
Use quotes to search for exact phrases:

```javascript
1select * from books2where to_tsvector(description || ' ' || title) @@ websearch_to_tsquery('english', '"Green Eggs"');3-- Matches documents containing "Green" immediately followed by "Eggs"
```


### OR searches#
Use "or" (case-insensitive) to search for multiple terms:

```javascript
1select * from books2where to_tsvector(description) @@ websearch_to_tsquery('english', 'puppy or rabbit');3-- Matches documents containing either "puppy" OR "rabbit"
```


### Negation#
Use a dash (-) to exclude terms:

```javascript
1select * from books2where to_tsvector(description) @@ websearch_to_tsquery('english', 'animal -rabbit');3-- Matches documents containing "animal" but NOT "rabbit"
```


### Complex queries#
Combine multiple operators for sophisticated searches:

```javascript
1select * from books2where to_tsvector(description || ' ' || title) @@3  websearch_to_tsquery('english', '"Harry Potter" or "Dr. Seuss" -vegetables');4-- Matches books by "Harry Potter" or "Dr. Seuss" but excludes those mentioning vegetables
```


### Creating indexes#
Now that you have Full Text Search working, create anindex. This allows Postgres to "build" the documents preemptively so that they
don't need to be created at the time we execute the query. This will make our queries much faster.


### Searchable columns#
Let's create a new columnftsinside thebookstable to store the searchable index of thetitleanddescriptioncolumns.

We can use a special feature of Postgres calledGenerated Columnsto ensure that the index is updated any time the values in thetitleanddescriptioncolumns change.

```javascript
1alter table2  books3add column4  fts tsvector generated always as (to_tsvector('english', description || ' ' || title)) stored;56create index books_fts on books using gin (fts); -- generate the index78select id, fts9from books;
```


### Search using the new column#
Now that we've created and populated our index, we can search it using the same techniques as before:

```javascript
1select2  *3from4  books5where6  fts @@ to_tsquery('little & big');
```


### Query operators#
VisitPostgres: Text Search Functions and Operatorsto learn about additional query operators you can use to do more advancedfull text queries, such as:


### Proximity:<->#
The proximity symbol is useful for searching for terms that are a certain "distance" apart.
For example, to find the phrasebig dreams, where the a match for "big" is followed immediately by a match for "dreams":

```javascript
1select2  *3from4  books5where6  to_tsvector(description) @@ to_tsquery('big <-> dreams');
```

We can also use the<->to find words within a certain distance of each other. For example to findyearandschoolwithin 2 words of each other:

```javascript
1select2  *3from4  books5where6  to_tsvector(description) @@ to_tsquery('year <2> school');
```


### Negation:!#
The negation symbol can be used to find phrases whichdon'tcontain a search term.
For example, to find records that have the wordbigbut notlittle:

```javascript
1select2  *3from4  books5where6  to_tsvector(description) @@ to_tsquery('big & !little');
```


### Ranking search results#
Postgres provides ranking functions to sort search results by relevance, helping you present the most relevant matches first. Since ranking functions need to be computed server-side, use RPC functions and generated columns.


### Creating a search function with ranking#
First, create a Postgres function that handles search and ranking:

```javascript
1create or replace function search_books(search_query text)2returns table(id int, title text, description text, rank real) as $$3begin4  return query5  select6    books.id,7    books.title,8    books.description,9    ts_rank(to_tsvector('english', books.description), to_tsquery(search_query)) as rank10  from books11  where to_tsvector('english', books.description) @@ to_tsquery(search_query)12  order by rank desc;13end;14$$ language plpgsql;
```

Now you can call this function from your client:

```javascript
1const { data, error } = await supabase.rpc('search_books', { search_query: 'big' })
```


### Ranking with weighted columns#
Postgres allows you to assign different importance levels to different parts of your documents using weight labels. This is especially useful when you want matches in certain fields (like titles) to rank higher than matches in other fields (like descriptions).


### Understanding weight labels#
Postgres uses four weight labels:A,B,C, andD, where:

- A= Highest importance (weight 1.0)
- B= High importance (weight 0.4)
- C= Medium importance (weight 0.2)
- D= Low importance (weight 0.1)


### Creating weighted search columns#
First, create a weighted tsvector column that gives titles higher priority than descriptions:

```javascript
1-- Add a weighted fts column2alter table books3add column fts_weighted tsvector4generated always as (5  setweight(to_tsvector('english', title), 'A') ||6  setweight(to_tsvector('english', description), 'B')7) stored;89-- Create index for the weighted column10create index books_fts_weighted on books using gin (fts_weighted);
```

Now create a search function that uses this weighted column:

```javascript
1create or replace function search_books_weighted(search_query text)2returns table(id int, title text, description text, rank real) as $$3begin4  return query5  select6    books.id,7    books.title,8    books.description,9    ts_rank(books.fts_weighted, to_tsquery(search_query)) as rank10  from books11  where books.fts_weighted @@ to_tsquery(search_query)12  order by rank desc;13end;14$$ language plpgsql;
```


### Custom weight arrays#
You can also specify custom weights by providing a weight array tots_rank():

```javascript
1create or replace function search_books_custom_weights(search_query text)2returns table(id int, title text, description text, rank real) as $$3begin4  return query5  select6    books.id,7    books.title,8    books.description,9    ts_rank(10      '{0.0, 0.2, 0.5, 1.0}'::real[], -- Custom weights {D, C, B, A}11      books.fts_weighted,12      to_tsquery(search_query)13    ) as rank14  from books15  where books.fts_weighted @@ to_tsquery(search_query)16  order by rank desc;17end;18$$ language plpgsql;
```

This example uses custom weights where:

- A-labeled terms (titles) have maximum weight (1.0)
- B-labeled terms (descriptions) have medium weight (0.5)
- C-labeled terms have low weight (0.2)
- D-labeled terms are ignored (0.0)


### Using the weighted search#
```javascript
1// Search with standard weighted ranking2const { data, error } = await supabase.rpc('search_books_weighted', { search_query: 'Harry' })34// Search with custom weights5const { data: customData, error: customError } = await supabase.rpc('search_books_custom_weights', {6  search_query: 'Harry',7})
```


### Practical example with results#
Say you search for "Harry". With weighted columns:

This ensures that books with "Harry" in the title ranks significantly higher than books that only mention "Harry" in the description, providing more relevant search results for users.


### Using ranking with indexes#
When using theftscolumn you created earlier, ranking becomes more efficient. Create a function that uses the indexed column:

```javascript
1create or replace function search_books_fts(search_query text)2returns table(id int, title text, description text, rank real) as $$3begin4  return query5  select6    books.id,7    books.title,8    books.description,9    ts_rank(books.fts, to_tsquery(search_query)) as rank10  from books11  where books.fts @@ to_tsquery(search_query)12  order by rank desc;13end;14$$ language plpgsql;
```

```javascript
1const { data, error } = await supabase.rpc('search_books_fts', { search_query: 'little & big' })
```


### Using web search syntax with ranking#
You can also create a function that combineswebsearch_to_tsquery()with ranking for user-friendly search:

```javascript
1create or replace function websearch_books(search_text text)2returns table(id int, title text, description text, rank real) as $$3begin4  return query5  select6    books.id,7    books.title,8    books.description,9    ts_rank(books.fts, websearch_to_tsquery('english', search_text)) as rank10  from books11  where books.fts @@ websearch_to_tsquery('english', search_text)12  order by rank desc;13end;14$$ language plpgsql;
```

```javascript
1// Support natural search syntax2const { data, error } = await supabase.rpc('websearch_books', {3  search_text: '"little puppy" or train -vegetables',4})
```


### Resources#
- Postgres: Text Search Functions and Operators


================================================================================


# Database Functions
Source: https://supabase.com/docs/guides/database/functions

Database Functions

Postgres has built-in support forSQL functions.
These functions live inside your database, and they can beused with the API.


### Quick demo#

### Getting started#
Supabase provides several options for creating database functions. You can use the Dashboard or create them directly using SQL.
We provide a SQL editor within the Dashboard, or you canconnectto your database
and run the SQL queries yourself.


### Simple functions#
Let's create a basic Database Function which returns a string "hello world".

```javascript
1create or replace function hello_world() -- 12returns text -- 23language sql -- 34as $$  -- 45  select 'hello world';  -- 56$$; --6
```

At it's most basic a function has the following parts:

When naming your functions, make the name of the function unique as overloaded functions are not supported.

After the Function is created, we have several ways of "executing" the function - either directly inside the database using SQL, or with one of the client libraries.

```javascript
1select hello_world();
```


### Returning data sets#
Database Functions can also return data sets fromTablesor Views.

For example, if we had a database with some Star Wars data inside:


### Planets
```javascript
1| id  | name     |2| --- | -------- |3| 1   | Tatooine |4| 2   | Alderaan |5| 3   | Kashyyyk |
```


### People
```javascript
1| id  | name             | planet_id |2| --- | ---------------- | --------- |3| 1   | Anakin Skywalker | 1         |4| 2   | Luke Skywalker   | 1         |5| 3   | Princess Leia    | 2         |6| 4   | Chewbacca        | 3         |
```

We could create a function which returns all the planets:

```javascript
1create or replace function get_planets()2returns setof planets3language sql4as $$5  select * from planets;6$$;
```

Because this function returns a table set, we can also apply filters and selectors. For example, if we only wanted the first planet:

```javascript
1select *2from get_planets()3where id = 1;
```


### Passing parameters#
Let's create a Function to insert a new planet into theplanetstable and return the new ID. Note that this time we're using theplpgsqllanguage.

```javascript
1create or replace function add_planet(name text)2returns bigint3language plpgsql4as $$5declare6  new_row bigint;7begin8  insert into planets(name)9  values (add_planet.name)10  returning id into new_row;1112  return new_row;13end;14$$;
```

Once again, you can execute this function either inside your database using aselectquery, or with the client libraries:

```javascript
1select * from add_planet('Jakku');
```


### Suggestions#

### Database Functions vs Edge Functions#
For data-intensive operations, use Database Functions, which are executed within your database
and can be called remotely using theREST and GraphQL API.

For use-cases which require low-latency, useEdge Functions, which are globally-distributed and can be written in Typescript.


### Securitydefinervsinvoker#
Postgres allows you to specify whether you want the function to be executed as the usercallingthe function (invoker), or as thecreatorof the function (definer). For example:

```javascript
1create function hello_world()2returns text3language plpgsql4security definer set search_path = ''5as $$6begin7  select 'hello world';8end;9$$;
```

It is best practice to usesecurity invoker(which is also the default). If you ever usesecurity definer, youmustset thesearch_path.
If you use an empty search path (search_path = ''), you must explicitly state the schema for every relation in the function body (e.g.from public.table).
This limits the potential damage if you allow access to schemas which the user executing the function should not have.


### Function privileges#
By default, database functions can be executed by any role. There are two main ways to restrict this:

On a case-by-case basis. Specifically revoke permissions for functions you want to protect. Execution needs to be revoked for bothpublicand the role you're restricting:

```javascript
1revoke execute on function public.hello_world from public;2revoke execute on function public.hello_world from anon;
```

Restrict function execution by default. Specificallygrantaccess when you want a function to be executable by a specific role.

To restrict all existing functions, revoke execution permissions from bothpublicandthe role you want to restrict:

```javascript
1revoke execute on all functions in schema public from public;2revoke execute on all functions in schema public from anon, authenticated;
```

To restrict all new functions, change the default privileges for bothpublicandthe role you want to restrict:

```javascript
1alter default privileges in schema public revoke execute on functions from public;2alter default privileges in schema public revoke execute on functions from anon, authenticated;
```

You can then regrant permissions for a specific function to a specific role:

```javascript
1grant execute on function public.hello_world to authenticated;
```


### Debugging functions#
You can add logs to help you debug functions. This is especially recommended for complex functions.

Good targets to log include:

- Values of (non-sensitive) variables
- Returned results from queries


### General logging#
To create custom logs in theDashboard's Postgres Logs, you can use theraisekeyword. By default, there are 3 observed severity levels:

- log
- warning
- exception(error level)

```javascript
1create function logging_example(2  log_message text,3  warning_message text,4  error_message text5)6returns void7language plpgsql8as $$9begin10  raise log 'logging message: %', log_message;11  raise warning 'logging warning: %', warning_message;1213  -- immediately ends function and reverts transaction14  raise exception 'logging error: %', error_message;15end;16$$;1718select logging_example('LOGGED MESSAGE', 'WARNING MESSAGE', 'ERROR MESSAGE');
```


### Error handling#
You can create custom errors with theraise exceptionkeywords.

A common pattern is to throw an error when a variable doesn't meet a condition:

```javascript
1create or replace function error_if_null(some_val text)2returns text3language plpgsql4as $$5begin6  -- error if some_val is null7  if some_val is null then8    raise exception 'some_val should not be NULL';9  end if;10  -- return some_val if it is not null11  return some_val;12end;13$$;1415select error_if_null(null);
```

Value checking is common, so Postgres provides a shorthand: theassertkeyword. It uses the following format:

```javascript
1-- throw error when condition is false2assert <some condition>, 'message';
```

Below is an example

```javascript
1create function assert_example(name text)2returns uuid3language plpgsql4as $$5declare6  student_id uuid;7begin8  -- save a user's id into the user_id variable9  select10    id into student_id11  from attendance_table12  where student = name;1314  -- throw an error if the student_id is null15  assert student_id is not null, 'assert_example() ERROR: student not found';1617  -- otherwise, return the user's id18  return student_id;19end;20$$;2122select assert_example('Harry Potter');
```

Error messages can also be captured and modified with theexceptionkeyword:

```javascript
1create function error_example()2returns void3language plpgsql4as $$5begin6  -- fails: cannot read from nonexistent table7  select * from table_that_does_not_exist;89  exception10      when others then11          raise exception 'An error occurred in function <function name>: %', sqlerrm;12end;13$$;
```


### Advanced logging#
For more complex functions or complicated debugging, try logging:

- Formatted variables
- Individual rows
- Start and end of function calls

```javascript
1create or replace function advanced_example(num int default 10)2returns text3language plpgsql4as $$5declare6    var1 int := 20;7    var2 text;8begin9    -- Logging start of function10    raise log 'logging start of function call: (%)', (select now());1112    -- Logging a variable from a SELECT query13    select14      col_1 into var115    from some_table16    limit 1;17    raise log 'logging a variable (%)', var1;1819    -- It is also possible to avoid using variables, by returning the values of your query to the log20    raise log 'logging a query with a single return value(%)', (select col_1 from some_table limit 1);2122    -- If necessary, you can even log an entire row as JSON23    raise log 'logging an entire row as JSON (%)', (select to_jsonb(some_table.*) from some_table limit 1);2425    -- When using INSERT or UPDATE, the new value(s) can be returned26    -- into a variable.27    -- When using DELETE, the deleted value(s) can be returned.28    -- All three operations use "RETURNING value(s) INTO variable(s)" syntax29    insert into some_table (col_2)30    values ('new val')31    returning col_2 into var2;3233    raise log 'logging a value from an INSERT (%)', var2;3435    return var1 || ',' || var2;36exception37    -- Handle exceptions here if needed38    when others then39        raise exception 'An error occurred in function <advanced_example>: %', sqlerrm;40end;41$$;4243select advanced_example();
```


### Resources#
- Official Client libraries:JavaScriptandFlutter
- Community client libraries:github.com/supabase-community
- Postgres Official Docs:Chapter 9. Functions and Operators
- Postgres Reference:CREATE FUNCTION


### Deep dive#

### Create Database Functions#

### Call Database Functions using JavaScript#

### Using Database Functions to call an external API#

================================================================================


# Hardening the Data API
Source: https://supabase.com/docs/guides/database/hardening-data-api

Hardening the Data API

Your database's auto-generated Data API exposes thepublicschema by default. You can change this to any schema in your database, or even disable the Data API completely.

Any tables that are accessible through the Data APImusthaveRow Level Securityenabled. Row Level Security (RLS) is enabled by default when you create tables from the Supabase Dashboard. If you create a table using the SQL editor or your own SQL client or migration runner, youmustenable RLS yourself.


### Shared responsibility#
Your application's security is your responsibility as a developer. This includes RLS, falling under theShared Responsibilitymodel. To help you:

- Supabase sends daily emails warning of any tables that are exposed to the Data API which do not have RLS enabled.
- Supabase provides a Security Advisor and other tools in the Supabase Dashboard to fix any issues.


### Private schemas#
We highly recommend creating aprivateschema for storing tables that you do not want to expose via the Data API. These tables can be accessed via Supabase Edge Functions or any other serverside tool. In this model, you should implement your security model in your serverside code. Although it's not required, westillrecommend enabling RLS for private tables and then connecting to your database using a Postgres role withbypassrlsprivileges.


### Managing the public schema#
If yourpublicschema is used by other tools as a default space, you might want to lock down this schema. This helps prevent accidental exposure of data that's automatically added topublic.

There are several levels of security hardening for the Data API:

- Disabling the Data API entirely. This is recommended if youneverneed to access your database via Supabase client libraries or the REST and GraphQL endpoints.
- Exposing a custom schemainstead ofpublic, giving you explicit control over what is accessible.
- Automatically enabling RLS on new tablesusing an event trigger.
- Adjusting table-level grantsto control which roles can access specific tables.


### Disabling the Data API#
You can disable the Data API entirely if you never intend to use the Supabase client libraries or the REST and GraphQL data endpoints. For example, if you only access your database via a direct connection on the server, disabling the Data API gives you the greatest layer of protection.


### Exposing a custom schema instead ofpublic#
If you want to use the Data API but with increased security, you can expose a custom schema instead ofpublic. By not usingpublic, which is often used as a default space and has laxer default permissions, you get more conscious control over your exposed data.

Any data, views, or functions that should be exposed need to be deliberately put within your custom schema (which we will callapi), rather than ending up there by default.


### Step 1: Removepublicfrom exposed schemas#

### Step 2: Create anapischema and expose it#
Connect to your database. You can usepsql, theSupabase SQL Editor, or the Postgres client of your choice.

Create a new schema namedapi:

```javascript
1create schema if not exists api;
```

Grant theanonandauthenticatedroles usage on this schema.

```javascript
1grant usage on schema api to anon, authenticated;
```

Go toAPI Settingsin the Supabase Dashboard.

UnderData API Settings, addapitoExposed schemas. Make sure it is the first schema in the list, so that it will be searched first by default.

Under these new settings,anonandauthenticatedcan execute functions defined in theapischema, but they have no automatic permissions on any tables. On a table-by-table basis, you can grant them permissions. For example:

```javascript
1grant select on table api.<your_table> to anon;2grant select, insert, update, delete on table api.<your_table> to authenticated;
```


### Automatically enabling RLS on new tables#
Tables created via the Supabase Dashboard have RLS enabled by default. However, if you or your team create tables using the SQL editor, migrations, or an external tool, RLS will not be enabled automatically.

You can use anevent triggerto automatically enable RLS whenever a new table is created in thepublicschema. This ensures that no table is accidentally left exposed without RLS protection.


### Table-level grants#
By default, tables in thepublicschema are granted full access (SELECT,INSERT,UPDATE,DELETE) to theanonandauthenticatedroles. This allows the Data API to query those tables on behalf of users.

You can adjust these privileges on a per-table basis to restrict which operations each role can perform. For example, you might want to:

- Allowanonusers to onlySELECTfrom a table, preventing anonymous writes.
- Preventanonusers from accessing a table entirely, making it available only to authenticated users.
- Restrictauthenticatedusers toSELECTandINSERTonly, preventing updates and deletes.

Table-level privileges work alongsideRow Level Security. Privileges controlwhich operationsare possible, while RLS policies controlwhich rowsare accessible. For full protection, use both: restrict privileges to limit operation types, and use RLS policies to control row-level access.


### Adjusting table-level grants via the Dashboard#
Adjusting table-level privileges via the Dashboard is currently in beta and will be available via gradual roll-out.


### Adjusting table-level grants via SQL#
You can also adjust privileges using SQL. For example, to allow onlySELECTaccess foranonon a table:

```javascript
1-- Revoke all existing privileges2revoke all on table public.your_table from anon;34-- Grant only SELECT5grant select on table public.your_table to anon;
```

To remove all access foranonfrom a table:

```javascript
1revoke all on table public.your_table from anon;
```

To restore full access:

```javascript
1grant select, insert, update, delete on table public.your_table to anon;
```


================================================================================


# Import data into Supabase
Source: https://supabase.com/docs/guides/database/import-data

Import data into Supabase

You can import data into Supabase in multiple ways. The best method depends on your data size and app requirements.

If you're working with small datasets in development, you can experiment quickly using CSV import in the Supabase dashboard. If you're working with a large dataset in production, you should plan your data import to minimize app latency and ensure data integrity.


### How to import data into Supabase#
You have multiple options for importing your data into Supabase:

If you're importing a large dataset or importing data into production, plan ahead andprepare your database.


### Option 1: CSV import via Supabase dashboard#
Supabase dashboard provides a user-friendly way to import data. However, for very large datasets, this method may not be the most efficient choice, given the size limit is 100MB. It's generally better suited for smaller datasets and quick data imports. Consider using alternative methods like pgloader for large-scale data imports.


### Option 2: Bulk import using pgloader#
pgloaderis a powerful tool for efficiently importing data into a Postgres database that supports a wide range of source database engines, including MySQL and MS SQL.

You can use it in conjunction with Supabase by following these steps:

Install pgloader on your local machine or a server. For more info, you can refer to theofficial pgloader installation page.

```javascript
1$ apt-get install pgloader
```

Create a configuration file that specifies the source data and the target Supabase database (e.g., config.load).
Here's an example configuration file:

```javascript
1LOAD DATABASE2    FROM sourcedb://USER:PASSWORD@HOST/SOURCE_DB3    INTO postgres://postgres.xxxx:password@xxxx.pooler.supabase.com:6543/postgres4ALTER SCHEMA 'public' OWNER TO 'postgres';5set wal_buffers = '64MB', max_wal_senders = 0, statement_timeout = 0, work_mem to '2GB';
```

Customize the source and Supabase database URL and options to fit your specific use case:

- wal_buffers: This parameter is set to '64MB' to allocate 64 megabytes of memory for write-ahead logging buffers. A larger value can help improve write performance by caching more data in memory before writing it to disk. This can be useful during data import operations to speed up the writing of transaction logs.
- max_wal_senders: It is set to 0, to disable replication connections. This is done during the data import process to prevent replication-related conflicts and issues.
- statement_timeout: The value is set to 0, which means it's disabled, allowing SQL statements to run without a time limit.
- work_mem: It is set to '2GB', allocating 2 GB of memory for query operations. This enhances the performance of complex queries by allowing larger in-memory datasets.

Run pgloader with the configuration file.

```javascript
1pgloader config.load
```

For databases using the Postgres engine, we recommend using thepg_dumpandpsqlcommand line tools.


### Option 3: Using Postgres copy command#
Read more aboutBulk data loading.


### Option 4: Using the Supabase API#
The Supabase API allows you to programmatically import data into your tables. You can use various client libraries to interact with the API and perform data import operations. This approach is useful when you need to automate data imports, and it gives you fine-grained control over the process. Refer to ourAPI guidefor more details.

When importing data via the Supabase API, it's advisable to refrain from bulk imports. This helps ensure a smooth data transfer process and prevents any potential disruptions.

Read more aboutRate Limiting, Resource Allocation, & Abuse Prevention.


### Preparing to import data#
Large data imports can affect your database performance. Failed imports can also cause data corruption. Importing data is a safe and common operation, but you should plan ahead if you're importing a lot of data, or if you're working in a production environment.


### 1. Back up your data#
Backups help you restore your data if something goes wrong. Databases on Pro, Team and Enterprise Plans are automatically backed up on schedule, but you can also take your own backup. SeeDatabase Backupsfor more information.


### 2. Increase statement timeouts#
By default, Supabase enforces query statement timeouts to ensure fair resource allocation and prevent long-running queries from affecting the overall system. When importing large datasets, you may encounter timeouts. To address this:

- Increase the Statement Timeout: You can adjust the statement timeout for your session or connection to accommodate longer-running queries. Be cautious when doing this, as excessively long queries can negatively impact system performance. Read more aboutStatement Timeouts.


### 3. Estimate your required disk size#
Large datasets consume disk space. Ensure your Supabase project has sufficient disk capacity to accommodate the imported data. If you know how big your database is going to be, you can manually increase the size in yourprojects database settings.

Read more aboutdisk management.


### 4. Disable triggers#
When importing large datasets, it's often beneficial to disable triggers temporarily. Triggers can significantly slow down the import process, especially if they involve complex logic or referential integrity checks. After the import, you can re-enable the triggers.

To disable triggers, use the following SQL commands:

```javascript
1-- Disable triggers on a specific table2ALTER TABLE table_name DISABLE TRIGGER ALL;34-- To re-enable triggers5ALTER TABLE table_name ENABLE TRIGGER ALL;
```


### 5. Rebuild indices after data import is complete#
Indexing is crucial for query performance, but building indices while importing a large dataset can be time-consuming. Consider building or rebuilding indices after the data import is complete. This approach can significantly speed up the import process and reduce the overall time required.

To build an index after the data import:

```javascript
1-- Create an index on a table2create index index_name on table_name (column_name);
```

Read more aboutManaging Indexes in Postgres.


================================================================================


# Debugging and monitoring
Source: https://supabase.com/docs/guides/database/inspect

Debugging and monitoring

Database performance is a large topic and many factors can contribute. Some of the most common causes of poor performance include:

- An inefficiently designed schema
- Inefficiently designed queries
- A lack of indexes causing slower than required queries over large tables
- Unused indexes causing slowINSERT,UPDATEandDELETEoperations
- Not enough compute resources, such as memory, causing your database to go to disk for results too often
- Lock contention from multiple queries operating on highly utilized tables
- Large amount of bloat on your tables causing poor query planning

You can examine your database and queries for these issues using either theSupabase CLIor SQL.


### Using the CLI#
The Supabase CLI comes with a range of tools to help inspect your Postgres instances for potential issues. The CLI gets the information fromPostgres internals. Therefore, most tools provided are compatible with any Postgres databases regardless if they are a Supabase project or not.

You can find installation instructions for the Supabase CLIhere.


### Theinspect dbcommand#
The inspection tools for your Postgres database are under theinspect dbcommand. You can get a full list of available commands by runningsupabase inspect db help.

```javascript
1$ supabase inspect db help2Tools to inspect your Supabase database34Usage:5  supabase inspect db [command]67Available Commands:8  bloat                Estimates space allocated to a relation that is full of dead tuples9  blocking             Show queries that are holding locks and the queries that are waiting for them to be released10  cache-hit            Show cache hit rates for tables and indices1112...
```


### Connect to any Postgres database#
Most inspection commands are Postgres agnostic. You can run inspection routines on any Postgres database even if it is not a Supabase project by providing a connection string via--db-url.

For example you can connect to your local Postgres instance:

```javascript
1supabase --db-url postgresql://postgres:postgres@localhost:5432/postgres inspect db bloat
```


### Connect to a Supabase instance#
Working with Supabase, you can link the Supabase CLI with your project:

```javascript
1supabase link --project-ref <project-id>
```

Then the CLI will automatically connect to your Supabase project whenever you are in the project folder and you no longer need to provide—db-url.


### Inspection commands#
Below are thedbinspection commands provided, grouped by different use cases.

Some commands might requirepg_stat_statementsto be enabled or a specific Postgres version to be used.


### Disk storage#
These commands are handy if you are running low on disk storage:

- bloat- estimates the amount of wasted space
- vacuum-stats- gives information on waste collection routines
- table-record-counts- estimates the number of records per table
- table-sizes- shows the sizes of tables
- index-sizes- shows the sizes of individual index
- table-index-sizes- shows the sizes of indexes for each table


### Query performance#
The commands below are useful if your Postgres database consumes a lot of resources like CPU, RAM or Disk IO. You can also use them to investigate slow queries.

- cache-hit- shows how efficient your cache usage is overall
- unused-indexes- shows indexes with low index scans
- index-usage- shows information about the efficiency of indexes
- seq-scans- show number of sequential scans recorded against all tables
- long-running-queries- shows long running queries that are executing right now
- outliers- shows queries with high execution time but low call count and queries with high proportion of execution time spent on synchronous I/O


### Locks#
- locks- shows statements which have taken out an exclusive lock on a relation
- blocking- shows statements that are waiting for locks to be released


### Connections#
- role-connections- shows number of active connections for all database roles (Supabase-specific command)
- replication-slots- shows information about replication slots on the database


### Notes onpg_stat_statements#
Following commands requirepg_stat_statementsto be enabled: calls, locks, cache-hit, blocking, unused-indexes, index-usage, bloat, outliers, table-record-counts, replication-slots, seq-scans, vacuum-stats, long-running-queries.

When usingpg_stat_statementsalso take note that it only stores the latest 5,000 statements. Moreover, consider resetting the analysis after optimizing any queries by runningselect pg_stat_statements_reset();

Learn more about pg_statshere.


### Using SQL#
If you're seeing aninsufficient privilegeerror when viewing the Query Performance page from the dashboard, run this command:

```javascript
1$ grant pg_read_all_stats to postgres;
```


### Postgres cumulative statistics system#
Postgres collects data about its own operations using thecumulative statistics system. In addition to this, every Supabase project has thepg_stat_statements extensionenabled by default. This extension records query execution performance details and is the best way to find inefficient queries. This information can be combined with the Postgres query plan analyzer to develop more efficient queries.

Here are some example queries to get you started.


### Most frequently called queries#
```javascript
1select2  auth.rolname,3  statements.query,4  statements.calls,5  -- -- Postgres 13, 14, 156  statements.total_exec_time + statements.total_plan_time as total_time,7  statements.min_exec_time + statements.min_plan_time as min_time,8  statements.max_exec_time + statements.max_plan_time as max_time,9  statements.mean_exec_time + statements.mean_plan_time as mean_time,10  -- -- Postgres <= 1211  -- total_time,12  -- min_time,13  -- max_time,14  -- mean_time,15  statements.rows / statements.calls as avg_rows16from17  pg_stat_statements as statements18  inner join pg_authid as auth on statements.userid = auth.oid19order by statements.calls desc20limit 100;
```

This query shows:

- query statistics, ordered by the number of times each query has been executed
- the role that ran the query
- the number of times it has been called
- the average number of rows returned
- the cumulative total time the query has spent running
- the min, max and mean query times.

This provides useful information about the queries you run most frequently. Queries that have highmax_timeormean_timetimes and are being called often can be good candidates for optimization.


### Slowest queries by execution time#
```javascript
1select2  auth.rolname,3  statements.query,4  statements.calls,5  -- -- Postgres 13, 14, 156  statements.total_exec_time + statements.total_plan_time as total_time,7  statements.min_exec_time + statements.min_plan_time as min_time,8  statements.max_exec_time + statements.max_plan_time as max_time,9  statements.mean_exec_time + statements.mean_plan_time as mean_time,10  -- -- Postgres <= 1211  -- total_time,12  -- min_time,13  -- max_time,14  -- mean_time,15  statements.rows / statements.calls as avg_rows16from17  pg_stat_statements as statements18  inner join pg_authid as auth on statements.userid = auth.oid19order by max_time desc20limit 100;
```

This query will show you statistics about queries ordered by the maximum execution time. It is similar to the query above ordered by calls, but this one highlights outliers that may have high executions times. Queries which have high or mean execution times are good candidates for optimization.


### Most time consuming queries#
```javascript
1select2  auth.rolname,3  statements.query,4  statements.calls,5  statements.total_exec_time + statements.total_plan_time as total_time,6  to_char(7    (8      (statements.total_exec_time + statements.total_plan_time) / sum(9        statements.total_exec_time + statements.total_plan_time10      ) over ()11    ) * 100,12    'FM90D0'13  ) || '%' as prop_total_time14from15  pg_stat_statements as statements16  inner join pg_authid as auth on statements.userid = auth.oid17order by total_time desc18limit 100;
```

This query will show you statistics about queries ordered by the cumulative total execution time. It shows the total time the query has spent running as well as the proportion of total execution time the query has taken up.

Queries which are the most time consuming are not necessarily bad, you may have a very efficient and frequently ran queries that end up taking a large total % time, but it can be useful to help spot queries that are taking up more time than they should.


### Hit rate#
Generally for most applications a small percentage of data is accessed more regularly than the rest. To make sure that your regularly accessed data is available, Postgres tracks your data access patterns and keeps this in itsshared_bufferscache.

Applications with lower cache hit rates generally perform more poorly since they have to hit the disk to get results rather than serving them from memory. Very poor hit rates can also cause you to burst past yourDisk IO limitscausing significant performance issues.

You can view your cache and index hit rate by executing the following query:

```javascript
1select2  'index hit rate' as name,3  (sum(idx_blks_hit)) / nullif(sum(idx_blks_hit + idx_blks_read), 0) * 100 as ratio4from pg_statio_user_indexes5union all6select7  'table hit rate' as name,8  sum(heap_blks_hit) / nullif(sum(heap_blks_hit) + sum(heap_blks_read), 0) * 100 as ratio9from pg_statio_user_tables;
```

This shows the ratio of data blocks fetched from the Postgresshared_bufferscache against the data blocks that were read from disk/OS cache.

If either of your index or table hit rate are < 99% then this can indicate your compute plan is too small for your current workload and you would benefit from more memory.Upgrading your computeis easy and can be done from yourproject dashboard.


### Optimizing poor performing queries#
Postgres has built in tooling to help you optimize poorly performing queries. You can use thequery plan analyzeron any expensive queries that you have identified:

```javascript
1explain analyze <query-statement-here>;
```

When you includeanalyzein the explain statement, the database attempts to execute the query and provides a detailed query plan along with actual execution times. So, be careful usingexplain analyzewithinsert/update/deletequeries, because the query will actually run, and could have unintended side-effects.

If you run justexplainwithout theanalyzekeyword, the database will only perform query planning without actually executing the query. This approach can be beneficial when you want to inspect the query plan without affecting the database or if you encounter timeouts in your queries.

Using the query plan analyzer to optimize your queries is a large topic, with a number of online resources available:

- Official docs.
- The Art of PostgreSQL.
- Postgres Wiki.
- Enterprise DB.

You can pair the information available frompg_stat_statementswith the detailed system metrics availablevia your metrics endpointto better understand the behavior of your DB and the queries you're executing against it.


================================================================================


# Querying Joins and Nested tables
Source: https://supabase.com/docs/guides/database/joins-and-nesting

Querying Joins and Nested tables

The data APIs automatically detect relationships between Postgres tables. Since Postgres is a relational database, this is a very common scenario.


### One-to-many joins#
Let's use an example database that storesorchestral_sectionsandinstruments:

Orchestral sections

Instruments

The APIs will automatically detect relationships based on the foreign keys:

```javascript
1const { data, error } = await supabase.from('orchestral_sections').select(`2  id,3  name,4  instruments ( id, name )5`)
```


### TypeScript types for joins#
supabase-jsalways returns adataobject (for success), and anerrorobject (for unsuccessful requests).

These helper types provide the result types from any query, including nested types for database joins.

Given the following schema with a relation between orchestral sections and instruments:

```javascript
1create table orchestral_sections (2  "id" serial primary key,3  "name" text4);56create table instruments (7  "id" serial primary key,8  "name" text,9  "section_id" int references "orchestral_sections"10);
```

We can get the nestedSectionsWithInstrumentstype like this:

```javascript
1import { QueryResult, QueryData, QueryError } from '@supabase/supabase-js'23const sectionsWithInstrumentsQuery = supabase.from('orchestral_sections').select(`4  id,5  name,6  instruments (7    id,8    name9  )10`)11type SectionsWithInstruments = QueryData<typeof sectionsWithInstrumentsQuery>1213const { data, error } = await sectionsWithInstrumentsQuery14if (error) throw error15const sectionsWithInstruments: SectionsWithInstruments = data
```


### Many-to-many joins#
The data APIs will detect many-to-many joins. For example, if you have a database which stored teams of users (where each user could belong to many teams):

```javascript
1create table users (2  "id" serial primary key,3  "name" text4);56create table teams (7  "id" serial primary key,8  "team_name" text9);1011create table members (12  "user_id" int references users,13  "team_id" int references teams,14  primary key (user_id, team_id)15);
```

In these cases you don't need to explicitly define the joining table (members). If we wanted to fetch all the teams and the members in each team:

```javascript
1const { data, error } = await supabase.from('teams').select(`2  id,3  team_name,4  users ( id, name )5`)
```


### Specifying theONclause for joins with multiple foreign keys#
For example, if you have a project that tracks when employees check in and out of work shifts:

```javascript
1-- Employees2create table users (3  "id" serial primary key,4  "name" text5);67-- Badge scans8create table scans (9  "id" serial primary key,10  "user_id" int references users,11  "badge_scan_time" timestamp12);1314-- Work shifts15create table shifts (16  "id" serial primary key,17  "user_id" int references users,18  "scan_id_start" int references scans, -- clocking in19  "scan_id_end" int references scans, -- clocking out20  "attendance_status" text21);
```

In this case, you need to explicitly define the join because the joining column onshiftsis ambiguous as they are both referencing thescanstable.

To fetch all theshiftswithscan_id_startandscan_id_endrelated to a specificscan, use the following syntax:

```javascript
1const { data, error } = await supabase.from('shifts').select(2  `3    *,4    start_scan:scans!scan_id_start (5      id,6      user_id,7      badge_scan_time8    ),9   end_scan:scans!scan_id_end (10     id,11     user_id,12     badge_scan_time13    )14  `15)
```


================================================================================


# Managing JSON and unstructured data
Source: https://supabase.com/docs/guides/database/json

Managing JSON and unstructured data


### Using the JSON data type in Postgres.
Using the JSON data type in Postgres.

Postgres supports storing and querying unstructured data.


### JSON vs JSONB#
Postgres supports two types of JSON columns:json(stored as a string) andjsonb(stored as a binary). The recommended type isjsonbfor almost all cases.

- jsonstores an exact copy of the input text. Database functions must reparse the content on each execution.
- jsonbstores database in a decomposed binary format. While this makes it slightly slower to input due to added conversion overhead, it is significantly faster to process, since no reparsing is needed.


### When to use JSON/JSONB#
Generally you should use ajsonbcolumn when you have data that is unstructured or has a variable schema. For example, if you wanted to store responses for various webhooks, you might not know the format of the response when creating the table. Instead, you could store thepayloadas ajsonbobject in a single column.

Don't go overboard withjson/jsonbcolumns. They are a useful tool, but most of the benefits of a relational database come from the ability to query and join structured data, and the referential integrity that brings.


### Create JSONB columns#
json/jsonbis just another "data type" for Postgres columns. You can create ajsonbcolumn in the same way you would create atextorintcolumn:

```javascript
1create table books (2  id serial primary key,3  title text,4  author text,5  metadata jsonb6);
```


### Inserting JSON data#
You can insert JSON data in the same way that you insert any other data. The data must be valid JSON.

```javascript
1insert into books2  (title, author, metadata)3values4  (5    'The Poky Little Puppy',6    'Janette Sebring Lowrey',7    '{"description":"Puppy is slower than other, bigger animals.","price":5.95,"ages":[3,6]}'8  ),9  (10    'The Tale of Peter Rabbit',11    'Beatrix Potter',12    '{"description":"Rabbit eats some vegetables.","price":4.49,"ages":[2,5]}'13  ),14  (15    'Tootle',16    'Gertrude Crampton',17    '{"description":"Little toy train has big dreams.","price":3.99,"ages":[2,5]}'18  ),19  (20    'Green Eggs and Ham',21    'Dr. Seuss',22    '{"description":"Sam has changing food preferences and eats unusually colored food.","price":7.49,"ages":[4,8]}'23  ),24  (25    'Harry Potter and the Goblet of Fire',26    'J.K. Rowling',27    '{"description":"Fourth year of school starts, big drama ensues.","price":24.95,"ages":[10,99]}'28  );
```


### Query JSON data#
Querying JSON data is similar to querying other data, with a few other features to access nested values.

Postgres support a range ofJSON functions and operators. For example, the->operator returns values asjsonbdata. If you want the data returned astext, use the->>operator.

```javascript
1select2  title,3  metadata ->> 'description' as description, -- returned as text4  metadata -> 'price' as price,5  metadata -> 'ages' -> 0 as low_age,6  metadata -> 'ages' -> 1 as high_age7from books;
```


### Validating JSON data#
Supabase provides thepg_jsonschemaextensionthat adds the ability to validatejsonandjsonbdata types againstJSON Schemadocuments.

Once you have enabled the extension, you can add a "check constraint" to your table to validate the JSON data:

```javascript
1create table customers (2  id serial primary key,3  metadata json4);56alter table customers7add constraint check_metadata check (8  json_matches_schema(9    '{10        "type": "object",11        "properties": {12            "tags": {13                "type": "array",14                "items": {15                    "type": "string",16                    "maxLength": 1617                }18            }19        }20    }',21    metadata22  )23);
```


### Resources#
- Postgres: JSON Functions and Operators
- Postgres JSON types


================================================================================


# Connecting to Metabase
Source: https://supabase.com/docs/guides/database/metabase

Connecting to Metabase

Metabaseis an Open Source data visualization tool. You can use it to explore your data stored in Supabase.


### Register
Create aMetabase accountor deploy locally withDocker

Deploying with Docker:

```javascript
1docker pull metabase/metabase:latest
```

Then run:

```javascript
1docker run -d -p 3000:3000 --name metabase metabase/metabase
```

The server should be available athttp://localhost:3000/setup


### Connect to Postgres
Connect your Postgres server to Metabase.

- On your project dashboard, clickConnect
- View parameters under "Session pooler"

If you're in anIPv6 environmentor have theIPv4 Add-On, you can use the direct connection string instead of Supavisor in Session mode.

- Enter your database credentials into Metabase

Example credentials:


### Explore
Explore your data in Metabase




================================================================================


# OrioleDB Overview
Source: https://supabase.com/docs/guides/database/orioledb

OrioleDB Overview

TheOrioleDBPostgres extension provides a drop-in replacement storage engine for the default heap storage method. It is designed to improve Postgres' scalability and performance.

OrioleDB addresses PostgreSQL's scalability limitations by removing bottlenecks in the shared memory cache under high concurrency. It also optimizes write-ahead-log (WAL) insertion through row-level WAL logging. These changes lead to significant improvements in the industry standard TPC-C benchmark, which approximates a real-world transactional workload. The following benchmark was performed on a c7g.metal instance and shows OrioleDB's performance outperforming the default Postgres heap method with a 3.3x speedup.

OrioleDB is in active development and currently hascertain limitations. Currently, only B-tree indexes are supported, so features like pg_vector's HNSW indexes are not yet available. An Index Access Method bridge to unlock support for all index types used with heap storage is under active development. In the Supabase OrioleDB image the default storage method has been updated to use OrioleDB, granting better performance out of the box.


### Concepts#

### Index-organized tables#
OrioleDB uses index-organized tables, where table data is stored in the index structure. This design eliminates the need for separate heap storage, reduces overhead and improves lookup performance for primary key queries.


### No buffer mapping#
In-memory pages are connected to the storage pages using direct links. This allows OrioleDB to bypass PostgreSQL's shared buffer pool and eliminate the associated complexity and contention in buffer mapping.


### Undo log#
Multi-Version Concurrency Control (MVCC) is implemented using an undo log. The undo log stores previous row versions and transaction information, which enables consistent reads while removing the need for table vacuuming completely.


### Copy-on-write checkpoints#
OrioleDB implements copy-on-write checkpoints to persist data efficiently. This approach writes only modified data during a checkpoint, reducing the I/O overhead compared to traditional Postgres checkpointing and allowing row-level WAL logging.


### Usage#

### Creating OrioleDB project#
You can get started with OrioleDB by enabling the extension in your Supabase dashboard.
To get started with OrioleDB you need tocreate a new Supabase projectand chooseOrioleDB Public AlphaPostgres version.


### Creating tables#
To create a table using the OrioleDB storage engine just execute the standardCREATE TABLEstatement. By default it will create a table using OrioleDB storage engine. For example:

```javascript
1-- Create a table2create table blog_post (3  id int8 not null,4  title text not null,5  body text not null,6  author text not null,7  published_at timestamptz not null default CURRENT_TIMESTAMP,8  views bigint not null,9  primary key (id)10);
```


### Creating indexes#
OrioleDB tables always have a primary key. If it wasn't defined explicitly, a hidden primary key is created using thectidcolumn.
Additionally you can create secondary indexes.

Currently, only B-tree indexes are supported, so features like pg_vector's HNSW indexes are not yet available.

```javascript
1-- Create an index2create index blog_post_published_at on blog_post (published_at);34create index blog_post_views on blog_post (views) where (views > 1000);
```


### Data manipulation#
You can query and modify data in OrioleDB tables using standard SQL statements, includingSELECT,INSERT,UPDATE,DELETEandINSERT ... ON CONFLICT.

```javascript
1INSERT INTO blog_post (id, title, body, author, views)2VALUES (1, 'Hello, World!', 'This is my first blog post.', 'John Doe', 1000);34SELECT * FROM blog_post ORDER BY published_at DESC LIMIT 10;5 id │     title     │            body             │  author  │         published_at          │ views6────┼───────────────┼─────────────────────────────┼──────────┼───────────────────────────────┼───────7  1 │ Hello, World! │ This is my first blog post. │ John Doe │ 2024-11-15 12:04:18.756824+01 │  1000
```


### Viewing query plans#
You can see the execution plan using standardEXPLAINstatement.

```javascript
1EXPLAIN SELECT * FROM blog_post ORDER BY published_at DESC LIMIT 10;2                                                 QUERY PLAN3────────────────────────────────────────────────────────────────────────────────────────────────────────────4 Limit  (cost=0.15..1.67 rows=10 width=120)5   ->  Index Scan Backward using blog_post_published_at on blog_post  (cost=0.15..48.95 rows=320 width=120)67EXPLAIN SELECT * FROM blog_post WHERE id = 1;8                                    QUERY PLAN9──────────────────────────────────────────────────────────────────────────────────10 Index Scan using blog_post_pkey on blog_post  (cost=0.15..8.17 rows=1 width=120)11   Index Cond: (id = 1)1213EXPLAIN (ANALYZE, BUFFERS) SELECT * FROM blog_post ORDER BY published_at DESC LIMIT 10;14                                                                      QUERY PLAN15──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────16 Limit  (cost=0.15..1.67 rows=10 width=120) (actual time=0.052..0.054 rows=1 loops=1)17   ->  Index Scan Backward using blog_post_published_at on blog_post  (cost=0.15..48.95 rows=320 width=120) (actual time=0.050..0.052 rows=1 loops=1)18 Planning Time: 0.186 ms19 Execution Time: 0.088 ms
```


### Resources#
- Official OrioleDB documentation
- OrioleDB GitHub repository


================================================================================


# Database
Source: https://supabase.com/docs/guides/database/overview

Database

Every Supabase project comes with a fullPostgresdatabase, a free and open source database which is considered one of the world's most stable and advanced databases.


### Features#

### Table view#
You don't have to be a database expert to start using Supabase. Our table view makes Postgres as easy to use as a spreadsheet.




### Relationships#
Dig into the relationships within your data.


### Clone tables#
You can duplicate your tables, just like you would inside a spreadsheet.


### The SQL editor#
Supabase comes with a SQL Editor. You can also save your favorite queries to run later!


### Additional features#
- Supabase extends Postgres with realtime functionality using ourRealtime Server.
- Every project is a full Postgres database, withpostgreslevel access.
- Supabase manages your database backups.
- Import data directly from a CSV or excel spreadsheet.

Database backupsdo notinclude objects stored via the Storage API, as the database only includes metadata about these objects. Restoring an old backup does not restore objects that have been deleted since then.


### Extensions#
To expand the functionality of your Postgres database, you can use extensions.
You can enable Postgres extensions with the click of a button within the Supabase dashboard.

Learn moreabout all the extensions provided on Supabase.


### Terminology#

### Postgres or PostgreSQL?#
PostgreSQL the database was derived from the POSTGRES Project, a package written at the University of California at Berkeley in 1986. This package included a query language called "PostQUEL".

In 1994, Postgres95 was built on top of POSTGRES code, adding an SQL language interpreter as a replacement for PostQUEL.

Eventually, Postgres95 was renamed to PostgreSQL to reflect the SQL query capability.
After this, many people referred to it as Postgres since it's less prone to confusion. Supabase is all about simplicity, so we also refer to it as Postgres.


### Tips#
Read about resetting your database passwordhereand changing the timezone of your serverhere.


### Next steps#
- Read more aboutPostgres
- Sign in:supabase.com/dashboard


================================================================================


# Partitioning tables
Source: https://supabase.com/docs/guides/database/partitions

Partitioning tables

Table partitioning is a technique that allows you to divide a large table into smaller, more manageable parts called “partitions”.

Each partition contains a subset of the data based on a specified criteria, such as a range of values or a specific condition. Partitioning can significantly improve query performance and simplify data management for large datasets.


### Benefits of table partitioning#
- Improved query performance:allows queries to target specific partitions, reducing the amount of data scanned and improving query execution time.
- Scalability:With partitioning, you can add or remove partitions as your data grows or changes, enabling better scalability and flexibility.
- Efficient data management:simplifies tasks such as data loading, archiving, and deletion by operating on smaller partitions instead of the entire table.
- Enhanced maintenance operations:can optimize vacuuming and indexing, leading to faster maintenance tasks.


### Partitioning methods#
Postgres supports various partitioning methods based on how you want to partition your data. The commonly used methods are:


### Creating partitioned tables#
Let's consider an example of range partitioning for a sales table based on the order date. We'll create monthly partitions to store data for each month:

```javascript
1create table sales (2    id bigint generated by default as identity,3    order_date date not null,4    customer_id bigint,5    amount bigint,67    -- We need to include all the8    -- partitioning columns in constraints:9    primary key (order_date, id)10)11partition by range (order_date);1213create table sales_2000_0114	partition of sales15  for values from ('2000-01-01') to ('2000-02-01');1617create table sales_2000_0218	partition of sales19	for values from ('2000-02-01') to ('2000-03-01');
```

To create a partitioned table you appendpartition by range (<column_name>)to the table creation statement. The column that you are partitioning withmustbe included in any unique index, which is the reason why we specify a composite primary key here (primary key (order_date, id)).


### Querying partitioned tables#
To query a partitioned table, you have two options:


### Querying the parent table#
When you query the parent table, Postgres automatically routes the query to the relevant partitions based on the conditions specified in the query. This allows you to retrieve data from all partitions simultaneously.

Example:

```javascript
1select *2from sales3where order_date >= '2000-01-01' and order_date < '2000-03-01';
```

This query will retrieve data from both thesales_2000_01andsales_2000_02partitions.


### Querying specific partitions#
If you only need to retrieve data from a specific partition, you can directly query that partition instead of the parent table. This approach is useful when you want to target a specific range or condition within a partition.

```javascript
1select *2from sales_2000_02;
```

This query will retrieve data only from thesales_2000_02partition.


### When to partition your tables#
There is no real threshold to determine when you should use partitions. Partitions introduce complexity, and complexity should be avoided until it's needed. A few guidelines:

- If you are considering performance, avoid partitions until you see performance degradation on non-partitioned tables.
- If you are using partitions as a management tool, it's fine to create the partitions any time.
- If you don't know how you should partition your data, then it's probably too early.


### Examples#
Here are simple examples for each of the partitioning types in Postgres.


### Range partitioning#
Let's consider a range partitioning example for a table that stores sales data based on the order date. We'll create monthly partitions to store data for each month.

In this example, thesalestable is partitioned into two partitions:sales_januaryandsales_february. The data in these partitions is based on the specified range of order dates:

```javascript
1create table sales (2    id bigint generated by default as identity,3    order_date date not null,4    customer_id bigint,5    amount bigint,67    -- We need to include all the8    -- partitioning columns in constraints:9    primary key (order_date, id)10)11partition by range (order_date);1213create table sales_2000_0114	partition of sales15  for values from ('2000-01-01') to ('2000-02-01');1617create table sales_2000_0218	partition of sales19	for values from ('2000-02-01') to ('2000-03-01');
```


### List partitioning#
Let's consider a list partitioning example for a table that stores customer data based on their region. We'll create partitions to store customers from different regions.

In this example, thecustomerstable is partitioned into two partitions:customers_americasandcustomers_asia. The data in these partitions is based on the specified list of regions:

```javascript
1-- Create the partitioned table2create table customers (3    id bigint generated by default as identity,4    name text,5    country text,67    -- We need to include all the8    -- partitioning columns in constraints:9    primary key (country, id)10)11partition by list(country);1213create table customers_americas14	partition of customers15	for values in ('US', 'CANADA');1617create table customers_asia18	partition of customers19  for values in ('INDIA', 'CHINA', 'JAPAN');
```


### Hash partitioning#
You can use hash partitioning to evenly distribute data.

In this example, theproductstable is partitioned into two partitions:products_oneandproducts_two. The data is distributed across these partitions using a hash function:

```javascript
1create table products (2    id bigint generated by default as identity,3    name text,4    category text,5    price bigint6)7partition by hash (id);89create table products_one10	partition of products11  for values with (modulus 2, remainder 1);1213create table products_two14	partition of products15  for values with (modulus 2, remainder 0);
```


### Other tools#
There are several other tools available for Postgres partitioning, most notablypg_partman. Native partitioning was introduced in Postgres 10 and is generally thought to have better performance.


================================================================================


# Connecting with pgAdmin
Source: https://supabase.com/docs/guides/database/pgadmin

Connecting with pgAdmin


### What is pgAdmin?#
pgAdminis a GUI tool for managing Postgres databases. You can use it to connect to your database via SSL.


### Connecting pgAdmin with your Postgres database#

### Register
Register a new Postgres server.


### Name
Name your server.




### Connect
Add the connection info. Click the "Connect" button at the top of the page to open the connect Modal. Scroll down to "session pooler", click "view parameters" to toggle the parameters menu open and copy your connection parameters. Fill in your Database password that you made when creating your project (It can be reset in Database Settings above if you don't have it).




### SSL
Download your SSL certificate from Dashboard'sDatabase Settings.

In pgAdmin, navigate to the Parameters tab and select connection parameter as Root Certificate. Next navigate to the Root certificate input, it will open up a file-picker modal. Select the certificate you downloaded earlier and save the server details. pgAdmin should now be able to connect to your Postgres via SSL.




### Why connect to pgAdmin#
Connecting your Postgres instance topgAdmingives you a free, cross-platform GUI that makes tasks such as browsing objects, writing queries with autocomplete, running backups, and monitoring performance much faster and safer than usingpsqlalone.

It acts as a single control panel where you can manage multiple servers, inspect locks and slow queries in real time, and perform maintenance operations with a click.

For scripted migrations or ultra-light remote work you’ll still lean on plain SQL or CLI tools, but most teams findpgAdmininvaluable for exploration and routine administration.


================================================================================


# Postgres.js
Source: https://supabase.com/docs/guides/database/postgres-js

Postgres.js


### Connecting with Postgres.js#
Postgres.jsis a full-featured Postgres client for Node.js and Deno.


### Install
Install Postgres.js and related dependencies.

```javascript
1npm i postgres
```


### Connect
Create adb.jsfile with the connection details.

To get your connection details, go to theConnectpanel. ChooseTransaction poolerif you're on a platform with transient connections, such as a serverless function, andSession poolerif you have a long-lived connection. Copy the URI and save it as the environment variableDATABASE_URL.

```javascript
1// db.js2import postgres from 'postgres'34const connectionString = process.env.DATABASE_URL5const sql = postgres(connectionString)67export default sql
```


### Execute commands
Use the connection to execute commands.

```javascript
1import sql from './db.js'23async function getUsersOver(age) {4  const users = await sql`5    select name, age6    from users7    where age > ${ age }8  `9  // users = Result [{ name: "Walter", age: 80 }, { name: 'Murray', age: 68 }, ...]10  return users11}
```


================================================================================


# Cascade Deletes
Source: https://supabase.com/docs/guides/database/postgres/cascade-deletes

Cascade Deletes

There are 5 options for foreign key constraint deletes:

These options can be specified when defining a foreign key constraint using the "ON DELETE" clause. For example, the following SQL statement creates a foreign key constraint with theCASCADEoption:

```javascript
1alter table child_table2add constraint fk_parent foreign key (parent_id) references parent_table (id)3  on delete cascade;
```

This means that when a row is deleted from theparent_table, all related rows in thechild_tablewill be deleted as well.


### RESTRICTvsNO ACTION#
The difference betweenNO ACTIONandRESTRICTis subtle and can be a bit confusing.

BothNO ACTIONandRESTRICTare used to prevent deletion of a row in a parent table if there are related rows in a child table. However, there is a subtle difference in how they behave.

When a foreign key constraint is defined with the optionRESTRICT, it means that if a row in the parent table is deleted, the database will immediately raise an error and prevent the deletion of the row in the parent table. The database will not delete, update or set to NULL any rows in the referenced tables.

When a foreign key constraint is defined with the optionNO ACTION, it means that if a row in the parent table is deleted, the database will also raise an error and prevent the deletion of the row in the parent table. However unlikeRESTRICT,NO ACTIONhas the option to defer the check usingINITIALLY DEFERRED. This will only raise the above errorifthe referenced rows still exist at the end of the transaction.

The difference fromRESTRICTis that a constraint marked asNO ACTION INITIALLY DEFERREDis deferred until the end of the transaction, rather than running immediately. If, for example there is another foreign key constraint between the same tables marked asCASCADE, the cascade will occur first and delete the referenced rows, and no error will be thrown by the deferred constraint. Otherwise if there are still rows referencing the parent row by the end of the transaction, an error will be raised just like before. Just likeRESTRICT, the database will not delete, update or set to NULL any rows in the referenced tables.

In practice, you can use eitherNO ACTIONorRESTRICTdepending on your needs.NO ACTIONis the default behavior if you do not specify anything. If you prefer to defer the check until the end of the transaction, useNO ACTION INITIALLY DEFERRED.


### Example#
Let's further illustrate the difference with an example. We'll use the following data:

grandparent

parent

child

To create these tables and their data, we run:

```javascript
1create table grandparent (2  id serial primary key,3  name text4);56create table parent (7  id serial primary key,8  name text,9  parent_id integer references grandparent (id)10    on delete cascade11);1213create table child (14  id serial primary key,15  name text,16  father integer references parent (id)17    on delete restrict18);1920insert into grandparent21  (id, name)22values23  (1, 'Elizabeth');2425insert into parent26  (id, name, parent_id)27values28  (1, 'Charles', 1);2930insert into parent31  (id, name, parent_id)32values33  (2, 'Diana', 1);3435-- We'll just link the father for now36insert into child37  (id, name, father)38values39  (1, 'William', 1);
```


### RESTRICT#
RESTRICTwill prevent a delete and raise an error:

```javascript
1postgres=# delete from grandparent;2ERROR: update or delete on table "parent" violates foreign key constraint "child_father_fkey" on table "child"3DETAIL: Key (id)=(1) is still referenced from table "child".
```

Even though the foreign key constraint between parent and grandparent isCASCADE, the constraint between child and father isRESTRICT. Therefore an error is raised and no records are deleted.


### NO ACTION#
Let's change the child-father relationship toNO ACTION:

```javascript
1alter table child2drop constraint child_father_fkey;34alter table child5add constraint child_father_fkey foreign key (father) references parent (id)6  on delete no action;
```

We see thatNO ACTIONwill also prevent a delete and raise an error:

```javascript
1postgres=# delete from grandparent;2ERROR: update or delete on table "parent" violates foreign key constraint "child_father_fkey" on table "child"3DETAIL: Key (id)=(1) is still referenced from table "child".
```


### NO ACTION INITIALLY DEFERRED#
We'll change the foreign key constraint between child and father to beNO ACTION INITIALLY DEFERRED:

```javascript
1alter table child2drop constraint child_father_fkey;34alter table child5add constraint child_father_fkey foreign key (father) references parent (id)6  on delete no action initially deferred;
```

Here you will see thatINITIALLY DEFFEREDseems to operate likeNO ACTIONorRESTRICT. When we run a delete, it seems to make no difference:

```javascript
1postgres=# delete from grandparent;2ERROR: update or delete on table "parent" violates foreign key constraint "child_father_fkey" on table "child"3DETAIL: Key (id)=(1) is still referenced from table "child".
```

But, when we combine it withotherconstraints, then any other constraints take precedence. For example, let's run the same but add amothercolumn that has aCASCADEdelete:

```javascript
1alter table child2add column mother integer references parent (id)3  on delete cascade;45update child6set mother = 27where id = 1;
```

Then let's run a delete on thegrandparenttable:

```javascript
1postgres=# delete from grandparent;2DELETE 134postgres=# select * from parent;5 id | name | parent_id6----+------+-----------7(0 rows)89postgres=# select * from child;10 id | name | father | mother11----+------+--------+--------12(0 rows)
```

Themotherdeletion took precedence over thefather, and so William was deleted. After William was deleted, there was no reference to “Charles” and so he was free to be deleted, even though previously he wasn't (withoutINITIALLY DEFERRED).


================================================================================


# Column Level Security
Source: https://supabase.com/docs/guides/database/postgres/column-level-security

Column Level Security

PostgreSQL'sRow Level Security (RLS)gives you granular control over who can access rows of data. However, it doesn't give you control over which columns they can access within rows. Sometimes you want to restrict access to specific columns in your database. Column Level Privileges allows you to do just that.

This is an advanced feature. We do not recommend using column-level privileges for most users. Instead, we recommend using RLS policies in combination with a dedicated table for handling user roles.

Restricted roles cannot use the wildcard operator (*) on the affected table. Instead of usingSELECT * FROM <restricted_table>;or its API equivalent, you must specify the column names explicitly.


### Policies at the row level#
Policies in Row Level Security (RLS) are used to restrict access to rows in a table. Think of them like adding aWHEREclause to every query.

For example, let's assume you have apoststable with the following columns:

- id
- user_id
- title
- content
- created_at
- updated_at

You can restrict updates to just the user who created it usingRLS, with the following policy:

```javascript
1create policy "Allow update for owners" on posts for2update3  using ((select auth.uid()) = user_id);
```

However, this gives the post owner full access to update the row, including all of the columns.


### Privileges at the column level#
To restrict access to columns, you can usePrivileges.

There are two types of privileges in Postgres:

You can have both types of privileges on the same table. If you have both, and you revoke the column-level privilege, the table-level privilege will still be in effect.

By default, our table will have a table-levelUPDATEprivilege, which means that theauthenticatedrole can update all the columns in the table.

```javascript
1revoke2update3  on table public.posts4from5  authenticated;67grant8update9  (title, content) on table public.posts to authenticated;
```

In the above example, we are revoking the table-levelUPDATEprivilege from theauthenticatedrole and granting a column-levelUPDATEprivilege on just thetitleandcontentcolumns.

If we want to restrict access to updating thetitlecolumn:

```javascript
1revoke2update3  (title) on table public.posts4from5  authenticated;
```

This time, we are revoking the column-levelUPDATEprivilege of thetitlecolumn from theauthenticatedrole. We didn't need to revoke the table-levelUPDATEprivilege because it's already revoked.


### Manage column privileges in the Dashboard#
Column-level privileges are a powerful tool, but they're also quite advanced and in many cases, not the best fit for common access control needs. For that reason, we've intentionally moved the UI for this feature under the Feature Preview section in the dashboard.

You can view and edit the privileges in theSupabase Studio.




### Manage column privileges in migrations#
While you can manage privileges directly from the Dashboard, as your project grows you may want to manage them in your migrations. Read about database migrations in theLocal Developmentguide.


### Create a migration file
To get started, generate anew migrationto store the SQL needed to create your table along with row and column-level privileges.

```javascript
1supabase migration new create_posts_table
```


### Add the SQL to your migration file
This creates a new migration: supabase/migrations/<timestamp>
_create_posts_table.sql.

To that file, add the SQL to create thispoststable with row and column-level privileges.

```javascript
1create table2posts (3id bigint primary key generated always as identity,4user_id text,5title text,6content text,7created_at timestamptz default now()8updated_at timestamptz default now()9);1011-- Add row-level security12create policy "Allow update for owners" on posts for13update14using ((select auth.uid()) = user_id);1516-- Add column-level security17revoke18update19(title) on table public.posts20from21authenticated;
```


### Considerations when using column-level privileges#
- If you turn off a column privilege you won't be able to use that column at all.
- All operations (insert, update, delete) as well as usingselect *will fail.


================================================================================


# Database configuration
Source: https://supabase.com/docs/guides/database/postgres/configuration

Database configuration


### Updating the default configuration for your Postgres database.
Updating the default configuration for your Postgres database.

Postgres provides a set of sensible defaults for you database size. In some cases, these defaults can be updated. We do not recommend changing these defaults unless you know what you're doing.


### Timeouts#
See theTimeoutssection.


### Statement optimization#
All Supabase projects come with thepg_stat_statementsextension installed, which tracks planning and execution statistics for all statements executed against it. These statistics can be used in order to diagnose the performance of your project.

This data can further be used in conjunction with theexplainfunctionality of Postgres to optimize your usage.


### Managing timezones#
Every Supabase database is set to UTC timezone by default. We strongly recommend keeping it this way, even if your users are in a different location. This is because it makes it much easier to calculate differences between timezones if you adopt the mental model that everything in your database is in UTC time.


### Change timezone#
```javascript
1alter database postgres2set timezone to 'America/New_York';
```


### Full list of timezones#
Get a full list of timezones supported by your database. This will return the following columns:

- name: Time zone name
- abbrev: Time zone abbreviation
- utc_offset: Offset from UTC (positive means east of Greenwich)
- is_dst: True if currently observing daylight savings

```javascript
1select name, abbrev, utc_offset, is_dst2from pg_timezone_names()3order by name;
```


### Search for a specific timezone#
Useilike(case insensitive search) to find specific timezones.

```javascript
1select *2from pg_timezone_names()3where name ilike '%york%';
```


================================================================================


# Custom Claims & Role-based Access Control (RBAC)
Source: https://supabase.com/docs/guides/database/postgres/custom-claims-and-role-based-access-control-rbac

Custom Claims & Role-based Access Control (RBAC)

Custom Claims are special attributes attached to a user that you can use to control access to portions of your application. For example:

```javascript
1{2  "user_role": "admin",3  "plan": "TRIAL",4  "user_level": 100,5  "group_name": "Super Guild!",6  "joined_on": "2022-05-20T14:28:18.217Z",7  "group_manager": false,8  "items": ["toothpick", "string", "ring"]9}
```

To implement Role-Based Access Control (RBAC) withcustom claims, use aCustom Access Token Auth Hook. This hook runs before a token is issued. You can use it to add additional claims to the user's JWT.

This guide uses theSlack Clone exampleto demonstrate how to add auser_roleclaim and use it in yourRow Level Security (RLS) policies.


### Create a table to track user roles and permissions#
In this example, you will implement two user roles with specific permissions:

- moderator: A moderator can delete all messages but not channels.
- admin: An admin can delete all messages and channels.

```javascript
1-- Custom types2create type public.app_permission as enum ('channels.delete', 'messages.delete');3create type public.app_role as enum ('admin', 'moderator');45-- USER ROLES6create table public.user_roles (7  id        bigint generated by default as identity primary key,8  user_id   uuid references auth.users on delete cascade not null,9  role      app_role not null,10  unique (user_id, role)11);12comment on table public.user_roles is 'Application roles for each user.';1314-- ROLE PERMISSIONS15create table public.role_permissions (16  id           bigint generated by default as identity primary key,17  role         app_role not null,18  permission   app_permission not null,19  unique (role, permission)20);21comment on table public.role_permissions is 'Application permissions for each role.';
```

For thefull schema, see the example application onGitHub.

You can now manage your roles and permissions in SQL. For example, to add the mentioned roles and permissions from above, run:

```javascript
1insert into public.role_permissions (role, permission)2values3  ('admin', 'channels.delete'),4  ('admin', 'messages.delete'),5  ('moderator', 'messages.delete');
```


### Create Auth Hook to apply user role#
TheCustom Access Token Auth Hookruns before a token is issued. You can use it to edit the JWT.

```javascript
1-- Create the auth hook function2create or replace function public.custom_access_token_hook(event jsonb)3returns jsonb4language plpgsql5stable6as $$7  declare8    claims jsonb;9    user_role public.app_role;10  begin11    -- Fetch the user role in the user_roles table12    select role into user_role from public.user_roles where user_id = (event->>'user_id')::uuid;1314    claims := event->'claims';1516    if user_role is not null then17      -- Set the claim18      claims := jsonb_set(claims, '{user_role}', to_jsonb(user_role));19    else20      claims := jsonb_set(claims, '{user_role}', 'null');21    end if;2223    -- Update the 'claims' object in the original event24    event := jsonb_set(event, '{claims}', claims);2526    -- Return the modified or original event27    return event;28  end;29$$;3031grant usage on schema public to supabase_auth_admin;3233grant execute34  on function public.custom_access_token_hook35  to supabase_auth_admin;3637revoke execute38  on function public.custom_access_token_hook39  from authenticated, anon, public;4041grant all42  on table public.user_roles43to supabase_auth_admin;4445revoke all46  on table public.user_roles47  from authenticated, anon, public;4849create policy "Allow auth admin to read user roles" ON public.user_roles50as permissive for select51to supabase_auth_admin52using (true);
```


### Enable the hook#
In the dashboard, navigate toAuthentication > Hooks (Beta)and select the appropriate Postgres function from the dropdown menu.

When developing locally, follow thelocal developmentinstructions.

To learn more about Auth Hooks, see theAuth Hooks docs.


### Accessing custom claims in RLS policies#
To utilize Role-Based Access Control (RBAC) in Row Level Security (RLS) policies, create anauthorizemethod that reads the user's role from their JWT and checks the role's permissions:

```javascript
1create or replace function public.authorize(2  requested_permission app_permission3)4returns boolean as $$5declare6  bind_permissions int;7  user_role public.app_role;8begin9  -- Fetch user role once and store it to reduce number of calls10  select (auth.jwt() ->> 'user_role')::public.app_role into user_role;1112  select count(*)13  into bind_permissions14  from public.role_permissions15  where role_permissions.permission = requested_permission16    and role_permissions.role = user_role;1718  return bind_permissions > 0;19end;20$$ language plpgsql stable security definer set search_path = '';
```

You can read more about using functions in RLS policies in theRLS guide.

You can then use theauthorizemethod within your RLS policies. For example, to enable the desired delete access, you would add the following policies:

```javascript
1create policy "Allow authorized delete access" on public.channels for delete to authenticated using ( (SELECT authorize('channels.delete')) );2create policy "Allow authorized delete access" on public.messages for delete to authenticated using ( (SELECT authorize('messages.delete')) );
```


### Accessing custom claims in your application#
The auth hook will only modify the access token JWT but not the auth response. Therefore, to access the custom claims in your application, e.g. your browser client, or server-side middleware, you will need to decode theaccess_tokenJWT on the auth session.

In a JavaScript client application you can for example use thejwt-decodepackage:

```javascript
1import { jwtDecode } from 'jwt-decode'23const { subscription: authListener } = supabase.auth.onAuthStateChange(async (event, session) => {4  if (session) {5    const jwt = jwtDecode(session.access_token)6    const userRole = jwt.user_role7  }8})
```

For server-side logic you can use packages likeexpress-jwt,koa-jwt,PyJWT,dart_jsonwebtoken,Microsoft.AspNetCore.Authentication.JwtBearer, etc.


### Conclusion#
You now have a robust system in place to manage user roles and permissions within your database that automatically propagates to Supabase Auth.


### More resources#
- Auth Hooks
- Row Level Security
- RLS Functions
- Next.js Slack Clone Example


================================================================================


# Drop all tables in a PostgreSQL schema
Source: https://supabase.com/docs/guides/database/postgres/dropping-all-tables-in-schema

Drop all tables in a PostgreSQL schema

Execute the following query to drop all tables in a given schema.
Replacemy-schema-namewith the name of your schema. In Supabase, the default schema ispublic.

This deletes all tables and their associated data. Ensure you have a recentbackupbefore proceeding.

```javascript
1do $$ declare2    r record;3begin4    for r in (select tablename from pg_tables where schemaname = 'my-schema-name') loop5        execute 'drop table if exists ' || quote_ident(r.tablename) || ' cascade';6    end loop;7end $$;
```

This query works by listing out all the tables in the given schema and then executing adrop tablefor each (hence thefor... loop).

You can run this query using theSQL Editorin the Supabase Dashboard, or viapsqlif you'reconnecting directly to the database.


================================================================================


# Managing Enums in Postgres
Source: https://supabase.com/docs/guides/database/postgres/enums

Managing Enums in Postgres

Enums in Postgres are a custom data type. They allow you to define a set of values (or labels) that a column can hold. They are useful when you have a fixed set of possible values for a column.


### Creating enums#
You can define a Postgres Enum using thecreate typestatement. Here's an example:

```javascript
1create type mood as enum (2  'happy',3  'sad',4  'excited',5  'calm'6);
```

In this example, we've created an Enum called "mood" with four possible values.


### When to use enums#
There is a lot of overlap between Enums and foreign keys. Both can be used to define a set of values for a column. However, there are some advantages to using Enums:

- Performance: You can query a single table instead of finding the value from a lookup table.
- Simplicity: Generally the SQL is easier to read and write.

There are also some disadvantages to using Enums:

- Limited Flexibility: Adding and removing values requires modifying the database schema (i.e.: using migrations) rather than adding data to a table.
- Maintenance Overhead: Enum types require ongoing maintenance. If your application's requirements change frequently, maintaining enums can become burdensome.

In general you should only use Enums when the list of values is small, fixed, and unlikely to change often. Things like "a list of continents" or "a list of departments" are good candidates for Enums.


### Using enums in tables#
To use the Enum in a table, you can define a column with the Enum type. For example:

```javascript
1create table person (2  id serial primary key,3  name text,4  current_mood mood5);
```

Here, thecurrent_moodcolumn can only have values from the "mood" Enum.


### Inserting data with enums#
You can insert data into a table with Enum columns by specifying one of the Enum values:

```javascript
1insert into person2  (name, current_mood)3values4  ('Alice', 'happy');
```


### Querying data with enums#
When querying data, you can filter and compare Enum values as usual:

```javascript
1select * 2from person 3where current_mood = 'sad';
```


### Managing enums#
You can manage your Enums using thealter typestatement. Here are some examples:


### Updating enum values#
You can update the value of an Enum column:

```javascript
1update person2set current_mood = 'excited'3where name = 'Alice';
```


### Adding enum values#
To add new values to an existing Postgres Enum, you can use theALTER TYPEstatement. Here's how you can do it:

Let's say you have an existing Enum calledmood, and you want to add a new value,content:

```javascript
1alter type mood add value 'content';
```


### Removing enum values#
Even though it is possible, it is unsafe to remove enum values once they have been created. It's better to leave the enum value in place.

Read thePostgres mailing listfor more information:

There is noALTER TYPE DELETE VALUEin Postgres. Even if you delete every occurrence of an Enum value within a table (and vacuumed away those rows), the target value could still exist in upper index pages. If you delete thepg_enumentry you'll break the index.


### Getting a list of enum values#
Check your existing Enum values by querying the enum_range function:

```javascript
1select enum_range(null::mood);
```


### Resources#
- Official Postgres Docs:Enumerated Types


================================================================================


# Event Triggers
Source: https://supabase.com/docs/guides/database/postgres/event-triggers

Event Triggers


### Automatically execute SQL on database events.
Automatically execute SQL on database events.

In Postgres, anevent triggeris similar to atrigger, except that it is triggered by database level events (and is usually reserved forsuperusers)

With ourSupautilsextension (installed automatically for all Supabase projects), thepostgresuser has the ability to create and manage event triggers.

Some use cases for event triggers are:

- Capturing Data Definition Language (DDL) changes - these are changes to your database schema (though thepgAuditextension provides a more complete solution)
- Enforcing/monitoring/preventing actions - such as preventing tables from being dropped in Production or enforcing RLS on all new tables

The guide covers two example event triggers:


### Creating an event trigger#
Only thepostgresuser can create event triggers, so make sure you are authenticated as them. As with triggers, event triggers consist of 2 parts


### Example trigger function - prevent dropping tables#
This example protects any table from being dropped. You can override it by temporarily disabling the event trigger:ALTER EVENT TRIGGER dont_drop_trigger DISABLE;

```javascript
1-- Function2CREATE OR REPLACE FUNCTION dont_drop_function()3  RETURNS event_trigger LANGUAGE plpgsql AS $$4DECLARE5    obj record;6    tbl_name text;7BEGIN8    FOR obj IN SELECT * FROM pg_event_trigger_dropped_objects()9    LOOP10        IF obj.object_type = 'table' THEN11            RAISE EXCEPTION 'ERROR: All tables in this schema are protected and cannot be dropped';12        END IF;13    END LOOP;14END;15$$;1617-- Event trigger18CREATE EVENT TRIGGER dont_drop_trigger19ON sql_drop20EXECUTE FUNCTION dont_drop_function();
```


### Example trigger function - auto enable Row Level Security#
See how toauto enable RLS for new tables.


### Event trigger Functions and firing events#
Event triggers can be triggered on:

- ddl_command_start- occurs just before a DDL command for almost all objects within a schema
- ddl_command_end- occurs just after a DDL command for almost all objects within a schema
- sql_drop- occurs just beforeddl_command_endfor any DDL commands thatDROPa database object (note that altering a table can cause it to be dropped)
- table_rewrite- occurs just before a table is rewritten using theALTER TABLEcommand

Event triggers run for each DDL command specified above and can consume resources which may cause performance issues if not used carefully.

Within each event trigger, helper functions exist to view the objects being modified or the command being run. For example, our example callspg_event_trigger_dropped_objects()to view the object(s) being dropped. For a more comprehensive overview of these functions, read theofficial event trigger definition documentation

To view the matrix commands that cause an event trigger to fire, read theofficial event trigger matrix documentation


### Disabling an event trigger#
You can disable an event trigger using thealter event triggercommand:

```javascript
1ALTER EVENT TRIGGER dont_drop_trigger DISABLE;
```


### Dropping an event trigger#
You can delete a trigger using thedrop event triggercommand:

```javascript
1DROP EVENT TRIGGER dont_drop_trigger;
```


### Resources#
- Official Postgres Docs:Event Trigger Behaviours
- Official Postgres Docs:Event Trigger Firing Matrix
- Supabase blog:Postgres Event Triggers without superuser access


================================================================================


# Select first row for each group in PostgreSQL
Source: https://supabase.com/docs/guides/database/postgres/first-row-in-group

Select first row for each group in PostgreSQL

Given a tableseasons:

We want to find the rows containing the maximum number of pointsper team.

The expected output we want is:

From theSQL Editor, you can run a query like:

```javascript
1select distinct2  on (team) id,3  team,4  points5from6  seasons7order BY8  id,9  points desc,10  team;
```

The important bits here are:

- Thedesckeyword to order thepointsfrom highest to lowest.
- Thedistinctkeyword that tells Postgres to only return a single row per team.

This query can also be executed viapsqlor any other query editor if you prefer toconnect directly to the database.


================================================================================


# Managing Indexes in PostgreSQL
Source: https://supabase.com/docs/guides/database/postgres/indexes

Managing Indexes in PostgreSQL

An index makes your Postgres queries faster. The index is like a "table of contents" for your data - a reference list which allows queries to quickly locate a row in a given table without needing to scan the entire table (which in large tables can take a long time).

Indexes can be structured in a few different ways. The type of index chosen depends on the values you are indexing. By far the most common index type, and the default in Postgres, is the B-Tree. A B-Tree is the generalized form of a binary search tree, where nodes can have more than two children.

Even though indexes improve query performance, the Postgres query planner may not always make use of a given index when choosing which optimizations to make. Additionally indexes come with some overhead - additional writes and increased storage - so it's useful to understand how and when to use indexes, if at all.


### Create an index#
Let's take an example table:

```javascript
1create table persons (2  id bigint generated by default as identity primary key,3  age int,4  height int,5  weight int,6  name text,7  deceased boolean8);
```

All the queries in this guide can be run using theSQL Editorin the Supabase Dashboard, or viapsqlif you'reconnecting directly to the database.

We might want to frequently query users based on their age:

```javascript
1select name from persons where age = 32;
```

Without an index, Postgres will scan every row in the table to find equality matches on age.

You can verify this by doing an explain on the query:

```javascript
1explain select name from persons where age = 32;
```

Outputs:

```javascript
1Seq Scan on persons  (cost=0.00..22.75 rows=x width=y)2Filter: (age = 32)
```

To add a simple B-Tree index you can run:

```javascript
1create index idx_persons_age on persons (age);
```

It can take a long time to build indexes on large datasets and the default behaviour ofcreate indexis to lock the table from writes.

Luckily Postgres provides us withcreate index concurrentlywhich prevents blocking writes on the table, but does take a bit longer to build.

Here is a simplified diagram of the index we just created (note that in practice, nodes actually have more than two children).

You can see that in any large data set, traversing the index to locate a given value can be done in much less operations (O(log n)) than compared to scanning the table one value at a time from top to bottom (O(n)).


### Partial indexes#
If you are frequently querying a subset of rows then it may be more efficient to build a partial index. In our example, perhaps we only want to match onagewheredeceased is false. We could build a partial index:

```javascript
1create index idx_living_persons_age on persons (age)2where deceased is false;
```


### Ordering indexes#
By default B-Tree indexes are sorted in ascending order, but sometimes you may want to provide a different ordering. Perhaps our application has a page featuring the top 10 oldest people. Here we would want to sort in descending order, and includeNULLvalues last. For this we can use:

```javascript
1create index idx_persons_age_desc on persons (age desc nulls last);
```


### Reindexing#
After a while indexes can become stale and may need rebuilding. Postgres provides areindexcommand for this, but due to Postgres locks being placed on the index during this process, you may want to make use of theconcurrentkeyword.

```javascript
1reindex index concurrently idx_persons_age;
```

Alternatively you can reindex all indexes on a particular table:

```javascript
1reindex table concurrently persons;
```

Take note thatreindexcan be used inside a transaction, butreindex [index/table] concurrentlycannot.


### Index Advisor#
Indexes can improve query performance of your tables as they grow. The Supabase Dashboard offers an Index Advisor, which suggests potential indexes to add to your tables.

For more information on the Index Advisor and its suggestions, see theindex_advisorextension.

To use the Dashboard Index Advisor:


### Understanding Index Advisor results#
The Indexes tab shows the existing indexes used in the selected query. Note that indexes suggested in the "New Index Recommendations" section may not be used when you create them. Postgres' query planner may intentionally ignore an available index if it determines that the query will be faster without. For example, on a small table, a sequential scan might be faster than an index scan. In that case, the planner will switch to using the index as the table size grows, helping to future proof the query.

If additional indexes might improve your query, the Index Advisor shows the suggested indexes with the estimated improvement in startup and total costs:

- Startup cost is the cost to fetch the first row
- Total cost is the cost to fetch all the rows

Costs are in arbitrary units, where a single sequential page read costs 1.0 units.


================================================================================


# Postgres Roles
Source: https://supabase.com/docs/guides/database/postgres/roles

Postgres Roles


### Managing access to your Postgres database and configuring permissions.
Managing access to your Postgres database and configuring permissions.

Postgres manages database access permissions using the concept of roles. Generally you wouldn't use these roles for your own application - they are mostly for configuringsystem accessto your database. If you want to configureapplication access, then you should useRow Level Security(RLS). You can also implementRole-based Access Controlon top of RLS.


### Users vs roles#
In Postgres, roles can function as users or groups of users. Users are roles with login privileges, while groups (also known as role groups) are roles that don't have login privileges but can be used to manage permissions for multiple users.


### Creating roles#
You can create a role using thecreate rolecommand:

```javascript
1create role "role_name";
```


### Creating users#
Roles and users are essentially the same in Postgres, however if you want to use password-logins for a specific role, then you can useWITH LOGIN PASSWORD:

```javascript
1create role "role_name" with login password 'extremely_secure_password';
```


### Passwords#
Your Postgres database is the core of your Supabase project, so it's important that every role has a strong, secure password at all times. Here are some tips for creating a secure password:

- Use a password manager to generate it.
- Make a long password (12 characters at least).
- Don't use any common dictionary words.
- Use both upper and lower case characters, numbers, and special symbols.


### Special symbols in passwords#
If you use special symbols in your Postgres password, you must remember topercent-encodeyour password later if using the Postgres connection string, for example,postgresql://postgres.projectref:p%3Dword@aws-0-us-east-1.pooler.supabase.com:6543/postgres


### Changing your project password#
When you created your project you were also asked to enter a password. This is the password for thepostgresrole in your database. You can update this from the Dashboard under theDatabase Settingspage. You shouldnevergive this to third-party service unless you absolutely trust them. Instead, we recommend that you create a new user for every service that you want to give access too. This will also help you with debugging - you can see every query that each role is executing in your database withinpg_stat_statements.

Changing the password does not result in any downtime. All connected services, such as PostgREST, PgBouncer, and other Supabase managed services, are automatically updated to use the latest password to ensure availability. However, if you have any external services connecting to the Supabase database using hardcoded username/password credentials, a manual update will be required.


### Granting permissions#
Roles can be granted various permissions on database objects using theGRANTcommand. Permissions includeSELECT,INSERT,UPDATE, andDELETE. You can configure access to almost any object inside your database - including tables, views, functions, and triggers.


### Revoking permissions#
Permissions can be revoked using theREVOKEcommand:

```javascript
1REVOKE permission_type ON object_name FROM role_name;
```


### Role hierarchy#
Roles can be organized in a hierarchy, where one role can inherit permissions from another. This simplifies permission management, as you can define permissions at a higher level and have them automatically apply to all child roles.


### Role inheritance#
To create a role hierarchy, you first need to create the parent and child roles. The child role will inherit permissions from its parent. Child roles can be added using the INHERIT option when creating the role:

```javascript
1create role "child_role_name" inherit "parent_role_name";
```


### Preventing inheritance#
In some cases, you might want to prevent a role from having a child relationship (typically superuser roles). You can prevent inheritance relations usingNOINHERIT:

```javascript
1alter role "child_role_name" noinherit;
```


### Supabase roles#
Postgres comes with a set ofpredefined roles. Supabase extends this with a default set of roles which are configured on your database when you start a new project:


### postgres#
The default Postgres role. This has admin privileges.


### anon#
For unauthenticated, public access. This is the role which the API (PostgREST) will use when a useris notlogged in.


### authenticator#
A special role for the API (PostgREST). It has very limited access, and is used to validate a JWT and then
"change into" another role determined by the JWT verification.


### authenticated#
For "authenticated access." This is the role which the API (PostgREST) will use when a userislogged in.


### service_role#
For elevated access. This role is used by the API (PostgREST) to bypass Row Level Security.


### supabase_auth_admin#
Used by the Auth middleware to connect to the database and run migration. Access is scoped to theauthschema.


### supabase_storage_admin#
Used by the Auth middleware to connect to the database and run migration. Access is scoped to thestorageschema.


### supabase_etl_admin#
Used byReplication powered by Supabase ETLto replicate database changes to external destinations. Has read-all access and replication privileges for change data capture, bypasses Row Level Security, and can write to theetlschema.


### dashboard_user#
For running commands via the Supabase UI.


### supabase_admin#
An internal role Supabase uses for administrative tasks, such as running upgrades and automations.


### Resources#
- Official Postgres docs:Database Roles
- Official Postgres docs:Role Membership
- Official Postgres docs:Function Permissions


================================================================================


# Roles, superuser access and unsupported operations
Source: https://supabase.com/docs/guides/database/postgres/roles-superuser

Roles, superuser access and unsupported operations

Supabase provides the defaultpostgresrole to all instances deployed. Superuser access is not given as it allows destructive operations to be performed on the database.

To ensure you are not impacted by this, additional privileges are granted to thepostgresuser to allow it to run some operations that are normally restricted to superusers.

However, this does mean that some operations, that typically requiresuperuserprivileges, are not available on Supabase. These are documented below:


### Unsupported operations#
- COPY ... FROM PROGRAM
- ALTER USER ... WITH SUPERUSER


================================================================================


# Row Level Security
Source: https://supabase.com/docs/guides/database/postgres/row-level-security

Row Level Security


### Secure your data using Postgres Row Level Security.
Secure your data using Postgres Row Level Security.

When you need granular authorization rules, nothing beats Postgres'sRow Level Security (RLS).


### Row Level Security in Supabase#
Supabase allows convenient and secure data access from the browser, as long as you enable RLS.

RLSmustalways be enabled on any tables stored in an exposed schema. By default, this is thepublicschema.

RLS is enabled by default on tables created with the Table Editor in the dashboard. If you create one in raw SQL or with the SQL editor, remember to enable RLS yourself:

```javascript
1alter table <schema_name>.<table_name>2enable row level security;
```

RLS is incredibly powerful and flexible, allowing you to write complex SQL rules that fit your unique business needs. RLS can be combined withSupabase Authfor end-to-end user security from the browser to the database.

RLS is a Postgres primitive and can provide "defense in depth" to protect your data from malicious actors even when accessed through third-party tooling.


### Policies#
Policiesare Postgres's rule engine. Policies are easy to understand once you get the hang of them. Each policy is attached to a table, and the policy is executed every time a table is accessed.

You can just think of them as adding aWHEREclause to every query. For example a policy like this ...

```javascript
1create policy "Individuals can view their own todos."2on todos for select3using ( (select auth.uid()) = user_id );
```

.. would translate to this whenever a user tries to select from the todos table:

```javascript
1select *2from todos3where auth.uid() = todos.user_id;4-- Policy is implicitly added.
```


### Enabling Row Level Security#
You can enable RLS for any table using theenable row level securityclause:

```javascript
1alter table "table_name" enable row level security;
```

Once you have enabled RLS, no data will be accessible via theAPIwhen using the publicanonkey, until you create policies.


### Auto-enable RLS for new tables#
If you want RLS enabled automatically for new tables, you can create an event trigger that runs after table creation. This uses a Postgresevent triggerto callALTER TABLE ... ENABLE ROW LEVEL SECURITYon each newly created table.

```javascript
1CREATE OR REPLACE FUNCTION rls_auto_enable()2RETURNS EVENT_TRIGGER3LANGUAGE plpgsql4SECURITY DEFINER5SET search_path = pg_catalog6AS $$7DECLARE8  cmd record;9BEGIN10  FOR cmd IN11    SELECT *12    FROM pg_event_trigger_ddl_commands()13    WHERE command_tag IN ('CREATE TABLE', 'CREATE TABLE AS', 'SELECT INTO')14      AND object_type IN ('table','partitioned table')15  LOOP16     IF cmd.schema_name IS NOT NULL AND cmd.schema_name IN ('public') AND cmd.schema_name NOT IN ('pg_catalog','information_schema') AND cmd.schema_name NOT LIKE 'pg_toast%' AND cmd.schema_name NOT LIKE 'pg_temp%' THEN17      BEGIN18        EXECUTE format('alter table if exists %s enable row level security', cmd.object_identity);19        RAISE LOG 'rls_auto_enable: enabled RLS on %', cmd.object_identity;20      EXCEPTION21        WHEN OTHERS THEN22          RAISE LOG 'rls_auto_enable: failed to enable RLS on %', cmd.object_identity;23      END;24     ELSE25        RAISE LOG 'rls_auto_enable: skip % (either system schema or not in enforced list: %.)', cmd.object_identity, cmd.schema_name;26     END IF;27  END LOOP;28END;29$$;3031DROP EVENT TRIGGER IF EXISTS ensure_rls;32CREATE EVENT TRIGGER ensure_rls33ON ddl_command_end34WHEN TAG IN ('CREATE TABLE', 'CREATE TABLE AS', 'SELECT INTO')35EXECUTE FUNCTION rls_auto_enable();
```

Note that this applies to tables created after the trigger is installed. Existing tables still need RLS enabled manually.

When a request is made without an authenticated user (e.g., no access token is provided or the session has expired),auth.uid()returnsnull.

This means that a policy like:

```javascript
1USING (auth.uid() = user_id)
```

will silently fail for unauthenticated users, because:

```javascript
1null = user_id
```

is always false in SQL.

To avoid confusion and make your intention clear, we recommend explicitly checking for authentication:

```javascript
1USING (auth.uid() IS NOT NULL AND auth.uid() = user_id)
```


### Authenticated and unauthenticated roles#
Supabase maps every request to one of the roles:

- anon: an unauthenticated request (the user is not logged in)
- authenticated: an authenticated request (the user is logged in)

These are actuallyPostgres Roles. You can use these roles within your Policies using theTOclause:

```javascript
1create policy "Profiles are viewable by everyone"2on profiles for select3to authenticated, anon4using ( true );56-- OR78create policy "Public profiles are viewable only by authenticated users"9on profiles for select10to authenticated11using ( true );
```

Using theanonPostgres role is different from ananonymous userin Supabase Auth. An anonymous user assumes theauthenticatedrole to access the database and can be differentiated from a permanent user by checking theis_anonymousclaim in the JWT.


### Creating policies#
Policies are SQL logic that you attach to a Postgres table. You can attach as many policies as you want to each table.

Supabase provides somehelpersthat simplify RLS if you're using Supabase Auth. We'll use these helpers to illustrate some basic policies:


### SELECT policies#
You can specify select policies with theusingclause.

Let's say you have a table calledprofilesin the public schema and you want to enable read access to everyone.

```javascript
1-- 1. Create table2create table profiles (3  id uuid primary key,4  user_id uuid references auth.users,5  avatar_url text6);78-- 2. Enable RLS9alter table profiles enable row level security;1011-- 3. Create Policy12create policy "Public profiles are visible to everyone."13on profiles for select14to anon         -- the Postgres Role (recommended)15using ( true ); -- the actual Policy
```

Alternatively, if you only wanted users to be able to see their own profiles:

```javascript
1create policy "User can see their own profile only."2on profiles3for select using ( (select auth.uid()) = user_id );
```


### INSERT policies#
You can specify insert policies with thewith checkclause. Thewith checkexpression ensures that any new row data adheres to the policy constraints.

Let's say you have a table calledprofilesin the public schema and you only want users to be able to create a profile for themselves. In that case, we want to check their User ID matches the value that they are trying to insert:

```javascript
1-- 1. Create table2create table profiles (3  id uuid primary key,4  user_id uuid references auth.users,5  avatar_url text6);78-- 2. Enable RLS9alter table profiles enable row level security;1011-- 3. Create Policy12create policy "Users can create a profile."13on profiles for insert14to authenticated                          -- the Postgres Role (recommended)15with check ( (select auth.uid()) = user_id );      -- the actual Policy
```


### UPDATE policies#
You can specify update policies by combining both theusingandwith checkexpressions.

Theusingclause represents the condition that must be true for the update to be allowed, andwith checkclause ensures that the updates made adhere to the policy constraints.

Let's say you have a table calledprofilesin the public schema and you only want users to be able to update their own profile.

You can create a policy where theusingclause checks if the user owns the profile being updated. And thewith checkclause ensures that, in the resultant row, users do not change theuser_idto a value that is not equal to their User ID, maintaining that the modified profile still meets the ownership condition.

```javascript
1-- 1. Create table2create table profiles (3  id uuid primary key,4  user_id uuid references auth.users,5  avatar_url text6);78-- 2. Enable RLS9alter table profiles enable row level security;1011-- 3. Create Policy12create policy "Users can update their own profile."13on profiles for update14to authenticated                    -- the Postgres Role (recommended)15using ( (select auth.uid()) = user_id )       -- checks if the existing row complies with the policy expression16with check ( (select auth.uid()) = user_id ); -- checks if the new row complies with the policy expression
```

If nowith checkexpression is defined, then theusingexpression will be used both to determine which rows are visible (normal USING case) and which new rows will be allowed to be added (WITH CHECK case).

To perform anUPDATEoperation, a correspondingSELECTpolicyis required. Without aSELECTpolicy, theUPDATEoperation will not work as expected.


### DELETE policies#
You can specify delete policies with theusingclause.

Let's say you have a table calledprofilesin the public schema and you only want users to be able to delete their own profile:

```javascript
1-- 1. Create table2create table profiles (3  id uuid primary key,4  user_id uuid references auth.users,5  avatar_url text6);78-- 2. Enable RLS9alter table profiles enable row level security;1011-- 3. Create Policy12create policy "Users can delete a profile."13on profiles for delete14to authenticated                     -- the Postgres Role (recommended)15using ( (select auth.uid()) = user_id );      -- the actual Policy
```


### Views#
Views bypass RLS by default because they are usually created with thepostgresuser. This is a feature of Postgres, which automatically creates views withsecurity definer.

In Postgres 15 and above, you can make a view obey the RLS policies of the underlying tables when invoked byanonandauthenticatedroles by settingsecurity_invoker = true.

```javascript
1create view <VIEW_NAME>2with(security_invoker = true)3as select <QUERY>
```

In older versions of Postgres, protect your views by revoking access from theanonandauthenticatedroles, or by putting them in an unexposed schema.


### Helper functions#
Supabase provides some helper functions that make it easier to write Policies.


### auth.uid()#
Returns the ID of the user making the request.


### auth.jwt()#
Not all information present in the JWT should be used in RLS policies. For instance, creating an RLS policy that relies on theuser_metadataclaim can create security issues in your application as this information can be modified by authenticated end users.

Returns the JWT of the user making the request. Anything that you store in the user'sraw_app_meta_datacolumn or theraw_user_meta_datacolumn will be accessible using this function. It's important to know the distinction between these two:

- raw_user_meta_data- can be updated by the authenticated user using thesupabase.auth.update()function. It is not a good place to store authorization data.
- raw_app_meta_data- cannot be updated by the user, so it's a good place to store authorization data.

Theauth.jwt()function is extremely versatile. For example, if you store some team data insideapp_metadata, you can use it to determine whether a particular user belongs to a team. For example, if this was an array of IDs:

```javascript
1create policy "User is in team"2on my_table3to authenticated4using ( team_id in (select auth.jwt() -> 'app_metadata' -> 'teams'));
```

Keep in mind that a JWT is not always "fresh". In the example above, even if you remove a user from a team and update theapp_metadatafield, that will not be reflected usingauth.jwt()until the user's JWT is refreshed.

Also, if you are using Cookies for Auth, then you must be mindful of the JWT size. Some browsers are limited to 4096 bytes for each cookie, and so the total size of your JWT should be small enough to fit inside this limitation.


### MFA#
Theauth.jwt()function can be used to check forMulti-Factor Authentication. For example, you could restrict a user from updating their profile unless they have at least 2 levels of authentication (Assurance Level 2):

```javascript
1create policy "Restrict updates."2on profiles3as restrictive4for update5to authenticated using (6  (select auth.jwt()->>'aal') = 'aal2'7);
```


### Bypassing Row Level Security#
Supabase provides special "Service" keys, which can be used to bypass RLS. These should never be used in the browser or exposed to customers, but they are useful for administrative tasks.

Supabase will adhere to the RLS policy of the signed-in user, even if the client library is initialized with a Service Key.

You can also create newPostgres Roleswhich can bypass Row Level Security using the "bypass RLS" privilege:

```javascript
1alter role "role_name" with bypassrls;
```

This can be useful for system-level access. You shouldnevershare login credentials for any Postgres Role with this privilege.


### RLS performance recommendations#
Every authorization system has an impact on performance. While row level security is powerful, the performance impact is important to keep in mind. This is especially true for queries that scan every row in a table - like manyselectoperations, including those using limit, offset, and ordering.

Based on a series oftests, we have a few recommendations for RLS:


### Add indexes#
Make sure you've addedindexeson any columns used within the Policies which are not already indexed (or primary keys). For a Policy like this:

```javascript
1create policy "rls_test_select" on test_table2to authenticated3using ( (select auth.uid()) = user_id );
```

You can add an index like:

```javascript
1create index userid2on test_table3using btree (user_id);
```


### Benchmarks#

### Call functions withselect#
You can useselectstatement to improve policies that use functions. For example, instead of this:

```javascript
1create policy "rls_test_select" on test_table2to authenticated3using ( auth.uid() = user_id );
```

You can do:

```javascript
1create policy "rls_test_select" on test_table2to authenticated3using ( (select auth.uid()) = user_id );
```

This method works well for JWT functions likeauth.uid()andauth.jwt()as well assecurity definerFunctions. Wrapping the function causes aninitPlanto be run by the Postgres optimizer, which allows it to "cache" the results per-statement, rather than calling the function on each row.

You can only use this technique if the results of the query or function do not change based on the row data.


### Benchmarks#

### Add filters to every query#
Policies are "implicit where clauses," so it's common to runselectstatements without any filters. This is a bad pattern for performance. Instead of doing this (JS client example):

```javascript
1const { data } = supabase2  .from('table')3  .select()
```

You should always add a filter:

```javascript
1const { data } = supabase2  .from('table')3  .select()4  .eq('user_id', userId)
```

Even though this duplicates the contents of the Policy, Postgres can use the filter to construct a better query plan.


### Benchmarks#

### Use security definer functions#
A "security definer" function runs using the same role thatcreatedthe function. This means that if you create a role with a superuser (likepostgres), then that function will havebypassrlsprivileges. For example, if you had a policy like this:

```javascript
1create policy "rls_test_select" on test_table2to authenticated3using (4  exists (5    select 1 from roles_table6    where (select auth.uid()) = user_id and role = 'good_role'7  )8);
```

We can instead create asecurity definerfunction which can scanroles_tablewithout any RLS penalties:

```javascript
1create function private.has_good_role()2returns boolean3language plpgsql4security definer -- will run as the creator5as $$6begin7  return exists (8    select 1 from roles_table9    where (select auth.uid()) = user_id and role = 'good_role'10  );11end;12$$;1314-- Update our policy to use this function:15create policy "rls_test_select"16on test_table17to authenticated18using ( (select private.has_good_role()) );
```

Security-definer functions should never be created in a schema in the "Exposed schemas" inside yourAPI settings`.


### Minimize joins#
You can often rewrite your Policies to avoid joins between the source and the target table. Instead, try to organize your policy to fetch all the relevant data from the target table into an array or set, then you can use anINorANYoperation in your filter.

For example, this is an example of a slow policy which joins the sourcetest_tableto the targetteam_user:

```javascript
1create policy "rls_test_select" on test_table2to authenticated3using (4  (select auth.uid()) in (5    select user_id6    from team_user7    where team_user.team_id = team_id -- joins to the source "test_table.team_id"8  )9);
```

We can rewrite this to avoid this join, and instead select the filter criteria into a set:

```javascript
1create policy "rls_test_select" on test_table2to authenticated3using (4  team_id in (5    select team_id6    from team_user7    where user_id = (select auth.uid()) -- no join8  )9);
```

In this case you can also considerusing asecurity definerfunctionto bypass RLS on the join table:

If the list exceeds 1000 items, a different approach may be needed or you may need to analyze the approach to ensure that the performance is acceptable.


### Benchmarks#

### Specify roles in your policies#
Always use the Role of inside your policies, specified by theTOoperator. For example, instead of this query:

```javascript
1create policy "rls_test_select" on rls_test2using ( auth.uid() = user_id );
```

Use:

```javascript
1create policy "rls_test_select" on rls_test2to authenticated3using ( (select auth.uid()) = user_id );
```

This prevents the policy( (select auth.uid()) = user_id )from running for anyanonusers, since the execution stops at theto authenticatedstep.


### Benchmarks#

### More resources#
- Testing your database
- RLS Guide and Best Practices
- Community repo on testing RLS usingpgTAP and dbdev


================================================================================


# Replicate to another Postgres database using Logical Replication
Source: https://supabase.com/docs/guides/database/postgres/setup-replication-external

Replicate to another Postgres database using Logical Replication

For this example, you will need:

- A Supabase project
- A Postgres database (running v10 or newer)

You will be running commands on both of these databases to publish changes from the Supabase database to the external database.

```javascript
1CREATE PUBLICATION example_pub;
```

```javascript
1select pg_create_logical_replication_slot('example_slot', 'pgoutput');
```

This needs adirectconnection (not a Connection Pooler) to your database and you can find the connection info in theConnectpanelin theDirect connectionsection.

You will also need to ensure that IPv6 is supported by your replication destination (or you can enable theIPv4 add-on)

If you would prefer not to use thepostgresuser, then you can runCREATE ROLE <user> WITH REPLICATION;using thepostgresuser.

```javascript
1CREATE SUBSCRIPTION example_sub2CONNECTION 'host=db.oaguxblfdassqxvvwtfe.supabase.co user=postgres password=YOUR_PASS dbname=postgres'3PUBLICATION example_pub4WITH (copy_data = true, create_slot=false, slot_name=example_slot);
```

For projects running Postgres 17+, it is possible to subscribe to aRead
Replicaby using your Read Replica's connection string.

create_slotis set tofalsebecauseslot_nameis provided and the slot was already created in Step 2.
To copy data from before the slot was created, setcopy_datatotrue.

```javascript
1ALTER PUBLICATION example_pub ADD TABLE example_table;
```

```javascript
1select * from pg_stat_replication;
```

You can add more tables to the initial publication, but you're going to need to do a REFRESH on the subscribing database.
Seehttps://www.postgresql.org/docs/current/sql-alterpublication.html


================================================================================


# Timeouts
Source: https://supabase.com/docs/guides/database/postgres/timeouts

Timeouts


### Extend database timeouts to execute longer transactions
Extend database timeouts to execute longer transactions

Dashboard andClientqueries have a max-configurable timeout of 60 seconds. For longer transactions, useSupavisor or direct connections.


### Change Postgres timeout#
You can change the Postgres timeout at the:


### Session level#
Session level settings persist only for the duration of the connection.

Set the session timeout by running:

```javascript
1set statement_timeout = '10min';
```

Because it applies to sessions only, it can only be used with connections through Supavisor in session mode (port 5432) or a direct connection. It cannot be used in the Dashboard, with the Supabase Client API, nor with Supavisor in Transaction mode (port 6543).

This is most often used for single, long running, administrative tasks, such as creating an HSNW index. Once the setting is implemented, you can view it by executing:

```javascript
1SHOW statement_timeout;
```

See the full guide onchanging session timeouts.


### Function level#
This works with the Database REST API when called from the Supabase client libraries:

```javascript
1create or replace function myfunc()2returns void as $$3 select pg_sleep(3); -- simulating some long-running process4$$5language sql6set statement_timeout TO '4s'; -- set custom timeout
```

This is mostly for recurring functions that need a special exemption for runtimes.


### Role level#
This sets the timeout for a specific role.

The default role timeouts are:

- anon: 3s
- authenticated: 8s
- service_role: none (defaults to theauthenticatorrole's 8s timeout if unset)
- postgres: none (capped by default global timeout to be 2min)

Run the following query to change a role's timeout:

```javascript
1alter role example_role set statement_timeout = '10min'; -- could also use seconds '10s'
```

If you are changing the timeout for the Supabase Client API calls, you will need to reload PostgREST to reflect the timeout changes by running the following script:

```javascript
1NOTIFY pgrst, 'reload config';
```

Unlike global settings, the result cannot be checked withSHOW statement_timeout. Instead, run:

```javascript
1select2  rolname,3  rolconfig4from pg_roles5where6  rolname in (7    'anon',8    'authenticated',9    'postgres',10    'service_role'11    -- ,<ANY CUSTOM ROLES>12  );
```


### Global level#
This changes the statement timeout for all roles and sessions without an explicit timeout already set.

```javascript
1alter database postgres set statement_timeout TO '4s';
```

Check if your changes took effect:

```javascript
1show statement_timeout;
```

Although not necessary, if you are uncertain if a timeout has been applied, you can run a quick test:

```javascript
1create or replace function myfunc()2returns void as $$3  select pg_sleep(601); -- simulating some long-running process4$$5language sql;
```


### Identifying timeouts#
The Supabase Dashboard contains tools to help you identify timed-out and long-running queries.


### Using the Logs Explorer#
Go to theLogs Explorer, and run the following query to identify timed-out events (statement timeout) and queries that successfully run for longer than 10 seconds (duration).

```javascript
1select2  cast(postgres_logs.timestamp as datetime) as timestamp,3  event_message,4  parsed.error_severity,5  parsed.user_name,6  parsed.query,7  parsed.detail,8  parsed.hint,9  parsed.sql_state_code,10  parsed.backend_type11from12  postgres_logs13  cross join unnest(metadata) as metadata14  cross join unnest(metadata.parsed) as parsed15where16  regexp_contains(event_message, 'duration|statement timeout')17  -- (OPTIONAL) MODIFY OR REMOVE18  and parsed.user_name = 'authenticator' -- <--------CHANGE19order by timestamp desc20limit 100;
```


### Using the Query Performance page#
Go to theQuery Performance pageand filter by relevant role and query speeds. This only identifies slow-running but successful queries. Unlike the Log Explorer, it does not show you timed-out queries.


### Understanding roles in logs#
Each API server uses a designated user for connecting to the database:

Filter by theparsed.user_namefield to only retrieve logs made by specific users:

```javascript
1-- find events based on role/server2... query3where4  -- find events from the relevant role5  parsed.user_name = '<ROLE>'
```


================================================================================


# Postgres Triggers
Source: https://supabase.com/docs/guides/database/postgres/triggers

Postgres Triggers


### Automatically execute SQL on table events.
Automatically execute SQL on table events.

In Postgres, a trigger executes a set of actions automatically on table events such as INSERTs, UPDATEs, DELETEs, or TRUNCATE operations.


### Creating a trigger#
Creating triggers involve 2 parts:

An example of a trigger is:

```javascript
1create trigger "trigger_name"2after insert on "table_name"3for each row4execute function trigger_function();
```


### Trigger functions#
A trigger function is a user-definedFunctionthat Postgres executes when the trigger is fired.


### Example trigger function#
Here is an example that updatessalary_logwhenever an employee's salary is updated:

```javascript
1-- Example: Update salary_log when salary is updated2create function update_salary_log()3returns trigger4language plpgsql5as $$6begin7  insert into salary_log(employee_id, old_salary, new_salary)8  values (new.id, old.salary, new.salary);9  return new;10end;11$$;1213create trigger salary_update_trigger14after update on employees15for each row16execute function update_salary_log();
```


### Trigger variables#
Trigger functions have access to several special variables that provide information about the context of the trigger event and the data being modified. In the example above you can see the values inserted into the salary log areold.salaryandnew.salary- in this caseoldspecifies the previous values andnewspecifies the updated values.

Here are some of the key variables and options available within trigger functions:

- TG_NAME: The name of the trigger being fired.
- TG_WHEN: The timing of the trigger event (BEFOREorAFTER).
- TG_OP: The operation that triggered the event (INSERT,UPDATE,DELETE, orTRUNCATE).
- OLD: A record variable holding the old row's data inUPDATEandDELETEtriggers.
- NEW: A record variable holding the new row's data inUPDATEandINSERTtriggers.
- TG_LEVEL: The trigger level (ROWorSTATEMENT), indicating whether the trigger is row-level or statement-level.
- TG_RELID: The object ID of the table on which the trigger is being fired.
- TG_TABLE_NAME: The name of the table on which the trigger is being fired.
- TG_TABLE_SCHEMA: The schema of the table on which the trigger is being fired.
- TG_ARGV: An array of string arguments provided when creating the trigger.
- TG_NARGS: The number of arguments in theTG_ARGVarray.


### Types of triggers#
There are two types of trigger,BEFOREandAFTER:


### Trigger before changes are made#
Executes before the triggering event.

```javascript
1create trigger before_insert_trigger2before insert on orders3for each row4execute function before_insert_function();
```


### Trigger after changes are made#
Executes after the triggering event.

```javascript
1create trigger after_delete_trigger2after delete on customers3for each row4execute function after_delete_function();
```


### Execution frequency#
There are two options available for executing triggers:

- for each row: specifies that the trigger function should be executed once for each affected row.
- for each statement: the trigger is executed once for the entire operation (for example, once on insert). This can be more efficient thanfor each rowwhen dealing with multiple rows affected by a single SQL statement, as they allow you to perform calculations or updates on groups of rows at once.


### Dropping a trigger#
You can delete a trigger using thedrop triggercommand:

```javascript
1drop trigger "trigger_name" on "table_name";
```

If your trigger is inside a restricted schema, you won't be able to drop it due to permission restrictions. In those cases, you can drop the function it depends on instead using the CASCADE clause to automatically remove all triggers that call it:

```javascript
1drop function if exists restricted_schema.function_name() cascade;
```

Make sure you take a backup of the function before removing it in case you're planning to recreate it later.


### Resources#
- Official Postgres Docs:Triggers
- Official Postgres Docs:Overview of Trigger Behavior
- Official Postgres Docs:CREATE TRIGGER


================================================================================


# Print PostgreSQL version
Source: https://supabase.com/docs/guides/database/postgres/which-version-of-postgres

Print PostgreSQL version

It's important to know which version of Postgres you are running as each major version has different features and may cause breaking changes. You may also need to update your schema whenupgradingor downgrading to a major Postgres version.

Run the following query using theSQL Editorin the Supabase Dashboard:

```javascript
1select2  version();
```

Which should return something like:

```javascript
1PostgreSQL 15.1 on aarch64-unknown-linux-gnu, compiled by gcc (Ubuntu 10.3.0-1ubuntu1~20.04) 10.3.0, 64-bit
```

This query can also be executed viapsqlor any other query editor if you prefer toconnect directly to the database.


================================================================================


# Prisma
Source: https://supabase.com/docs/guides/database/prisma

Prisma

This quickly shows how to connect your Prisma application to Supabase Postgres. If you encounter any problems, reference thePrisma troubleshooting docs.

If you plan to solely use Prisma instead of the Supabase Data API (PostgREST), turn it off in theAPI Settings.


### Create a custom user for Prisma
- In theSQL Editor, create a Prisma DB user with full privileges on the public schema.
- This gives you better control over Prisma's access and makes it easier to monitor using Supabase tools like theQuery Performance DashboardandLog Explorer.

For security, consider using apassword generatorfor the Prisma role.

```javascript
1-- Create custom user2create user "prisma" with password 'custom_password' bypassrls createdb;34-- extend prisma's privileges to postgres (necessary to view changes in Dashboard)5grant "prisma" to "postgres";67-- Grant it necessary permissions over the relevant schemas (public)8grant usage on schema public to prisma;9grant create on schema public to prisma;10grant all on all tables in schema public to prisma;11grant all on all routines in schema public to prisma;12grant all on all sequences in schema public to prisma;13alter default privileges for role postgres in schema public grant all on tables to prisma;14alter default privileges for role postgres in schema public grant all on routines to prisma;15alter default privileges for role postgres in schema public grant all on sequences to prisma;
```

```javascript
1-- alter prisma password if needed2alter user "prisma" with password 'new_password';
```


### Create a Prisma Project
Create a new Prisma Project on your computer

Create a new directory

```javascript
1mkdir hello-prisma2cd hello-prisma
```

Initiate a new Prisma project

```javascript
1npm init -y2npm install prisma typescript ts-node @types/node --save-dev34npx tsc --init56npx prisma init
```


### Add your connection information to your .env file
- On your project dashboard, clickConnect
- Find your Supavisor Session pooler string. It should end with 5432. It will be used in your.envfile.

If you're in anIPv6 environmentor have the IPv4 Add-On, you can use the direct connection string instead of Supavisor in Session mode.

- If you plan on deploying Prisma to a serverless or auto-scaling environment, you'll also need your Supavisor transaction mode string.
- The string is identical to the session mode string but uses port 6543 at the end.

In your .env file, set the DATABASE_URL variable to your connection string

```javascript
1# Used for Prisma Migrations and within your application2DATABASE_URL="postgres://[DB-USER].[PROJECT-REF]:[PRISMA-PASSWORD]@[DB-REGION].pooler.supabase.com:5432/postgres"
```

Change your string's[DB-USER]toprismaand add the password you created in step 1

```javascript
1postgres://prisma.[PROJECT-REF]...
```


### Create your migrations
If you have already modified your Supabase database, synchronize it with your migration file. Otherwise create new tables for your database

Create new tables in your prisma.schema file

```javascript
1model Post {2  id        Int     @id @default(autoincrement())3  title     String4  content   String?5  published Boolean @default(false)6  author    User?   @relation(fields: [authorId], references: [id])7  authorId  Int?8}910model User {11  id    Int     @id @default(autoincrement())12  email String  @unique13  name  String?14  posts Post[]15}
```

commit your migration

```javascript
1npx prisma migrate dev --name first_prisma_migration
```


### Install the prisma client
Install the Prisma client and generate its model

```javascript
1npm install @prisma/client2npx prisma generate
```


### Test your API
Create a index.ts file and run it to test your connection

```javascript
1const { PrismaClient } = require('@prisma/client');23const prisma = new PrismaClient();45async function main() {6  //change to reference a table in your schema7  const val = await prisma.<SOME_TABLE_NAME>.findMany({8    take: 10,9  });10  console.log(val);11}1213main()14  .then(async () => {15    await prisma.$disconnect();16  })17  .catch(async (e) => {18    console.error(e);19    await prisma.$disconnect();20  process.exit(1);21});
```


================================================================================


# Troubleshooting prisma errors
Source: https://supabase.com/docs/guides/database/prisma/prisma-troubleshooting

Troubleshooting prisma errors

This guide addresses common Prisma errors that you might encounter while using Supabase.

A full list of errors can be found inPrisma's official docs.


### Understanding connection string parameters:#
Unlike other libraries, Prisma lets you configureits settingsthrough special options appended to your connection string.

These options, called "query parameters," can be used to address specific errors.

```javascript
1# Example of query parameters23connection_string.../postgres?KEY1=VALUE&KEY2=VALUE&KEY3=VALUE
```


### ... prepared statement already exists#
Supavisor in transaction mode (port 6543) does not supportprepared statements, which Prisma will try to create in the background.


### Solution:#
- Addpgbouncer=trueto the connection string. This turns off prepared statements in Prisma.

```javascript
1.../postgres?pgbouncer=true
```


### Can't reach database server at:#
Prisma couldn't establish a connection with Postgres or Supavisor before the timeout


### Possible causes:#
- Database overload: The database server is under heavy load, causing Prisma to struggle to connect.
- Malformed connection string: The connection string used by Prisma is incorrect or incomplete.
- Transient network issues: Temporary network problems are disrupting the connection.


### Solutions:#
- Check database health: Use theObservability Dashboardto monitor CPU, memory, and I/O usage. If the database is overloaded, consider increasing yourcompute sizeoroptimizing your queries.
- Verify connection string: Double-check the connection string in your Prisma configuration to ensure it matches in yourproject connect page.
- Increase connection timeout: Try increasing theconnect_timeoutparameter in your Prisma configuration to give it more time to establish a connection.

```javascript
1.../postgres?connect_timeout=30
```


### Timed out fetching a new connection from the connection pool:#
Prisma is unable to allocate connections to pending queries fast enough to meet demand.


### Possible causes:#
- Overwhelmed server: The server hosting Prisma is under heavy load, limiting its ability to manage connections. By default, Prisma will create the defaultnum_cpus * 2 + 1worth of connections. A common cause for server strain is increasing theconnection_limitsignificantly past the default.
- Insufficient pool size: The Supavisor pooler does not have enough connections available to quickly satisfy Prisma's requests.
- Slow queries: Prisma's queries are taking too long to execute, preventing it from releasing connections for reuse.


### Solutions:#
- Increase the pool timeout: Increase thepool_timeoutparameter in your Prisma configuration to give the pooler more time to allocate connections.
- Reduce the connection limit: If you've explicitly increased theconnection_limitparameter in your Prisma configuration, try reducing it to a more reasonable value.
- Increase pool size: If you are connecting with Supavisor, try increasing the pool size in theDatabase Settings.
- Optimize queries:Improve the efficiency of your queriesto reduce execution time.
- Increase compute size: Like the preceding option, this is a strategy to reduce query execution time.


### Server has closed the connection#
According to thisGitHub Issue for Prisma, this error may be related to large return values for queries. It may also be caused by significant database strain.


### Solutions:#
- Limit row return sizes: Try to limit the total amount of rows returned for particularly large requests.
- Minimize database strain:Check the Reports Page for database strain. If there is obvious strain, consideroptimizingor increasing compute size


### Drift detected: Your database schema is not in sync with your migration history#
Prisma relies on migration files to ensure your database aligns with Prisma's model. External schema changes are detected as "drift", which Prisma will try to overwrite, potentially causing data loss.


### Possible causes:#
- Supabase Managed Schemas: Supabase may update managed schemas like auth and storage to introduce new features. Granting Prisma access to these schemas can lead to drift during updates.
- External Schema Modifications: Your team or another tool might have modified the database schema outside of Prisma, causing drift.


### Solution:#
- Baselining migrations:baseliningre-syncs Prisma by capturing the current database schema as the starting point for future migrations.


### Max client connections reached#
Postgres or Supavisor rejected a request for more connections


### Possible causes:#
- When working in transaction mode (port 6543):The error "Max client connections reached" occurs when clients try to form more connections with the pooler than it can support.
- When working in session mode (port 5432):The max amount of clients is restricted to the "Pool Size" value in theDatabase Settings. If the "Pool Size" is set to 15, even if the pooler can handle 200 client connections, it will still be effectively capped at 15 for each unique"database-role+database" combination.
- When working with direct connections: Postgres is already servicing the max amount of connections


### Solutions#
- Transaction Mode for serverless apps: If you are using serverless functions (Supabase Edge, Vercel, AWS Lambda), switch to transaction mode (port 6543). It handles more connections than session mode or direct connections.
- Reduce the number of Prisma connections: A single client-server can establish multiple connections with a pooler. Typically, serverless setups do not need many connections. Starting with fewer, like five or three, or even just one, is often sufficient. In serverless setups, begin withconnection_limit=1, increasing cautiously if needed to avoid maxing out connections.
- Increase pool size: If you are connecting with Supavisor, try increasing the pool size in theDatabase Settings.
- Disconnect appropriately: Close Prisma connections when they are no longer needed.
- Decrease query time: Reduce query complexity or addstrategic indexesto your tables to speed up queries.
- Increase compute size: Sometimes the best option is to increase your compute size, which also increases your max client size and query execution speed


### Cross schema references are only allowed when the target schema is listed in the schemas property of your data-source#
A Prisma migration is referencing a schema it is not permitted to manage.


### Possible causes:#
- A migration references a schema that Prisma is not permitted to manage


### Solutions:#
- Multi-Schema support: If the external schema isn't Supabase managed, modify yourprisma.schemafile to enable the multi-Schema preview

```javascript
1generator client {2  provider        = "prisma-client-js"3  previewFeatures = ["multiSchema"]  //Add line4}56datasource db {7  provider  = "postgresql"8  url       = env("DATABASE_URL")9  directUrl = env("DIRECT_URL")10  schemas   = ["public", "other_schema"] //list out relevant schemas11}
```

- Supabase managed schemas: Schemas managed by Supabase, such asauthandstorage, may be changed to support new features. Referencing these schemas directly will cause schema drift in the future. It is best to remove references to these schemas from your migrations.

An alternative strategy to reference these tables is to duplicate values into Prisma managed table with triggers. Below is an example for duplicating values fromauth.usersinto a table calledprofiles.

```javascript
1-- Create the 'profiles' table in the 'public' schema2create table public.profiles (3  id uuid primary key,             -- 'id' is a UUID and the primary key for the table4  email varchar(256)               -- 'email' is a variable character field with a maximum length of 256 characters5);
```

```javascript
1-- Function to handle the insertion of a new user into the 'profiles' table2create function public.handle_new_user()3returns trigger4language plpgsql5security definer set search_path = ''6as $$7begin89  -- Insert the new user's data into the 'profiles' table10  insert into public.profiles (id, email)11  values (new.id, new.email);1213  return new;     -- Return the new record14end;15$$;
```

```javascript
1-- Function to handle the updating of a user's information in the 'profiles' table2create function public.update_user()3returns trigger4language plpgsql5security definer set search_path = ''6as7$$8begin9  -- Update the user's data in the 'profiles' table10  update public.profiles11  set email = new.email     -- Update the 'email' field12  where id = new.id;        -- Match the 'id' field with the new record1314  return new;  -- Return the new record15end;16$$;
```

```javascript
1-- Function to handle the deletion of a user from the 'profiles' table2create function public.delete_user()3returns trigger4language plpgsql5security definer set search_path = ''6as7$$8begin9  -- Delete the user's data from the 'profiles' table10  delete from public.profiles11  where id = old.id;  -- Match the 'id' field with the old record1213  return old;  -- Return the old record14end;15$$;
```

```javascript
1-- Trigger to run 'handle_new_user' function after a new user is inserted into 'auth.users' table2create trigger on_auth_user_created3  after insert on auth.users4  for each row execute procedure public.handle_new_user();56-- Trigger to run 'update_user' function after a user is updated in the 'auth.users' table7create trigger on_auth_user_updated8  after update on auth.users9  for each row execute procedure public.update_user();1011-- Trigger to run 'delete_user' function after a user is deleted from the 'auth.users' table12create trigger on_auth_user_deleted13  after delete on auth.users14  for each row execute procedure public.delete_user();
```


================================================================================


# Connecting with PSQL
Source: https://supabase.com/docs/guides/database/psql

Connecting with PSQL

psqlis a command-line tool that comes with Postgres.


### Connecting with SSL#
You should connect to your database using SSL wherever possible, to prevent snooping and man-in-the-middle attacks.

You can obtain your connection info and Server root certificate from your application's dashboard:



Download yourSSL certificateto/path/to/prod-supabase.cer.

Find your connection settings. Go to the projectConnectpaneland copy the URL from theSession poolersection, and copy the parameters into the connection string:

```javascript
1psql "sslmode=verify-full sslrootcert=/path/to/prod-supabase.cer host=[CLOUD_PROVIDER]-0-[REGION].pooler.supabase.com dbname=postgres user=postgres.[PROJECT_REF]"
```


================================================================================


# Query Optimization
Source: https://supabase.com/docs/guides/database/query-optimization

Query Optimization


### Choosing indexes to improve your query performance.
Choosing indexes to improve your query performance.

When working with Postgres, or any relational database, indexing is key to improving query performance. Aligning indexes with common query patterns can speed up data retrieval by an order of magnitude.

This guide is intended to:

- help identify parts of a query that have the potential to be improved by indexes
- introduce tooling to help identify useful indexes

This is not a comprehensive resource, but rather a helpful starting point for your optimization journey.

If you're new to query optimization, you may be interested inindex_advisor, our tool for automatically detecting indexes that improve performance on a given query.


### Example query#
Consider the following example query that retrieves customer names and purchase dates from two tables:

```javascript
1select2  a.name,3  b.date_of_purchase4from5  customers as a6  join orders as b on a.id = b.customer_id7where a.sign_up_date > '2023-01-01' and b.status = 'shipped'8order by b.date_of_purchase9limit 10;
```

In this query, there are several parts that indexes could likely help in optimizing the performance:


### whereclause:#
Thewhereclause filters rows based on certain conditions, and indexing the columns involved can improve this process:

- a.sign_up_date: If filtering bysign_up_dateis common, indexing this column can speed up the query.
- b.status: Indexing the status may be beneficial if the column has diverse values.

```javascript
1create index idx_customers_sign_up_date on customers (sign_up_date);23create index idx_orders_status on orders (status);
```


### joincolumns#
Indexes on the columns used for joining tables can help Postgres avoid scanning tables in their entirety when connecting tables.

- Indexinga.idandb.customer_idwould likely improve the performance of the join in this query.
- Note that ifa.idis the primary key of thecustomerstable it is already indexed

```javascript
1create index idx_orders_customer_id on orders (customer_id);
```


### order byclause#
Sorting can also be optimized by indexing:

- An index onb.date_of_purchasecan improve the sorting process, and is particularly beneficial when a subset of rows is being returned with alimitclause.

```javascript
1create index idx_orders_date_of_purchase on orders (date_of_purchase);
```


### Key concepts#
Here are some concepts and tools to keep in mind to help you identify the best index for the job, and measure the impact that your index had:


### Analyze the query plan#
Use theexplaincommand to understand the query's execution. Look for slow parts, such as Sequential Scans or high cost numbers. If creating an index does not reduce the cost of the query plan, remove it.

For example:

```javascript
1explain select * from customers where sign_up_date > 25;
```


### Use appropriate index types#
Postgres offers various index types likeB-tree, Hash, GIN, etc. Select the type that best suits your data and query pattern. Using the right index type can make a significant difference. For example, using a BRIN index on a field that always increases and lives within a table that updates infrequently - likecreated_aton anorderstable - routinely results in indexes that are +10x smaller than the equivalent default B-tree index. That translates into better scalability.

```javascript
1create index idx_orders_created_at ON customers using brin(created_at);
```


### Partial indexes#
For queries that frequently target a subset of data, a partial index could be faster and smaller than indexing the entire column. A partial index contains awhereclause to filter the values included in the index. Note that a query'swhereclause must match the index for it to be used.

```javascript
1create index idx_orders_status on orders (status)2where status = 'shipped';
```


### Composite indexes#
If filtering or joining on multiple columns, a composite index prevents Postgres from referring to multiple indexes when identifying the relevant rows.

```javascript
1create index idx_customers_sign_up_date_priority on customers (sign_up_date, priority);
```


### Over-Indexing#
Avoid the urge to index columns you operate on infrequently. While indexes can speed up reads, they also slow down writes, so it's important to balance those factors when making indexing decisions.


### Statistics#
Postgres maintains a set of statistics about the contents of your tables. Those statistics are used by the query planner to decide when it's is more efficient to use an index vs scanning the entire table. If the collected statistics drift too far from reality, the query planner may make poor decisions. To avoid this risk, you can periodicallyanalyzetables.

```javascript
1analyze customers;
```

By following this guide, you'll be able to discern where indexes can optimize queries and enhance your Postgres performance. Remember that each database is unique, so always consider the specific context and use case of your queries.


================================================================================


# Database Replication
Source: https://supabase.com/docs/guides/database/replication

Database Replication


### An introduction to database replication and change data capture.
An introduction to database replication and change data capture.

Replication is the process of copying changes from your database to another location. It's also referred to as change data capture (CDC): capturing all the changes that occur to your data.


### Use cases#
You might use database replication for:

- Analytics and Data Warehousing: Replicate your operational database to analytics platforms for complex analysis without impacting your application's performance.
- Data Integration: Keep your data synchronized across different systems and services in your tech stack.
- Backup and Disaster Recovery: Maintain up-to-date copies of your data in different locations.


### Replication methods#
Supabase supports three methods for replicating your database to external destinations:


### Read Replicas#
Additional databases that are kept in sync with your Primary database. These read-only databases can be deployed across multiple regions, for lower latency and better resource management.

- Set up Read Replicas


### Replication#
Replication is currently in private alpha. Access is limited and features may change.

Replication powered bySupabase ETLautomatically replicates data to supported systems.

- Set up replication


### Manual replication#
Configure your own replication using external tools and Postgres's native logical replication. This gives you full control over the replication process and allows you to use any tool that supports Postgres logical replication.

- Set up Manual Replication


### Related features#
Choose the data syncing method based on your use case:

- For realtime features and syncing data to clients (browsers, mobile apps), seeRealtime
- For deploying read-only databases across multiple regions, seeRead Replicas


### Concepts and terms#

### Write-Ahead Log (WAL)#
Postgres uses a system called the Write-Ahead Log (WAL) to manage changes to the database. As you make changes, they are appended to the WAL, which is a series of files (also called "segments") where the file size can be specified. Once one segment is full, Postgres will start appending to a new segment. After a period of time, a checkpoint occurs and Postgres synchronizes the WAL with your database. Once the checkpoint is complete, then the WAL files can be removed from disk and free up space.


### Logical replication and WAL#
Logical replication is a method of replication where Postgres uses the WAL files and transmit those changes to another Postgres database, or a system that supports reading WAL files.


### LSN#
LSN is a Log Sequence Number that is used to identify the position of a WAL file in the WAL directory. It is often used to determine the progress of replication in subscribers and calculate the lag of a replication slot.


### Logical replication architecture#
When setting up logical replication, three key components are involved:

- publication- A set of tables on your primary database that will bepublished
- replication slot- A slot used for replicating the data from a single publication. The slot, when created, will specify the output format of the changes
- subscription- A subscription is created from an external system (i.e. another Postgres database) and must specify the name of thepublication. If you do not specify a replication slot, one is automatically created


### Logical replication output format#
Logical replication is typically output in 2 forms,pgoutputandwal2json. The output method is how Postgres sends changes to any active replication slot.


### Logical replication configuration#
When using logical replication, Postgres is then configured to keep WAL files around for longer than it needs them. If the files are removed too quickly, then yourreplication slotwill become inactive and, if the database receives a large number of changes in a short time, then thereplication slotcan become lost as it was not able to keep up.

In order to mitigate this, Postgres has many options and settings that can betweakedto manage the WAL usage effectively. Not all of these settings are user configurable as they can impact the stability of your database. For those that are, these should be considered as advanced configuration and not changed without understanding that they can cause additional disk space and resources to be used, as well as incur additional costs.


================================================================================


# Manual Replication FAQ
Source: https://supabase.com/docs/guides/database/replication/manual-replication-faq

Manual Replication FAQ


### Common questions and considerations when setting up manual replication.
Common questions and considerations when setting up manual replication.


### Which connection string should be used?#
Always use the direct connection string for logical replication.

Connections through a pooler, such as Supavisor, will not work.


### The tool in use does not support IPv6#
You can enable theIPv4 add-onfor your project.


### What is XMIN and should it be used?#
Xmin is a different form of replication from logical replication and should only be used if logical replication is not available for your database (i.e. older versions of Postgres).

Xmin performs replication by checking thexmin system columnand determining if that row has already been synchronized.

It does not capture deletion of data and isnot recommended, particularly for larger databases.


### Can replication be configured in the Dashboard?#
You can viewpublicationsin the Dashboard but all steps to configure replication must be done using theSQL Editoror a CLI tool of your choice.


### How to configure database settings for replication?#
Using the Supabase CLI, you canconfigure database settingsto optimize them for your replication needs. These values can vary depending on your database size and activity.


### What are some important configuration options?#
Some of the more important options to be aware of are:

- max_wal_size- Maximum size the WAL can grow between automatic WAL checkpoints
- max_slot_wal_keep_size- Maximum size of WAL files that replication slots are allowed to retain
- wal_keep_size- Minimum number of past WAL files to keep for standby servers
- max_wal_senders- Maximum number of concurrent connections from standby servers or streaming backup clients

These settings help ensure your replication slots don't run out of space and that replicas can reconnect without requiring a full re-sync.


================================================================================


# Manual Replication Monitoring
Source: https://supabase.com/docs/guides/database/replication/manual-replication-monitoring

Manual Replication Monitoring


### Track replication health and performance.
Track replication health and performance.

Monitoring replication lag is important and there are 3 ways to do this:

- pg_stat_subscription (subscriber) - if PID is null, then the subscription is not active
- pg_stat_subscription_stats - look here for error_count to see if there were issues applying or syncing (if yes, check the logs for why)
- pg_replication_slots - use this to check if the slot is active and you can also calculate the lag from here

- replication_slots_max_lag_bytes - this is the more important one
- pg_stat_replication_replay_lag - lag to replay WAL files from the source DB on the target DB (throttled by disk or high activity)
- pg_stat_replication_send_lag - lag in sending WAL files from the source DB (a high lag means that the publisher is not being asked to send new WAL files OR network issues)


### Primary#

### Replication status and lag#
Thepg_stat_replicationtable shows the status of any replicas connected to the primary database.

```javascript
1select pid, application_name, state, sent_lsn, write_lsn, flush_lsn, replay_lsn, sync_state2from pg_stat_replication;
```


### Replication slot status#
A replication slot can be in one of three states:

- active- The slot is active and is receiving data
- inactive- The slot is not active and is not receiving data
- lost- The slot is lost and is not receiving data

The state can be checked using thepg_replication_slotstable:

```javascript
1select slot_name, active, state from pg_replication_slots;
```


### WAL size#
The WAL size can be checked using thepg_ls_waldir()function:

```javascript
1select * from pg_ls_waldir();
```


### Check the LSN#
```javascript
1select pg_current_wal_lsn();
```


### Subscriber#

### Subscription status#
Thepg_subscriptiontable shows the status of any subscriptions on a replica and thepg_subscription_reltable shows the status of each table within a subscription.

Thesrsubstatecolumn inpg_subscription_relcan be one of the following:

- i- Initializing - The subscription is being initialized
- d- Data Synchronizing - The subscription is synchronizing data for the first time (i.e. doing the initial copy)
- s- Synchronized - The subscription is synchronized
- r- Replicating - The subscription is replicating data

```javascript
1SELECT2    sub.subname AS subscription_name,3    relid::regclass AS table_name,4    srel.srsubstate AS replication_state,5    CASE srel.srsubstate6        WHEN 'i' THEN 'Initializing'7        WHEN 'd' THEN 'Data Synchronizing'8        WHEN 's' THEN 'Synchronized'9        WHEN 'r' THEN 'Replicating'10        ELSE 'Unknown'11    END AS state_description,12    srel.srsyncedlsn AS last_synced_lsn13FROM14    pg_subscription sub15JOIN16    pg_subscription_rel srel ON sub.oid = srel.srsubid17ORDER BY18    table_name;
```


### Check the LSN#
```javascript
1select pg_last_wal_replay_lsn();
```


================================================================================


# Manual Replication Setup
Source: https://supabase.com/docs/guides/database/replication/manual-replication-setup

Manual Replication Setup


### Set up replication with Airbyte, Estuary, Fivetran, and other tools.
Set up replication with Airbyte, Estuary, Fivetran, and other tools.

This guide covers setting upmanual logical replicationusing external tools. If you prefer a simpler, managed solution, readthe Replication setup docsinstead.

This guide is for replicating data to external systems using your own tools. For deploying read-only databases across multiple regions, seeRead Replicasinstead.


### Prerequisites#
To set up replication, the following is recommended:

- Instance size of XL or greater
- IPv4 add-onenabled

To create a replication slot, you will need to use thepostgresuser and follow the instructions in theexternal replication setup guide.

If you are running Postgres 17 or higher, you can create a new user and grant them replication permissions with thepostgresuser. For versions below 17, you will need to use thepostgresuser.

If you are replicating to an external system and using any of the tools below, check their documentation first. Additional information is provided where the setup with Supabase can vary.

Estuary has the followingdocumentationfor setting up Postgres as a source.


================================================================================


# Replication FAQ
Source: https://supabase.com/docs/guides/database/replication/replication-faq

Replication FAQ


### Common questions and answers about replication.
Common questions and answers about replication.

Replication is currently in private alpha. Access is limited and features may change.


### What destinations are supported?#
Replication currently supports Analytics Buckets (Iceberg format) and BigQuery. See the destination tabs in theSetup guidefor configuration details.

Availability varies based on the planned roll-out strategy. The destinations you can access depend on your project and access level.


### Why is a table not being replicated?#
Common reasons:

- Missing primary key: Tables must have a primary key to be replicated (Postgres logical replication requirement)
- Not in publication: Ensure the table is included in your Postgres publication
- Unsupported data types: Tables with custom data types are not supported

Check your publication settings and verify your table meets the requirements.


### Why aren't publication changes reflected after adding or removing tables?#
After modifying your Postgres publication, you must restart the replication pipeline for changes to take effect. SeeAdding or removing tablesfor instructions.


### Why is a pipeline in failed state?#
Pipeline failures occur during the streaming phase when an error happens while replicating live data. This prevents data loss. To recover:

SeeHandling errorsfor more details.


### Why is a table in error state?#
Table errors occur during the copy phase. To recover, clickView status, find the affected table, and reset the table state. This will restart the table copy from the beginning.


### How to verify replication is working#
Check theDatabase→replicationpage:

See theReplication Monitoring guidefor comprehensive monitoring instructions.


### How to stop or pause replication#
You can manage your pipeline using the actions menu in the destinations list. SeeManaging your pipelinefor details on available actions.

Note: Stopping replication will cause changes to queue up in the WAL.


### What happens if a table is deleted at the destination?#
If a table is deleted downstream at the destination (e.g., in your Analytics Bucket or BigQuery dataset), the replication pipeline will automatically recreate it.

This behavior is by design to prevent the pipeline from breaking if tables are accidentally deleted. The pipeline ensures that all tables in your publication are always present at the destination.

To permanently remove a table from your destination:

You have two options:

Option 1: Pause the pipeline first

Option 2: Remove from publication first

Note: Removing a table from the publication and restarting the pipeline does not delete the table downstream, it only stops replicating new changes to it.


### Can data duplicates occur during pipeline operations?#
Yes, data duplicates can occur in certain scenarios when stopping a pipeline.

When you stop a pipeline (for restarts or updates), the replication process tries to finish processing any transactions that are currently being sent to your destination. It waits up to a few minutes to allow these in-progress transactions to complete cleanly before stopping.

However, if a transaction in your database takes longer than this waiting period to complete, the pipeline will stop before that entire transaction has been fully processed. When the pipeline starts again, it must restart the incomplete transaction from the beginning to maintain transaction boundaries, which results in some data being sent twice to your destination.

Understanding transaction boundaries: A transaction is a group of database changes that happen together (for example, all changes within aBEGIN...COMMITblock). Postgres logical replication must process entire transactions - it cannot process part of a transaction, stop, and then continue from the middle. This means if a transaction is interrupted, the whole transaction must be replayed when the pipeline resumes.

Example scenario: Suppose you have a batch operation that updates 10,000 rows within a single transaction. If this operation takes 10 minutes to complete and you stop the pipeline after 5 minutes (when 5,000 rows have been processed), the pipeline cannot resume from row 5,001. Instead, when it restarts, it must reprocess all 10,000 rows from the beginning, resulting in the first 5,000 rows being sent to your destination twice.

Important: There are currently no plans to implement automatic deduplication. If your use case requires guaranteed exactly-once delivery, you should implement deduplication logic in your downstream systems based on primary keys or other unique identifiers.


### Where to find replication logs#
Navigate toLogs→Replicationto see all pipeline logs. Logs contain diagnostic information. If you're experiencing issues, contact support with your error details.


### How to get help#
If you need assistance:


================================================================================


# Replication Monitoring
Source: https://supabase.com/docs/guides/database/replication/replication-monitoring

Replication Monitoring


### Track replication status, view logs, and troubleshoot issues.
Track replication status, view logs, and troubleshoot issues.

Replication is currently in private alpha. Access is limited and features may change.

After setting up replication, you can monitor the status and health of your replication pipelines directly from the Supabase Dashboard. The pipeline is the active Postgres replication process that continuously streams changes from your database to your destination.


### Viewing pipeline status#
To monitor your replication pipelines:


### Pipeline states#
Each destination shows its pipeline in one of these states:


### Viewing detailed pipeline metrics#
For detailed information about a specific pipeline, clickView statuson the destination. This opens the pipeline status page where you can monitor replication performance and table states.


### Replication lag metrics#
The status page shows replication lag metrics that help you determine how fast your pipeline is replicating data. These metrics are loaded directly from Postgres itself.


### Table states#
The pipeline status page also shows the state of individual tables being replicated. Each table can be in one of these states:


### Handling errors#
Errors can occur at two levels: per table or per pipeline.


### Table errors#
Table errors occur during the copy phase and affect individual tables. These errors can be retried without stopping the entire pipeline.

Viewing table error details:

Recovering from table errors:

When a table encounters an error during the copy phase, you can reset the table state. This will restart the table copy from the beginning.


### Pipeline errors#
Pipeline errors occur during the streaming phase (Live state) and affect the entire pipeline. When streaming data, if an error occurs, the entire pipeline will stop and enter aFailedstate. This prevents data loss by ensuring no changes are skipped.

When a pipeline error occurs, you'll receive an email notification immediately. This ensures you're promptly notified of any issues so you can take action to resolve them.

Viewing pipeline error details:

Recovering from pipeline errors:

To recover from a pipeline error, you'll need to:


### Viewing logs#
To see detailed logs for all your replication pipelines:

Logs contain diagnostic information that may be too technical for most users. If you're experiencing issues with replication, reaching out to support with your error details is recommended.


### Common monitoring scenarios#

### Checking if replication is healthy#

### Investigating errors#
If you see aFailedstatus:


### Monitoring performance#
To ensure optimal performance:


### Troubleshooting#
If you notice issues with your replication:

For more troubleshooting tips, see theReplication FAQ.


### Next steps#
- Set up replication
- View replication FAQ


================================================================================


# Replication Setup
Source: https://supabase.com/docs/guides/database/replication/replication-setup

Replication Setup


### Configure publications and destinations for replication.
Configure publications and destinations for replication.

Replication is currently in private alpha. Access is limited and features may change.

Replication usesPostgres logical replicationto stream changes from your database. Powered bySupabase ETL, an open source tool built for Postgres logical replication, it provides a managed interface through the Dashboard to configure and monitor replication pipelines.


### Setup overview#
Replication requires two main components: aPostgres publication(defines what to replicate) and adestination(where data is sent). Follow these steps to set up your replication pipeline.

If you already have a Postgres publication set up, you can skip toStep 2: Enable replication.


### Step 1: Create a Postgres publication#
A Postgres publication defines which tables and change types will be replicated from your database. You create publications using SQL.


### Creating a publication#
The following SQL examples assume you haveusersandorderstables in your database.

```javascript
1-- Create publication for both tables2create publication pub_users_orders3for table users, orders;
```

This publication will track all changes (INSERT, UPDATE, DELETE, TRUNCATE) for both theusersandorderstables.

```javascript
1-- Create a publication for all tables in the public schema2create publication pub_all_public for tables in schema public;
```

This will track changes for all existing and future tables in thepublicschema.

```javascript
1-- Create a publication for all tables2create publication pub_all_tables for all tables;
```

This will track changes for all tables in your database.


### Advanced publication options#
You can replicate only a subset of columns from a table:

```javascript
1-- Replicate only specific columns from the users table2create publication pub_users_subset3for table users (id, email, created_at);
```

This will only replicate theid,email, andcreated_atcolumns from theuserstable.

You can filter which rows to replicate using a WHERE clause:

```javascript
1-- Only replicate active users2create publication pub_active_users3for table users where (status = 'active');45-- Only replicate recent orders6create publication pub_recent_orders7for table orders where (created_at > '2024-01-01');
```


### Viewing publications in the Dashboard#
After creating a publication via SQL, you can view it in the Supabase Dashboard:


### Step 2: Enable replication#
Before adding destinations, you need to enable replication for your project:


### Step 3: Add a destination#
Once replication is enabled and you have a Postgres publication, you can add a destination. The destination is where your replicated data will be stored, while the pipeline is the active Postgres replication process that continuously streams changes from your database to that destination.


### Choose and configure your destination#
Follow these steps to configure your destination. The specific configuration depends on which destination type you choose. Both Analytics Buckets and BigQuery destinations are supported, though availability varies based on the planned roll-out strategy.

Analytics Bucketsare specialized storage buckets in Supabase Storage designed for analytical workloads. They provide S3-compatible storage and use theApache Icebergopen table format, making your data accessible via standard tools like DuckDB, Spark, and other analytics platforms.

When you replicate to Analytics Buckets, your database changes are automatically written in Iceberg format, creating tables in object storage that you can query for analytics.

First, create an analytics bucket to store your replicated data:

Navigate toStorage→Analyticsin your Supabase Dashboard

ClickNew bucket

Fill in the bucket details:

- Name: A unique name for your bucket (e.g.,analytics_warehouse)
- Region: Select the region where your data will be stored

ClickCreate bucket

Copy the credentialsdisplayed after bucket creation. You'll need these in the next steps:

- Catalog Token: Authentication token for accessing the Iceberg catalog
- S3 Access Key ID: Access key for S3-compatible storage
- S3 Secret Access Key: Secret key for S3-compatible storage

Navigate toDatabase→replicationin your Supabase Dashboard

ClickAdd destination

Configure the general settings:

- Destination name: A name to identify this destination (e.g., "Analytics Warehouse")
- Publication: The publication to replicate data from (created inStep 1)
- Destination type: SelectAnalytics Buckets

Configure Analytics Buckets settings:

- Bucket: The name of your analytics bucket from Step 1
- Namespace: The schema name where your tables will be replicated (e.g.,public)
- Catalog Token: Authentication token from Step 1
- S3 Access Key ID: Access key from Step 1
- S3 Secret Access Key: Secret key from Step 1

ConfigureAdvanced Settings(optional):

- Batch wait time (milliseconds): How long to wait for more changes before sending a batch. Default is recommended for optimal performance.

ClickCreate and startto begin replication

Your replication pipeline will now start copying data from your database to the analytics bucket in Iceberg format.

Once configured, replication to Analytics Buckets:

Replicated tables use a changelog structure:

- Tables are created with a_changelogsuffix in the name (e.g.,users_changelog)
- Each table contains acdc_operationcolumn indicating the operation type:INSERT,UPDATE,DELETE, orTRUNCATE
- This append-only format preserves the complete history of all changes

- Append-only log: Currently provides an append-only log format rather than a full table representation

- Analytics Buckets documentation- Learn more about Analytics Buckets and S3-compatible storage
- Realtime Data Sync to Analytics Buckets- Step-by-step guide for replicating to Analytics Buckets
- Apache Iceberg- Official Iceberg documentation


### Step 4: Monitor your pipeline#
After creating a destination, the replication pipeline will start and appear in the destinations list. You can monitor the pipeline's status and performance from the Dashboard.

For comprehensive monitoring instructions including pipeline states, metrics, and logs, see theReplication Monitoring guide.


### Managing your pipeline#
You can manage your pipeline from the destinations list using the actions menu.

Available actions:

- Start: Begin replication for a stopped pipeline
- Stop: Pause replication (changes will queue up in the WAL)
- Restart: Stop and start the pipeline (required after publication changes)
- Edit destination: Modify destination settings like credentials or advanced options
- Delete: Remove the destination and permanently stop replication


### Adding or removing tables#
If you need to modify which tables are replicated after your replication pipeline is already running, follow these steps:

If your Postgres publication usesFOR ALL TABLESorFOR TABLES IN SCHEMA, new tables in that scope are automatically included in the publication. However, you stillmust restart the replication pipelinefor the changes to take effect.


### Adding tables to replication#
Add the table to your publication using SQL:

```javascript
1-- Add a single table to an existing publication2alter publication pub_users_orders add table products;34-- Or add multiple tables at once5alter publication pub_users_orders add table products, categories;
```

Restart the replication pipelineusing the actions menu (seeManaging your pipeline) for the changes to take effect.


### Removing tables from replication#
Remove the table from your Postgres publication using SQL:

```javascript
1-- Remove a single table from a publication2alter publication pub_users_orders drop table orders;34-- Or remove multiple tables at once5alter publication pub_users_orders drop table orders, products;
```

Restart the replication pipelineusing the actions menu (seeManaging your pipeline) for the changes to take effect.

Deleted tables are automatically recreated by the pipeline. To permanently delete a table, pause the pipeline first or remove it from the publication before deleting. See theFAQfor details.


### How it works#
Once configured, replication:

Changes are sent in batches to optimize performance and reduce costs. The batch size and timing can be adjusted using the advanced settings. The replication pipeline currently performs data extraction and loading only, without transformation - your data is replicated as-is to the destination.


### Troubleshooting#
If you encounter issues during setup:

- Publication not appearing: Ensure you created the Postgres publication via SQL and refresh the dashboard
- Tables not showing in publication: Verify your tables have primary keys (required for Postgres logical replication)
- Pipeline failed to start: Check the error message in the status view for specific details
- No data being replicated: Verify your Postgres publication includes the correct tables and event types

For more troubleshooting help, see theReplication FAQ.


### Limitations#
Replication has the following limitations:

- Primary keys required: Tables must have primary keys (Postgres logical replication requirement)
- Custom data types: Not supported
- Schema changes: Not automatically handled
- No data transformation: Data is replicated as-is without transformation
- Data duplicates: Duplicates can occur when stopping a pipeline if your database has transactions that take longer than a few minutes to complete. SeeCan data duplicates occur?for details

Destination-specific limitations (such as Iceberg's append-only log format or BigQuery's row size limits) are detailed in each destination tab inStep 3above.


### Future work#
Replication is actively being developed. Planned improvements include:

- DDL support: Automatic handling of Postgres schema changes (ALTER TABLE, ADD COLUMN, etc.)
- Additional destinations: Support for more data warehouses and analytics platforms

There are no public timelines for these features, but they represent the roadmap for making replication more robust and flexible.


### Next steps#
- Monitor Replication
- View Replication FAQ


================================================================================


# Securing your data
Source: https://supabase.com/docs/guides/database/secure-data

Securing your data

Supabase helps you control access to your data. With access policies, you can protect sensitive data and make sure users only access what they're allowed to see.


### Connecting your app securely#
Supabase allows you to access your database using the auto-generatedData APIs. This speeds up the process of building web apps, since you don't need to write your own backend services to pass database queries and results back and forth.

You can keep your data secure while accessing the Data APIs from the frontend, so long as you:

- Turn onRow Level Security(RLS) for your tables
- Use your Supabaseanon keywhen you create a Supabase client

Your anon key is safe to expose with RLS enabled, because row access permission is checked against your access policies and the user'sJSON Web Token (JWT). The JWT is automatically sent by the Supabase client libraries if the user is logged in using Supabase Auth.

Unlike your anon key, yourservice role keyisneversafe to expose because it bypasses RLS. Only use your service role key on the backend. Treat it as a secret (for example, import it as a sensitive environment variable instead of hardcoding it).


### More information#
Supabase and Postgres provide you with multiple ways to manage security, including but not limited to Row Level Security. See the Access and Security pages for more information:

- Row Level Security
- Column Level Security
- Hardening the Data API
- Managing Postgres roles
- Managing secrets with Vault


================================================================================


# Supavisor
Source: https://supabase.com/docs/guides/database/supavisor

Supavisor


### Troubleshooting Supavisor errors
Troubleshooting Supavisor errors

Supavisor logs are available underPooler Logsin the Dashboard. The following are common errors and their solutions:


================================================================================


# Tables and Data
Source: https://supabase.com/docs/guides/database/tables

Tables and Data

Tables are where you store your data.

Tables are similar to excel spreadsheets. They contain columns and rows.
For example, this table has 3 "columns" (id,name,description) and 4 "rows" of data:

There are a few important differences from a spreadsheet, but it's a good starting point if you're new to Relational databases.


### Creating tables#
When creating a table, it's best practice to add columns at the same time.

You must define the "data type" of each column when it is created. You can add and remove columns at any time after creating a table.

Supabase provides several options for creating tables. You can use the Dashboard or create them directly using SQL.
We provide a SQL editor within the Dashboard, or you canconnectto your database
and run the SQL queries yourself.

When naming tables, use lowercase and underscores instead of spaces (e.g.,table_name, notTable Name).


### Columns#
You must define the "data type" when you create a column.


### Data types#
Every column is a predefined type. Postgres provides manydefault types, and you can even design your own (or use extensions) if the default types don't fit your needs. You can use any data type that Postgres supports via the SQL editor. We only support a subset of these in the Table Editor in an effort to keep the experience simple for people with less experience with databases.

You can "cast" columns from one type to another, however there can be some incompatibilities between types.
For example, if you cast atimestampto adate, you will lose all the time information that was previously saved.


### Primary keys#
A table can have a "primary key" - a unique identifier for every row of data. A few tips for Primary Keys:

- It's recommended to create a Primary Key for every table in your database.
- You can use any column as a primary key, as long as it is unique for every row.
- It's common to use auuidtype or a numberedidentitycolumn as your primary key.

```javascript
1create table movies (2  id bigint generated always as identity primary key3);
```

In the example above, we have:

We could also usegenerated by default as identity, which would allow us to insert our own unique values.

```javascript
1create table movies (2  id bigint generated by default as identity primary key3);
```


### Loading data#
There are several ways to load data in Supabase. You can load data directly into the database or using theAPIs.
Use the "Bulk Loading" instructions if you are loading large data sets.


### Basic data loading#
```javascript
1insert into movies2  (name, description)3values4  (5    'The Empire Strikes Back',6    'After the Rebels are brutally overpowered by the Empire on the ice planet Hoth, Luke Skywalker begins Jedi training with Yoda.'7  ),8  (9    'Return of the Jedi',10    'After a daring mission to rescue Han Solo from Jabba the Hutt, the Rebels dispatch to Endor to destroy the second Death Star.'11  );
```


### Bulk data loading#
When inserting large data sets it's best to use PostgreSQL'sCOPYcommand.
This loads data directly from a file into a table. There are several file formats available for copying data: text, CSV, binary, JSON, etc.

For example, if you wanted to load a CSV file into your movies table:

```javascript
1"The Empire Strikes Back", "After the Rebels are brutally overpowered by the Empire on the ice planet Hoth, Luke Skywalker begins Jedi training with Yoda."2"Return of the Jedi", "After a daring mission to rescue Han Solo from Jabba the Hutt, the Rebels dispatch to Endor to destroy the second Death Star."
```

You wouldconnectto your database directly and load the file with the COPY command:

```javascript
1psql -h DATABASE_URL -p 5432 -d postgres -U postgres \2  -c "\COPY movies FROM './movies.csv';"
```

Additionally use theDELIMITER,HEADERandFORMAToptions as defined in the PostgresCOPYdocs.

```javascript
1psql -h DATABASE_URL -p 5432 -d postgres -U postgres \2  -c "\COPY movies FROM './movies.csv' WITH DELIMITER ',' CSV HEADER"
```

If you receive an errorFATAL:  password authentication failed for user "postgres", reset your database password in the Database Settings and try again.


### Joining tables with foreign keys#
Tables can be "joined" together using Foreign Keys.

This is where the "Relational" naming comes from, as data typically forms some sort of relationship.

In our "movies" example above, we might want to add a "category" for each movie (for example, "Action", or "Documentary").
Let's create a new table calledcategoriesand "link" ourmoviestable.

```javascript
1create table categories (2  id bigint generated always as identity primary key,3  name text -- category name4);56alter table movies7  add column category_id bigint references categories;
```

You can also create "many-to-many" relationships by creating a "join" table.
For example if you had the following situations:

- You have a list ofmovies.
- A movie can have severalactors.
- Anactorcan perform in several movies.


### Schemas#
Tables belong toschemas. Schemas are a way of organizing your tables, often for security reasons.

If you don't explicitly pass a schema when creating a table, Postgres will assume that you want to create the table in thepublicschema.

We can create schemas for organizing tables. For example, we might want a private schema which is hidden from our API:

```javascript
1create schema private;
```

Now we can create tables inside theprivateschema:

```javascript
1create table private.salaries (2  id bigint generated by default as identity primary key,3  salary bigint not null,4  actor_id bigint not null references public.actors5);
```


### Views#
A View is a convenient shortcut to a query. Creating a view does not involve new tables or data. When run, an underlying query is executed, returning its results to the user.

Say we have the following tables from a database of a university:

students

courses

grades

Creating a view consisting of all the three tables will look like this:

```javascript
1create view transcripts as2    select3        students.name,4        students.type,5        courses.title,6        courses.code,7        grades.result8    from grades9    left join students on grades.student_id = students.id10    left join courses on grades.course_id = courses.id;1112grant all on table transcripts to authenticated;
```

Once done, we can now access the underlying query with:

```javascript
1select * from transcripts;
```


### View security#
By default, views are accessed with their creator's permission ("security definer"). If a privileged role creates a view, others accessing it will use that role's elevated permissions. To enforce row level security policies, define the view with the "security invoker" modifier.

```javascript
1-- alter a security_definer view to be security_invoker2alter view <view name>3set (security_invoker = true);45-- create a view with the security_invoker modifier6create view <view name> with(security_invoker=true) as (7  select * from <some table>8);
```


### When to use views#
Views provide several benefits:

- Simplicity
- Consistency
- Logical Organization
- Security


### Simplicity#
As a query becomes more complex, it can be a hassle to call it over and over - especially when we run it regularly. In the example above, instead of repeatedly running:

```javascript
1select2  students.name,3  students.type,4  courses.title,5  courses.code,6  grades.result7from8  grades9  left join students on grades.student_id = students.id10  left join courses on grades.course_id = courses.id;
```

We can run this instead:

```javascript
1select * from transcripts;
```

Additionally, a view behaves like a typical table. We can safely use it in tableJOINs or even create new views using existing views.


### Consistency#
Views ensure that the likelihood of mistakes decreases when repeatedly executing a query. In our example above, we may decide that we want to exclude the courseIntroduction to Postgres. The query would become:

```javascript
1select2  students.name,3  students.type,4  courses.title,5  courses.code,6  grades.result7from8  grades9  left join students on grades.student_id = students.id10  left join courses on grades.course_id = courses.id11where courses.code != 'PG101';
```

Without a view, we would need to go into every dependent query to add the new rule. This would increase in the likelihood of errors and inconsistencies, as well as introducing a lot of effort for a developer. With views, we can alter just the underlying query in the viewtranscripts. The change will be applied to all applications using this view.


### Logical organization#
With views, we can give our query a name. This is extremely useful for teams working with the same database. Instead of guessing what a query is supposed to do, a well-named view can explain it. For example, by looking at the name of the viewtranscripts, we can infer that the underlying query might involve thestudents,courses, andgradestables.


### Security#
Views can restrict the amount and type of data presented to a user. Instead of allowing a user direct access to a set of tables, we provide them a view instead. We can prevent them from reading sensitive columns by excluding them from the underlying query.


### Materialized views#
Amaterialized viewis a form of view but it also stores the results to disk. In subsequent reads of a materialized view, the time taken to return its results would be much faster than a conventional view. This is because the data is readily available for a materialized view while the conventional view executes the underlying query each time it is called.

Using our example above, a materialized view can be created like this:

```javascript
1create materialized view transcripts as2  select3    students.name,4    students.type,5    courses.title,6    courses.code,7    grades.result8  from9    grades10    left join students on grades.student_id = students.id11    left join courses on grades.course_id = courses.id;
```

Reading from the materialized view is the same as a conventional view:

```javascript
1select * from transcripts;
```


### Refreshing materialized views#
Unfortunately, there is a trade-off - data in materialized views are not always up to date. We need to refresh it regularly to prevent the data from becoming too stale. To do so:

```javascript
1refresh materialized view transcripts;
```

It's up to you how regularly refresh your materialized views, and it's probably different for each view depending on its use-case.


### Materialized views vs conventional views#
Materialized views are useful when execution times for queries or views are too slow. These could likely occur in views or queries involving multiple tables and billions of rows. When using such a view, however, there should be tolerance towards data being outdated. Some use-cases for materialized views are internal dashboards and analytics.

Creating a materialized view is not a solution to inefficient queries. You should always seek to optimize a slow running query even if you are implementing a materialized view.


### Resources#
- Official Docs: Create table
- Official Docs: Create view
- Postgres Tutorial: Create tables
- Postgres Tutorial: Add column
- Postgres Tutorial: Views


================================================================================


# Testing Your Database
Source: https://supabase.com/docs/guides/database/testing

Testing Your Database

To ensure that queries return the expected data, RLS policies are correctly applied and etc., we encourage you to write automated tests. There are essentially two approaches to testing:

- Firstly, you can write tests that interface with a Supabase client instance (same way you use Supabase client in your application code) in the programming language(s) you use in your application and using your favorite testing framework.
- Secondly, you can test through the Supabase CLI, which is a more low-level approach where you write tests in SQL.

Firstly, you can write tests that interface with a Supabase client instance (same way you use Supabase client in your application code) in the programming language(s) you use in your application and using your favorite testing framework.

Secondly, you can test through the Supabase CLI, which is a more low-level approach where you write tests in SQL.

You can use the Supabase CLI to test your database. The minimum required version of the CLI isv1.11.4. To get started:

- Install the Supabase CLIon your local machine


### Creating a test#
Create a tests folder inside thesupabasefolder:

```javascript
1mkdir -p ./supabase/tests/database
```

Create a new file with the.sqlextension which will contain the test.

```javascript
1touch ./supabase/tests/database/hello_world.test.sql
```


### Writing tests#
Allsqlfiles usepgTAPas the test runner.

Let's write a simple test to check that ourauth.userstable has an ID column. Openhello_world.test.sqland add the following code:

```javascript
1begin;2select plan(1); -- only one statement to run34SELECT has_column(5    'auth',6    'users',7    'id',8    'id should exist'9);1011select * from finish();12rollback;
```


### Running tests#
To run the test, you can use:

```javascript
1supabase test db
```

This will produce the following output:

```javascript
1$ supabase test db2supabase/tests/database/hello_world.test.sql .. ok3All tests successful.4Files=1, Tests=1,  1 wallclock secs ( 0.01 usr  0.00 sys +  0.04 cusr  0.02 csys =  0.07 CPU)5Result: PASS
```


### More resources#
- Testing RLS policies
- pgTAP extension
- OfficialpgTAP documentation


================================================================================


# Vault
Source: https://supabase.com/docs/guides/database/vault

Vault


### Managing secrets in Postgres.
Managing secrets in Postgres.

Vault is a Postgres extension and accompanying Supabase UI that makes it safe and easy to store encrypted secrets and other data in your database. This opens up a lot of possibilities to use Postgres in ways that go beyond what is available in a stock distribution.

Under the hood, the Vault is a table of Secrets that are stored usingAuthenticated Encryptionon disk. They are then available in decrypted form through a Postgres view so that the secrets can be used by applications from SQL. Because the secrets are stored on disk encrypted and authenticated, any backups or replication streams also preserve this encryption in a way that can't be decrypted or forged.

Supabase provides a dashboard UI for the Vault that makes storing secrets easy. Click a button, type in your secret, and save.

You can use Vault to store secrets - everything from Environment Variables to API Keys. You can then use these secrets anywhere in your database: PostgresFunctions, Triggers, andWebhooks. From a SQL perspective, accessing secrets is as easy as querying a table (or in this case, a view). The underlying secrets tables will be stored in encrypted form.


### Using Vault#
You can manage secrets from the UI or using SQL.


### Adding secrets#
There is also a handy function for creating secrets calledvault.create_secret():

```javascript
1select vault.create_secret('my_s3kre3t');
```

The function returns the UUID of the new secret.

```javascript
1-[ RECORD 1 ]-+-------------------------------------2create_secret | c9b00867-ca8b-44fc-a81d-d20b8169be17
```

Secrets can also have an optionaluniquename and an optional description. These are also arguments tovault.create_secret():

```javascript
1select vault.create_secret('another_s3kre3t', 'unique_name', 'This is the description');
```

```javascript
1-[ RECORD 1 ]-----------------------------------------------------------------2id          | 7095d222-efe5-4cd5-b5c6-5755b451e2233name        | unique_name4description | This is the description5secret      | 3mMeOcoG84a5F2uOfy2ugWYDp9sdxvCTmi6kTeT97bvA8rCEsG5DWWZtTU8VVeE=6key_id      |7nonce       | \x9f2d60954ba5eb566445736e0760b0e38created_at  | 2022-12-14 02:34:23.85159+009updated_at  | 2022-12-14 02:34:23.85159+00
```


### Viewing secrets#
If you look in thevault.secretstable, you will see that your data is stored encrypted. To decrypt the data, there is an automatically created viewvault.decrypted_secrets. This view will decrypt secret data on the fly:

```javascript
1select * 2from vault.decrypted_secrets 3order by created_at desc 4limit 3;
```

```javascript
1-[ RECORD 1 ]----+-----------------------------------------------------------------2id               | 7095d222-efe5-4cd5-b5c6-5755b451e2233name             | unique_name4description      | This is the description5secret           | 3mMeOcoG84a5F2uOfy2ugWYDp9sdxvCTmi6kTeT97bvA8rCEsG5DWWZtTU8VVeE=6decrypted_secret | another_s3kre3t7key_id           |8nonce            | \x9f2d60954ba5eb566445736e0760b0e39created_at       | 2022-12-14 02:34:23.85159+0010updated_at       | 2022-12-14 02:34:23.85159+0011-[ RECORD 2 ]----+-----------------------------------------------------------------12id               | c9b00867-ca8b-44fc-a81d-d20b8169be1713name             |14description      |15secret           | a1CE4vXwQ53+N9bllJj1D7fasm59ykohjb7K90PPsRFUd9IbBdxIGZNoSQLIXl4=16decrypted_secret | another_s3kre3t17key_id           |18nonce            | \x1d3b2761548c4efb2d29ca11d44aa22f19created_at       | 2022-12-14 02:32:50.58921+0020updated_at       | 2022-12-14 02:32:50.58921+0021-[ RECORD 3 ]----+-----------------------------------------------------------------22id               | d91596b8-1047-446c-b9c0-66d98af6d00123name             |24description      |25secret           | S02eXS9BBY+kE3r621IS8beAytEEtj+dDHjs9/0AoMy7HTbog+ylxcS22A==26decrypted_secret | s3kre3t_k3y27key_id           |28nonce            | \x3aa2e92f9808e496aa4163a59304b89529created_at       | 2022-12-14 02:29:21.3625+0030updated_at       | 2022-12-14 02:29:21.3625+00
```

Notice how this view has adecrypted_secretcolumn that contains the decrypted secrets. Views are not stored on disk, they are only run at query time, so the secret remains encrypted on disk, and in any backup dumps or replication streams.

You should ensure that you protect access to this view with the appropriate SQL privilege settings at all times, as anyone that has access to the view has access to decrypted secrets.


### Updating secrets#
A secret can be updated with thevault.update_secret()function, this function makes updating secrets easy, just provide the secret UUID as the first argument, and then an updated secret, updated optional unique name, or updated description:

```javascript
1select2  vault.update_secret(3    '7095d222-efe5-4cd5-b5c6-5755b451e223',4    'n3w_upd@ted_s3kret',5    'updated_unique_name',6    'This is the updated description'7  );
```

```javascript
1-[ RECORD 1 ]-+-2update_secret |34postgres=> select * from vault.decrypted_secrets where id = '7095d222-efe5-4cd5-b5c6-5755b451e223';5-[ RECORD 1 ]----+---------------------------------------------------------------------6id               | 7095d222-efe5-4cd5-b5c6-5755b451e2237name             | updated_unique_name8description      | This is the updated description9secret           | lhb3HBFxF+qJzp/HHCwhjl4QFb5dYDsIQEm35DaZQOovdkgp2iy6UMufTKJGH4ThMrU=10decrypted_secret | n3w_upd@ted_s3kret11key_id           |12nonce            | \x9f2d60954ba5eb566445736e0760b0e313created_at       | 2022-12-14 02:34:23.85159+0014updated_at       | 2022-12-14 02:51:13.938396+00
```


### Deep dive#
As we mentioned, Vault uses Transparent Column Encryption (TCE) to store secrets in an authenticated encrypted form. There are some details around that you may be curious about. What does authenticated mean? Where is the encryption key stored? This section explains those details.


### Authenticated encryption with associated data#
The first important feature of TCE is that it uses anAuthenticated Encryption with Associated Dataencryption algorithm (based onlibsodium).


### Encryption key location#
Authenticated Encryptionmeans that in addition to the data being encrypted, it is also signed so that it cannot be forged. You can guarantee that the data was encrypted by someone you trust, which you wouldn't get with encryption alone. The decryption function verifies that the signature is validbefore decrypting the value.

Associated Datameans that you can include any other columns from the same row as part of the signature computation. This doesn't encrypt those other columns - rather it ensures that your encrypted value is only associated with columns from that row. If an attacker were to copy an encrypted value from another row to the current one, the signature would be rejected (assuming you used a unique column in the associated data).

Another important feature is that the encryption key is never stored in the database alongside the encrypted data. Even if an attacker can capture a dump of your entire database, they will see only encrypted data,never the encryption key itself.

This is an important safety precaution - there is little value in storing the encryption key in the database itself as this would be like locking your front door but leaving the key in the lock! Storing the key outside the database fixes this issue.

Where is the key stored? Supabase creates and manages the encryption key in our secured backend systems. We keep this key safe and separate from your data. You remain in control of your key - a separate API endpoint is available that you can use to access the key if you want to decrypt your data outside of Supabase.

Which roles should have access to thevault.secretstable should be carefully considered. There are two ways to grant access, the first is that thepostgresuser can explicitly grant access to the vault table itself.


### Resources#
- Read more about Supabase Vault in theblog post
- Supabase Vault on GitHub
- Column Encryption


================================================================================


# Database Webhooks
Source: https://supabase.com/docs/guides/database/webhooks

Database Webhooks


### Trigger external payloads on database events.
Trigger external payloads on database events.

Database Webhooks allow you to send real-time data from your database to another system whenever a table event occurs.

You can hook into three table events:INSERT,UPDATE, andDELETE. All events are firedaftera database row is changed.


### Webhooks vs triggers#
Database Webhooks are very similar to triggers, and that's because Database Webhooks are just a convenience wrapper around triggers using thepg_netextension. This extension is asynchronous, and therefore will not block your database changes for long-running network requests.

This video demonstrates how you can create a new customer in Stripe each time a row is inserted into aprofilestable:


### Creating a webhook#
Since webhooks are just database triggers, you can also create one from SQL statement directly.

```javascript
1create trigger "my_webhook" after insert2on "public"."my_table" for each row3execute function "supabase_functions"."http_request"(4  'http://host.docker.internal:3000',5  'POST',6  '{"Content-Type":"application/json"}',7  '{}',8  '1000'9);
```

We currently support HTTP webhooks. These can be sent asPOSTorGETrequests with a JSON payload.


### Payload#
The payload is automatically generated from the underlying table record:

```javascript
1type InsertPayload = {2  type: 'INSERT'3  table: string4  schema: string5  record: TableRecord<T>6  old_record: null7}8type UpdatePayload = {9  type: 'UPDATE'10  table: string11  schema: string12  record: TableRecord<T>13  old_record: TableRecord<T>14}15type DeletePayload = {16  type: 'DELETE'17  table: string18  schema: string19  record: null20  old_record: TableRecord<T>21}
```


### Monitoring#
Logging history of webhook calls is available under thenetschema of your database. For more info, see theGitHub Repo.


### Local development#
When using Database Webhooks on your local Supabase instance, you need to be aware that the Postgres database runs inside a Docker container. This means thatlocalhostor127.0.0.1in your webhook URL will refer to the container itself, not your host machine where your application is running.

To target services running on your host machine, usehost.docker.internal. If that doesn't work, you may need to use your machine's local IP address instead.

For example, if you want to trigger an edge function when a webhook fires, your webhook URL would be:

```javascript
1http://host.docker.internal:54321/functions/v1/my-function-name
```

If you're experiencing connection issues with webhooks locally, verify you're using the correct hostname instead oflocalhost.


### Resources#
- pg_net: an async networking extension for Postgres


================================================================================


# Edge Functions
Source: https://supabase.com/docs/guides/functions

Edge Functions


### Globally distributed TypeScript functions.
Globally distributed TypeScript functions.

Edge Functions are server-side TypeScript functions, distributed globally at the edge—close to your users. They can be used for listening to webhooks or integrating your Supabase project with third-partieslike Stripe. Edge Functions are developed usingDeno, which offers a few benefits to you as a developer:

- It is open source.
- It is portable. Supabase Edge Functions run locally, and on any other Deno-compatible platform (including self-hosted infrastructure).
- It is TypeScript first and supports WASM.
- Edge Functions are globally distributed for low-latency.


### How it works#
- Request enters an edge gateway (relay)— the gateway routes traffic, handles auth headers/JWT validation, and applies routing/traffic rules.
- Auth & policies are applied— the gateway (or your function) can validate Supabase JWTs, apply rate-limits, and centralize security checks before executing code.
- Edge runtimeexecutes your function— the function runs on a regionally-distributed Edge Runtime node closest to the user for minimal latency.
- Integrations & data access— functions commonly call Supabase APIs (Auth, Postgres, Storage) or third-party APIs. For Postgres, prefer connection strategies suited for edge/serverless environments (see theconnect-to-postgresguide).
- Observability and logs— invocations emit logs and metrics you can explore in the dashboard or downstream monitoring (Sentry, etc.).
- Response returns via the gateway— the gateway forwards the response back to the client and records request metadata.


### Quick technical notes#
- Runtime:Supabase Edge Runtime (Deno compatible runtime with TypeScript first). Functions are simple.tsfiles that export a handler.
- Local dev parity:Use Supabase CLI for a local runtime similar to production for faster iteration (supabase functions servecommand).
- Global deployment:Deploy your Edge Functions via Supabase Dashboard, CLI or MCP.
- Cold starts & concurrency:cold starts are possible — design for short-lived, idempotent operations. Heavy long-running jobs should be moved tobackground workers.
- Database connections:treat Postgres like a remote, pooled service — use connection pools or serverless-friendly drivers.
- Secrets:store credentials in Supabaseproject secretsand access them via environment variables.


### When to use Edge Functions#
- Authenticated or public HTTP endpoints that need low latency.
- Webhook receivers (Stripe, GitHub, etc.).
- On-demand image or Open Graph generation.
- Small AI inference tasks or orchestrating calls to external LLM APIs (like OpenAI)
- Sending transactional emails.
- Building messaging bots for Slack, Discord, etc.


### Examples#
Check out theEdge Function Examplesin our GitHub repository.

With supabase-js

Type-Safe SQL with Kysely

Monitoring with Sentry

With CORS headers

React Native with Stripe

Flutter with Stripe

Building a RESTful Service API

Working with Supabase Storage

Open Graph Image Generation

OG Image Generation & Storage CDN Caching

Get User Location

Cloudflare Turnstile

Connect to Postgres

GitHub Actions

Oak Server Middleware

Hugging Face

Amazon Bedrock

OpenAI

Stripe Webhooks

Send emails

Web Stream

Puppeteer

Discord Bot

Telegram Bot

Upload File

Upstash Redis

Rate Limiting

Slack Bot Mention Edge Function


================================================================================


# Running AI Models
Source: https://supabase.com/docs/guides/functions/ai-models

Running AI Models


### Run AI models in Edge Functions using the built-in Supabase AI API.
Run AI models in Edge Functions using the built-in Supabase AI API.

Edge Functions have a built-in API for running AI models. You can use this API to generate embeddings, build conversational workflows, and do other AI related tasks in your Edge Functions.

This allows you to:

- Generate text embeddings without external dependencies
- Run Large Language Models via Ollama or Llamafile
- Build conversational AI workflows


### Setup#
There are no external dependencies or packages to install to enable the API.

Create a new inference session:

```javascript
1const model = new Supabase.ai.Session('model-name')
```

To get type hints and checks for the API, import types fromfunctions-js:

```javascript
1import 'jsr:@supabase/functions-js/edge-runtime.d.ts'
```


### Running a model inference#
Once the session is instantiated, you can call it with inputs to perform inferences:

```javascript
1// For embeddings (gte-small model)2const embeddings = await model.run('Hello world', {3  mean_pool: true,4  normalize: true,5})67// For text generation (non-streaming)8const response = await model.run('Write a haiku about coding', {9  stream: false,10  timeout: 30,11})1213// For streaming responses14const stream = await model.run('Tell me a story', {15  stream: true,16  mode: 'ollama',17})
```


### Generate text embeddings#
Generate text embeddings using the built-ingte-smallmodel:

gte-smallmodel exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens. While you can provide inputs longer than 512 tokens, truncation may affect the accuracy.

```javascript
1const model = new Supabase.ai.Session('gte-small')23Deno.serve(async (req: Request) => {4  const params = new URL(req.url).searchParams5  const input = params.get('input')6  const output = await model.run(input, { mean_pool: true, normalize: true })7  return new Response(JSON.stringify(output), {8    headers: {9      'Content-Type': 'application/json',10      Connection: 'keep-alive',11    },12  })13})
```


### Using Large Language Models (LLM)#
Inference via larger models is supported viaOllamaandMozilla Llamafile. In the first iteration, you can use it with a self-managed Ollama orLlamafile server.

We are progressively rolling out support for the hosted solution. To sign up for early access, fill outthis form.


### Running locally#

### Install Ollama
Install Ollamaand pull the Mistral model

```javascript
1ollama pull mistral
```


### Run the Ollama server
```javascript
1ollama serve
```


### Set the function secret
Set a function secret calledAI_INFERENCE_API_HOSTto point to the Ollama server

```javascript
1echo "AI_INFERENCE_API_HOST=http://host.docker.internal:11434" >> supabase/functions/.env
```


### Create a new function
```javascript
1supabase functions new ollama-test
```

```javascript
1import 'jsr:@supabase/functions-js/edge-runtime.d.ts'2const session = new Supabase.ai.Session('mistral')34Deno.serve(async (req: Request) => {5  const params = new URL(req.url).searchParams6  const prompt = params.get('prompt') ?? ''78  // Get the output as a stream9  const output = await session.run(prompt, { stream: true })1011  const headers = new Headers({12    'Content-Type': 'text/event-stream',13    Connection: 'keep-alive',14  })1516  // Create a stream17  const stream = new ReadableStream({18    async start(controller) {19      const encoder = new TextEncoder()2021      try {22        for await (const chunk of output) {23          controller.enqueue(encoder.encode(chunk.response ?? ''))24        }25      } catch (err) {26        console.error('Stream error:', err)27      } finally {28        controller.close()29      }30    },31  })3233  // Return the stream to the user34  return new Response(stream, {35    headers,36  })37})
```


### Serve the function
```javascript
1supabase functions serve --env-file supabase/functions/.env
```


### Execute the function
```javascript
1curl --get "http://localhost:54321/functions/v1/ollama-test" \2--data-urlencode "prompt=write a short rap song about Supabase, the Postgres Developer platform, as sung by Nicki Minaj" \3-H "Authorization: $ANON_KEY"
```


### Deploying to production#
Once the function is working locally, it's time to deploy to production.


### Deploy an Ollama or Llamafile server
Deploy an Ollama or Llamafile server and set a function secret calledAI_INFERENCE_API_HOSTto point to the deployed server:

```javascript
1supabase secrets set AI_INFERENCE_API_HOST=https://path-to-your-llm-server/
```


### Deploy the function
```javascript
1supabase functions deploy
```


### Execute the function
```javascript
1curl --get "https://project-ref.supabase.co/functions/v1/ollama-test" \2--data-urlencode "prompt=write a short rap song about Supabase, the Postgres Developer platform, as sung by Nicki Minaj" \3-H "Authorization: $ANON_KEY"
```

As demonstrated in the video above, running Ollama locally is typically slower than running it in on a server with dedicated GPUs. We are collaborating with the Ollama team to improve local performance.

In the future, a hosted LLM API, will be provided as part of the Supabase platform. Supabase will scale and manage the API and GPUs for you. To sign up for early access, fill upthis form.


================================================================================


# Edge Functions Architecture
Source: https://supabase.com/docs/guides/functions/architecture

Edge Functions Architecture


### Understanding the Architecture of Supabase Edge Functions
Understanding the Architecture of Supabase Edge Functions

This guide explains the architecture and inner workings of Supabase Edge Functions, based on the concepts demonstrated in the video "Supabase Edge Functions Explained". Edge functions are serverless compute resources that run at the edge of the network, close to users, enabling low-latency execution for tasks like API endpoints, webhooks, and real-time data processing. This guide breaks down Edge Functions into key sections: an example use case, deployment process, global distribution, and execution mechanics.


### 1. Understanding Edge Functions through an example: Image filtering#
To illustrate how edge functions operate, consider a photo-sharing app where users upload images and apply filters (e.g., grayscale or sepia) before saving them.

- Workflow Overview:A user uploads an original image to Supabase Storage.When the user selects a filter, the client-side app (using the Supabase JavaScript SDK) invokes an edge function named something like "apply-filter."The edge function:Downloads the original image from Supabase Storage.Applies the filter using a library like ImageMagick.Uploads the processed image back to Storage.Returns the path to the filtered image to the client.
- Why Edge Functions?:They handle compute-intensive tasks without burdening the client device or the database.Execution happens server-side but at the edge, ensuring speed and scalability.Developers define the function in a simple JavaScript file within the Supabase functions directory.

Workflow Overview:

- A user uploads an original image to Supabase Storage.
- When the user selects a filter, the client-side app (using the Supabase JavaScript SDK) invokes an edge function named something like "apply-filter."
- The edge function:Downloads the original image from Supabase Storage.Applies the filter using a library like ImageMagick.Uploads the processed image back to Storage.Returns the path to the filtered image to the client.

Why Edge Functions?:

- They handle compute-intensive tasks without burdening the client device or the database.
- Execution happens server-side but at the edge, ensuring speed and scalability.
- Developers define the function in a simple JavaScript file within the Supabase functions directory.

This example highlights edge functions as lightweight, on-demand code snippets that integrate seamlessly with Supabase services like Storage and Auth.


### 2. Deployment process#
Deploying an edge function is straightforward and automated, requiring no manual server setup.

- Steps to Deploy:Write the function code in your local Supabase project (e.g., insupabase/functions/apply-filter/index.ts).Run the commandsupabase functions deploy apply-filtervia the Supabase CLI.The CLI bundles the function and its dependencies into anESZip file—a compact format created by Deno that includes a complete module graph for quick loading and execution.The bundled file is uploaded to Supabase's backend.Supabase generates a unique URL for the function, making it accessible globally.
- Key Benefits of Deployment:Automatic handling of dependencies and bundling.No need to manage infrastructure; Supabase distributes the function across its global edge network.

Steps to Deploy:

Key Benefits of Deployment:

- Automatic handling of dependencies and bundling.
- No need to manage infrastructure; Supabase distributes the function across its global edge network.

Once deployed, the function is ready for invocation from anywhere, with Supabase handling scaling and availability.


### 3. Global distribution and routing#
Edge functions leverage a distributed architecture to minimize latency by running code close to the user.

- Architecture Components:Global API Gateway: Acts as the entry point for all requests. It uses the requester's IP address to determine geographic location and routes the request to the nearest edge location (e.g., routing a request from Amsterdam to Frankfurt).Edge Locations: Supabase's network of data centers worldwide where functions are replicated. The ESZip bundle is automatically distributed to these locations upon deployment.Routing Logic: Based on geolocation mapping, ensuring the function executes as close as possible to the user for optimal performance.
- How Distribution Works:Post-deployment, the function is propagated to all edge nodes.This setup eliminates the need for developers to configure CDNs or regional servers manually.

Architecture Components:

- Global API Gateway: Acts as the entry point for all requests. It uses the requester's IP address to determine geographic location and routes the request to the nearest edge location (e.g., routing a request from Amsterdam to Frankfurt).
- Edge Locations: Supabase's network of data centers worldwide where functions are replicated. The ESZip bundle is automatically distributed to these locations upon deployment.
- Routing Logic: Based on geolocation mapping, ensuring the function executes as close as possible to the user for optimal performance.

How Distribution Works:

- Post-deployment, the function is propagated to all edge nodes.
- This setup eliminates the need for developers to configure CDNs or regional servers manually.

This global edge network is what makes edge functions "edge-native," providing consistent performance regardless of user location.


### 4. Execution mechanics: Fast and isolated#
The core of edge functions' efficiency lies in their execution environment, which prioritizes speed, isolation, and scalability.

- Request Handling:A client sends an HTTP request (e.g., POST) to the function's URL, including parameters like auth headers, image ID, and filter type.The global API gateway routes it to the nearest edge location.At the edge, Supabase'sedge runtimevalidates the request (e.g., checks authorization).
- Execution Environment:A newV8 isolateis spun up for each invocation. V8 is the JavaScript engine used by Chrome and Node.js, providing a lightweight, sandboxed environment.Each isolate has its own memory heap and execution thread, ensuring complete isolation—no interference between concurrent requests.The ESZip bundle is loaded into the isolate, and the function code runs.After execution, the response (e.g., filtered image path) is sent back to the client.
- Performance Optimizations:Cold Starts: Even initial executions are fast (milliseconds) due to the compact ESZip format and minimal Deno runtime overhead.Warm Starts: Isolates can remain active for a period (plan-dependent) to handle subsequent requests without restarting.Concurrency: Multiple isolates can run simultaneously in the same edge location, supporting high traffic.
- Isolation and Security:Isolates prevent side effects from one function affecting others, enhancing reliability.No persistent state; each run is stateless, ideal for ephemeral tasks.

Request Handling:

Execution Environment:

- A newV8 isolateis spun up for each invocation. V8 is the JavaScript engine used by Chrome and Node.js, providing a lightweight, sandboxed environment.
- Each isolate has its own memory heap and execution thread, ensuring complete isolation—no interference between concurrent requests.
- The ESZip bundle is loaded into the isolate, and the function code runs.
- After execution, the response (e.g., filtered image path) is sent back to the client.

Performance Optimizations:

- Cold Starts: Even initial executions are fast (milliseconds) due to the compact ESZip format and minimal Deno runtime overhead.
- Warm Starts: Isolates can remain active for a period (plan-dependent) to handle subsequent requests without restarting.
- Concurrency: Multiple isolates can run simultaneously in the same edge location, supporting high traffic.

Isolation and Security:

- Isolates prevent side effects from one function affecting others, enhancing reliability.
- No persistent state; each run is stateless, ideal for ephemeral tasks.

Compared to traditional serverless or monolithic architectures, this setup offers lower latency, automatic scaling, and no infrastructure management, making it perfect for global apps.


### Benefits and use cases#
- Advantages:Low Latency: Proximity to users reduces round-trip times.Scalability: Handles variable loads without provisioning servers.Developer-Friendly: Focus on code; Supabase manages the rest.Cost-Effective: Pay-per-use model, with fast execution minimizing costs.
- Common Use Cases:Real-time data transformations (e.g., image processing).API integrations and webhooks.Personalization and A/B testing at the edge.

Advantages:

- Low Latency: Proximity to users reduces round-trip times.
- Scalability: Handles variable loads without provisioning servers.
- Developer-Friendly: Focus on code; Supabase manages the rest.
- Cost-Effective: Pay-per-use model, with fast execution minimizing costs.

Common Use Cases:

- Real-time data transformations (e.g., image processing).
- API integrations and webhooks.
- Personalization and A/B testing at the edge.


================================================================================


# Securing Edge Functions
Source: https://supabase.com/docs/guides/functions/auth

Securing Edge Functions


### Best practices on securing Edge Functions
Best practices on securing Edge Functions

In the past Supabase Auth used asymmetricsecret to sign legacy JWTs.
But it was replaced by newJWT Signing Keys. This guide covers the new patterns for securing your Edge Functions.

If you need to validate using the old method, read theLegacy JWT Secret guide.

Before continuing, read theJWT Signing Keys guidefor details about the main differences compared to Legacy JWTs.


### Overview#
When an HTTP request is sent to Edge Functions, you can use Supabase Auth to secure endpoints. In the past, this verification was controlled by theverify_jwtflag.

But, this method is incompatible with the newJWT Signing Keysand also caused trouble when attemptingthird-party integration.

For this reason we decided to no longer implicitly force JWT verification, but instead suggest patterns and templates to handle this task. This allows users to own and control the auth code, instead of hiding it internally under Edge Runtime infrastructure.

Following theupcoming API key changestimetable, theverify_jwtflag will still be supported and enabled by default. To move to the newJWT Signing Keys, you need to manuallyskip the authorization checksand follow the steps below.


### Integrating with Supabase Auth#
Important notes to consider:

- This is doneinsidetheDeno.serve()callback argument, so that the Authorization header is set for each request.
- UseDeno.env.get('SUPABASE_URL')to get the URL associated with your project. Using a value such ashttp://localhost:54321for local development will fail due to Docker containerization.


### Get API details#
Now that you've created some database tables, you are ready to insert data using the auto-generated API.

To do this, you need to get the Project URL and key fromthe projectConnectdialog.

Supabase is changing the way keys work to improve project security and developer experience. You canread the full announcement, but in the transition period, you can use both the currentanonandservice_rolekeys and the new publishable key with the formsb_publishable_xxxwhich will replace the older keys.

In most cases, you can get the correct key fromthe Project'sConnectdialog, but if you want a specific key, you can find all keys inthe API Keys section of a Project's Settings page:

- For legacy keys, copy theanonkey for client-side operations and theservice_rolekey for server-side operations from theLegacy API Keystab.
- For new keys, open theAPI Keystab, if you don't have a publishable key already, clickCreate new API Keys, and copy the value from thePublishable keysection.

Read the API keys docsfor a full explanation of all key types and their uses.

Currently, the new API keys are not available by default on the Edge Functions environment.
But you can manually expose them assecretusing theSB_prefix.

We're working on exposing these secrets and making them default in the future.

```javascript
1import 'jsr:@supabase/functions-js/edge-runtime.d.ts'2import { createClient } from 'npm:@supabase/supabase-js@2'34const supabase = createClient(Deno.env.get('SUPABASE_URL')!, Deno.env.get('SB_PUBLISHABLE_KEY')!)56Deno.serve(async (req) => {7  const authHeader = req.headers.get('Authorization')!8  const token = authHeader.replace('Bearer ', '')910  const { data, error } = await supabase.auth.getClaims(token)11  const userEmail = data?.claims?.email12  if (!userEmail || error) {13    return Response.json(14      { msg: 'Invalid JWT' },15      {16        status: 401,17      }18    )19  }2021  return Response.json({ message: `hello ${userEmail}` })22})
```


### Verifying JWT#

### Using Supabase template#
You can seea custom JWT verification example on GitHuband a variety ofauth function templatesalso on GitHub.

To verify incoming requests, you can copy/download the specified template and start using it:

The following example usesjoselibrary to verify received JWTs.

```javascript
1// ...23import * as jose from "jsr:@panva/jose@6";45const SUPABASE_JWT_ISSUER = Deno.env.get("SB_JWT_ISSUER") ??6  Deno.env.get("SUPABASE_URL") + "/auth/v1";78const SUPABASE_JWT_KEYS = jose.createRemoteJWKSet(9  new URL(Deno.env.get("SUPABASE_URL")! + "/auth/v1/.well-known/jwks.json"),10);1112function getAuthToken(req: Request) {13  const authHeader = req.headers.get("authorization");14  if (!authHeader) {15    throw new Error("Missing authorization header");16  }17  const [bearer, token] = authHeader.split(" ");18  if (bearer !== "Bearer") {19    throw new Error(`Auth header is not 'Bearer {token}'`);20  }2122  return token;23}2425function verifySupabaseJWT(jwt: string) {26  return jose.jwtVerify(jwt, SUPABASE_JWT_KEYS, {27    issuer: SUPABASE_JWT_ISSUER,28  });29}3031// Validates authorization header32export async function AuthMiddleware(33  req: Request,34  next: (req: Request) => Promise<Response>,35) {36  if (req.method === "OPTIONS") return await next(req);3738  try {39    const token = getAuthToken(req);40    const isValidJWT = await verifySupabaseJWT(token);4142    if (isValidJWT) return await next(req);4344    return Response.json({ msg: "Invalid JWT" }, {45      status: 401,46    });47  } catch (e) {48    return Response.json({ msg: e?.toString() }, {49      status: 401,50    });51  }52}
```

```javascript
1// ...23import { AuthMiddleware } from "../_shared/jwt/default.ts";45interface reqPayload {6  name: string;7}89Deno.serve((r) =>10  AuthMiddleware(r, async (req) => {11    const { name }: reqPayload = await req.json();12    const data = {13      message: `Hello ${name} from foo!`,14    };1516    return Response.json(data);17  })18);
```


================================================================================


# Integrating With Supabase Auth
Source: https://supabase.com/docs/guides/functions/auth-legacy-jwt

Integrating With Supabase Auth


### Integrate Supabase Auth with Edge Functions
Integrate Supabase Auth with Edge Functions

Edge Functions work withSupabase Auth.

This allows you to:

- Automatically identify users through Legacy JWT tokens
- Enforce Row Level Security policies
- Integrate with your existing auth flow


### Setting up auth context#
When a user makes a request to an Edge Function, you can use theAuthorizationheader to set the Auth context in the Supabase client and enforce Row Level Security policies.

```javascript
1import { createClient } from 'npm:@supabase/supabase-js@2'23Deno.serve(async (req: Request) => {4  const supabaseClient = createClient(5    Deno.env.get('SUPABASE_URL') ?? '',6    Deno.env.get('SUPABASE_ANON_KEY') ?? '',7    // Create client with Auth context of the user that called the function.8    // This way your row-level-security (RLS) policies are applied.9    {10      global: {11        headers: { Authorization: req.headers.get('Authorization')! },12      },13    }14  );1516  //...17})
```

This context setting happens in theDeno.serve()callback argument, so that theAuthorizationheader is set for each individual request scope.


### Fetching the user#
By getting the JWT from theAuthorizationheader, you can provide the token togetUser()to fetch the user object to obtain metadata for the logged in user.

```javascript
1Deno.serve(async (req: Request) => {2  // ...3  const authHeader = req.headers.get('Authorization')!4  const token = authHeader.replace('Bearer ', '')5  const { data } = await supabaseClient.auth.getUser(token)6  // ...7})
```


### Row Level Security#
After initializing a Supabase client with the Auth context, all queries will be executed with the context of the user. For database queries, this meansRow Level Securitywill be enforced.

```javascript
1import { createClient } from 'npm:@supabase/supabase-js@2'23Deno.serve(async (req: Request) => {4  // ...5  // This query respects RLS - users only see rows they have access to6  const { data, error } = await supabaseClient.from('profiles').select('*');78  if (error) {9    return new Response('Database error', { status: 500 })10  }1112  // ...13})
```


### Example#
See the fullexample on GitHub.

```javascript
1// Follow this setup guide to integrate the Deno language server with your editor:2// https://deno.land/manual/getting_started/setup_your_environment3// This enables autocomplete, go to definition, etc.45import { createClient } from 'npm:supabase-js@2'6// New approach (v2.95.0+)7import { corsHeaders } from 'jsr:@supabase/supabase-js@2/cors'8// For older versions:9// import { corsHeaders } from '../_shared/cors.ts'1011console.log(`Function "select-from-table-with-auth-rls" up and running!`)1213Deno.serve(async (req: Request) => {14  // This is needed if you're planning to invoke your function from a browser.15  if (req.method === 'OPTIONS') {16    return new Response('ok', { headers: corsHeaders })17  }1819  try {20    // Create a Supabase client with the Auth context of the logged in user.21    const supabaseClient = createClient(22      // Supabase API URL - env var exported by default.23      Deno.env.get('SUPABASE_URL') ?? '',24      // Supabase API ANON KEY - env var exported by default.25      Deno.env.get('SUPABASE_ANON_KEY') ?? '',26      // Create client with Auth context of the user that called the function.27      // This way your row-level-security (RLS) policies are applied.28      {29        global: {30          headers: { Authorization: req.headers.get('Authorization')! },31        },32      }33    )3435    // First get the token from the Authorization header36    const token = req.headers.get('Authorization').replace('Bearer ', '')3738    // Now we can get the session or user object39    const {40      data: { user },41    } = await supabaseClient.auth.getUser(token)4243    // And we can run queries in the context of our authenticated user44    const { data, error } = await supabaseClient.from('users').select('*')45    if (error) throw error4647    return new Response(JSON.stringify({ user, data }), {48      headers: { ...corsHeaders, 'Content-Type': 'application/json' },49      status: 200,50    })51  } catch (error) {52    return new Response(JSON.stringify({ error: error.message }), {53      headers: { ...corsHeaders, 'Content-Type': 'application/json' },54      status: 400,55    })56  }57})5859// To invoke:60// curl -i --location --request POST 'http://localhost:54321/functions/v1/select-from-table-with-auth-rls' \61//   --header 'Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24ifQ.625_WdcF3KHqz5amU0x2X5WWHP-OEs_4qj0ssLNHzTs' \62//   --header 'Content-Type: application/json' \63//   --data '{"name":"Functions"}'
```


================================================================================


# Background Tasks
Source: https://supabase.com/docs/guides/functions/background-tasks

Background Tasks


### Run background tasks in an Edge Function outside of the request handler.
Run background tasks in an Edge Function outside of the request handler.

Edge Function instances can process background tasks outside of the request handler. Background tasks are useful for asynchronous operations like uploading a file to Storage, updating a database, or sending events to a logging service. You can respond to the request immediately and leave the task running in the background.

This allows you to:

- Respond quickly to users while processing continues
- Handle async operations without blocking the response


### Overview#
You can useEdgeRuntime.waitUntil(promise)to explicitly mark background tasks. The Function instance continues to run until the promise provided towaitUntilcompletes.

```javascript
1// Mark the asyncLongRunningTask's returned promise as a background task.2// ⚠️ We are NOT using `await` because we don't want it to block!3EdgeRuntime.waitUntil(asyncLongRunningTask())45Deno.serve(async (req) => {6  return new Response(...)7})
```

You can callEdgeRuntime.waitUntilin the request handler too. This will not block the request.

```javascript
1Deno.serve(async (req) => {2  // Won't block the request, runs in background.3  EdgeRuntime.waitUntil(asyncLongRunningTask())45  return new Response(...)6})
```

You can listen to thebeforeunloadevent handler to be notified when the Function is about to be shut down.

```javascript
1EdgeRuntime.waitUntil(asyncLongRunningTask())23// Use beforeunload event handler to be notified when function is about to shutdown4addEventListener('beforeunload', (ev) => {5  console.log('Function will be shutdown due to', ev.detail?.reason)6  // Save state or log the current progress7})89Deno.serve(async (req) => {10  return new Response(...)11})
```


### Handling errors#
We recommend usingtry/catchblocks within your background task function to handle errors.

You can also add an event listener tounhandledrejectionto handle any promises without a rejection handler.

```javascript
1addEventListener('unhandledrejection', (ev) => {2  console.log('unhandledrejection', ev.reason)3  ev.preventDefault()4})
```

The maximum duration is capped based on the wall-clock, CPU, and memory limits. The function will shut down when it reaches one of theselimits.


### Testing background tasks locally#
When testing Edge Functions locally with Supabase CLI, the instances are terminated automatically after a request is completed. This will prevent background tasks from running to completion.

To prevent that, you can update thesupabase/config.tomlwith the following settings:

```javascript
1[edge_runtime]2policy = "per_worker"
```


================================================================================


# Handling Compressed Requests
Source: https://supabase.com/docs/guides/functions/compression

Handling Compressed Requests


### Handling Gzip compressed requests.
Handling Gzip compressed requests.

To decompress Gzip bodies, you can usegunzipSyncfrom thenode:zlibAPI to decompress and then read the body.

```javascript
1import {  } from 'node:zlib'23.(async () => {4  try {5    // Check if the request body is gzip compressed6    const  = ..('content-encoding')7    if ( !== 'gzip') {8      return new ('Request body is not gzip compressed', {9        : 400,10      })11    }1213    // Read the compressed body14    const  = await .()1516    // Decompress the body17    const  = (new ())1819    // Convert the decompressed body to a string20    const  = new ().()21    const  = .()2223    // Process the decompressed body as needed24    .(`Received: ${.()}`)2526    return new ('ok', {27      : { 'Content-Type': 'text/plain' },28    })29  } catch () {30    .('Error:', )31    return new ('Error processing request', { : 500 })32  }33})
```

Edge functions have a runtime memory limit of 150MB. Overly large compressed payloads may result in an out-of-memory error.


================================================================================


# Integrating with Supabase Database (Postgres)
Source: https://supabase.com/docs/guides/functions/connect-to-postgres

Integrating with Supabase Database (Postgres)


### Connect to your Postgres database from Edge Functions.
Connect to your Postgres database from Edge Functions.

Connect to your Postgres database from an Edge Function by using thesupabase-jsclient.
You can also use other Postgres clients likeDeno Postgres


### Using supabase-js#
Thesupabase-jsclient handles authorization with Row Level Security and automatically formats responses as JSON. This is the recommended approach for most applications:

```javascript
1import { createClient } from 'npm:@supabase/supabase-js@2'23Deno.serve(async (req) => {4  try {5    const supabase = createClient(6      Deno.env.get('SUPABASE_URL') ?? '',7      Deno.env.get('SUPABASE_PUBLISHABLE_KEY') ?? '',8      { global: { headers: { Authorization: req.headers.get('Authorization')! } } }9    )1011    const { data, error } = await supabase.from('countries').select('*')1213    if (error) {14      throw error15    }1617    return new Response(JSON.stringify({ data }), {18      headers: { 'Content-Type': 'application/json' },19      status: 200,20    })21  } catch (err) {22    return new Response(String(err?.message ?? err), { status: 500 })23  }24})
```

This enables:

- Automatic Row Level Security enforcement
- Built-in JSON serialization
- Consistent error handling
- TypeScript support for database schema


### Using a Postgres client#
Because Edge Functions are a server-side technology, it's safe to connect directly to your database using any popular Postgres client. This means you can run raw SQL from your Edge Functions.

Here is how you can connect to the database using Deno Postgres driver and run raw SQL. Check out thefull example.

```javascript
1import { Pool } from 'https://deno.land/x/postgres@v0.17.0/mod.ts'23// Create a database pool with one connection.4const pool = new Pool(5  {6    tls: { enabled: false },7    database: 'postgres',8    hostname: Deno.env.get('DB_HOSTNAME'),9    user: Deno.env.get('DB_USER'),10    port: 6543,11    password: Deno.env.get('DB_PASSWORD'),12  },13  114)1516Deno.serve(async (_req) => {17  try {18    // Grab a connection from the pool19    const connection = await pool.connect()2021    try {22      // Run a query23      const result = await connection.queryObject`SELECT * FROM animals`24      const animals = result.rows // [{ id: 1, name: "Lion" }, ...]2526      // Encode the result as pretty printed JSON27      const body = JSON.stringify(28        animals,29        (_key, value) => (typeof value === 'bigint' ? value.toString() : value),30        231      )3233      // Return the response with the correct content type header34      return new Response(body, {35        status: 200,36        headers: {37          'Content-Type': 'application/json; charset=utf-8',38        },39      })40    } finally {41      // Release the connection back into the pool42      connection.release()43    }44  } catch (err) {45    console.error(err)46    return new Response(String(err?.message ?? err), { status: 500 })47  }48})
```


### Using Drizzle#
You can use Drizzle together withPostgres.js. Both can be loaded directly from npm:

Set up dependencies inimport_map.json:

```javascript
1{2  "imports": {3    "drizzle-orm": "npm:drizzle-orm@0.29.1",4    "drizzle-orm/": "npm:/drizzle-orm@0.29.1/",5    "postgres": "npm:postgres@3.4.3"6  }7}
```

Use in your function:

```javascript
1import { drizzle } from 'drizzle-orm/postgres-js'2import postgres from 'postgres'3import { countries } from '../_shared/schema.ts'45const connectionString = Deno.env.get('SUPABASE_DB_URL')!67Deno.serve(async (_req) => {8  // Disable prefetch as it is not supported for "Transaction" pool mode9  const client = postgres(connectionString, { prepare: false })10  const db = drizzle(client)11  const allCountries = await db.select().from(countries)1213  return Response.json(allCountries)14})
```

You can find the full example onGitHub.


### SSL connections#

### Production#
Deployed edge functions are pre-configured to use SSL for connections to the Supabase database. You don't need to add any extra configurations.


### Local development#
If you want to use SSL connections during local development, follow these steps:

```javascript
1SSL_CERT_FILE=/path/to/cert.crt # set the path to the downloaded cert2DENO_TLS_CA_STORE=mozilla,system
```

Then, restart your local development server:

```javascript
1supabase functions serve your-function
```


================================================================================


# CORS (Cross-Origin Resource Sharing) support for Invoking from the browser
Source: https://supabase.com/docs/guides/functions/cors

CORS (Cross-Origin Resource Sharing) support for Invoking from the browser

To invoke edge functions from the browser, you need to handleCORS Preflightrequests.

See theexample on GitHub.


### Recommended setup#
For@supabase/supabase-jsv2.95.0 and later:Import CORS headers directly from the SDK to ensure they stay synchronized with any new headers added to the client libraries.

ImportcorsHeadersfrom@supabase/supabase-js/corsto automatically get all required headers:

```javascript
1import { corsHeaders } from '@supabase/supabase-js/cors'23console.log(`Function "browser-with-cors" up and running!`)45Deno.serve(async (req) => {6  // This is needed if you're planning to invoke your function from a browser.7  if (req.method === 'OPTIONS') {8    return new Response('ok', { headers: corsHeaders })9  }1011  try {12    const { name } = await req.json()13    const data = {14      message: `Hello ${name}!`,15    }1617    return new Response(JSON.stringify(data), {18      headers: { ...corsHeaders, 'Content-Type': 'application/json' },19      status: 200,20    })21  } catch (error) {22    return new Response(JSON.stringify({ error: error.message }), {23      headers: { ...corsHeaders, 'Content-Type': 'application/json' },24      status: 400,25    })26  }27})
```

This approach ensures that when new headers are added to the Supabase SDK, your Edge Functions automatically include them, preventing CORS errors.


### For versions before 2.95.0#
If you're using@supabase/supabase-jsbefore v2.95.0, you'll need to hardcode the CORS headers. Add acors.tsfile within a_sharedfolder:

```javascript
1export const  = {2  'Access-Control-Allow-Origin': '*',3  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',4}
```

Then import it in your function:

```javascript
1import { corsHeaders } from '../_shared/cors.ts'23// ... rest of your function code
```


================================================================================


# Dart Edge
Source: https://supabase.com/docs/guides/functions/dart-edge

Dart Edge

Be aware that the Dart Edge project is currently not actively maintained due to numerous breaking changes in Dart's development of (WASM) support.

Dart Edgeis an experimental project that enables you to write Supabase Edge Functions using Dart. It's built and maintained byInvertase.

For detailed information on how to set up and use Dart Edge with Supabase, refer to theofficial Dart Edge documentation for Supabase.


================================================================================


# Local Debugging
Source: https://supabase.com/docs/guides/functions/debugging-tools

Local Debugging


### Debug your Edge Functions locally using Chrome DevTools for easy breakpoint debugging and code inspection.
Debug your Edge Functions locally using Chrome DevTools for easy breakpoint debugging and code inspection.

Sincev1.171.0the Supabase CLI supports debugging Edge Functions via the v8 inspector protocol, allowing for debugging viaChrome DevToolsand other Chromium-based browsers.


### Inspect with Chrome Developer Tools#
```javascript
1supabase functions serve --inspect-mode brk
```



Now you should have Chrome DevTools configured and ready to debug your functions.


================================================================================


# Managing dependencies
Source: https://supabase.com/docs/guides/functions/dependencies

Managing dependencies


### Handle dependencies within Edge Functions.
Handle dependencies within Edge Functions.


### Importing dependencies#
Supabase Edge Functions support several ways to import dependencies:

- JavaScript modules from npm (https://docs.deno.com/examples/npm/)
- Built-inNode APIs
- Modules published toJSRordeno.land/x

```javascript
1// NPM packages (recommended)2import { createClient } from 'npm:@supabase/supabase-js@2'34// Node.js built-ins5import process from 'node:process'67// JSR modules (Deno's registry)8import path from 'jsr:@std/path@1.0.8'
```


### Usingdeno.json(recommended)#
Each function should have its owndeno.jsonfile to manage dependencies and configure Deno-specific settings. This ensures proper isolation between functions and is the recommended approach for deployment. When you update the dependencies for one function, it won't accidentally break another function that needs different versions.

```javascript
1{2  "imports": {3    "supabase": "npm:@supabase/supabase-js@2",4    "lodash": "https://cdn.skypack.dev/lodash"5  }6}
```

You can add this file directly to the function’s own directory:

```javascript
1└── supabase2    ├── functions3    │   ├── function-one4    │   │   ├── index.ts5    │   │   └── deno.json    # Function-specific Deno configuration6    │   └── function-two7    │       ├── index.ts8    │       └── deno.json    # Function-specific Deno configuration9    └── config.toml
```

It's possible to use a globaldeno.jsonin the/supabase/functionsdirectory for local development, but this approach is not recommended for deployment. Each function should maintain its own configuration to ensure proper isolation and dependency management.


### Using import maps (legacy)#
Import Maps are a legacy way to manage dependencies, similar to apackage.jsonfile. While still supported, we recommend usingdeno.json. If both exist,deno.jsontakes precedence.

Each function should have its ownimport_map.jsonfile for proper isolation:

```javascript
1# /function-one/import_map.json2{3  "imports": {4    "lodash": "https://cdn.skypack.dev/lodash"5  }6}
```

This JSON file should be located within the function’s own directory:

```javascript
1└── supabase2    ├── functions3    │   ├── function-one4    │   │   ├── index.ts5    │   │   └── import_map.json    # Function-specific import map
```

It's possible to use a globalimport_map.jsonin the/supabase/functionsdirectory for local development, but this approach is not recommended for deployment. Each function should maintain its own configuration to ensure proper isolation and dependency management.

If you’re using import maps with VSCode, update your.vscode/settings.jsonto point to your function-specific import map:

```javascript
1{2  "deno.enable": true,3  "deno.unstable": ["bare-node-builtins", "byonm"],4  "deno.importMap": "./supabase/functions/function-one/import_map.json"5}
```

You can override the default import map location using the--import-map <string>flag with serve and deploy commands, or by setting theimport_mapproperty in yourconfig.tomlfile:

```javascript
1[functions.my-function]2import_map = "./supabase/functions/function-one/import_map.json"
```


### Private NPM packages#
To use private npm packages, create a.npmrcfile within your function’s own directory.

```javascript
1└── supabase2    └── functions3        └── my-function4            ├── index.ts5            ├── deno.json6            └── .npmrc       # Function-specific npm configuration
```

It's possible to use a global.npmrcin the/supabase/functionsdirectory for local development, but this approach is not recommended for deployment. Each function should maintain its own configuration to ensure proper isolation and dependency management.

Add your registry details in the.npmrcfile. Followthis guideto learn more about the syntax of npmrc files.

```javascript
1# /my-function/.npmrc2@myorg:registry=https://npm.registryhost.com3//npm.registryhost.com/:_authToken=VALID_AUTH_TOKEN
```

After configuring your.npmrc, you can import the private package in your function code:

```javascript
1import package from 'npm:@myorg/private-package@v1.0.1'
```


### Using a custom NPM registry#
Some organizations require a custom NPM registry for security and compliance purposes. In such cases, you can specify the custom NPM registry to use viaNPM_CONFIG_REGISTRYenvironment variable.

You can define it in the project's.envfile or directly specify it when running the deploy command:

```javascript
1NPM_CONFIG_REGISTRY=https://custom-registry/ supabase functions deploy my-function
```


### Importing types#
If yourenvironment is set up properlyand the module you're importing is exporting types, the import will have types and autocompletion support.

Some npm packages may not ship out of the box types and you may need to import them from a separate package. You can specify their types with a@deno-typesdirective:

```javascript
1// @deno-types="npm:@types/express@^4.17"2import express from 'npm:express@^4.17'
```

To include types for built-in Node APIs, add the following line to the top of your imports:

```javascript
1/// <reference types="npm:@types/node" />
```


================================================================================


# Deploy to Production
Source: https://supabase.com/docs/guides/functions/deploy

Deploy to Production


### Deploy your Edge Functions to your remote Supabase Project.
Deploy your Edge Functions to your remote Supabase Project.

Once you have developed your Edge Functions locally, you can deploy them to your Supabase project.

Before getting started, make sure you have the Supabase CLI installed. Check out the CLI installation guide for installation methods and troubleshooting.


### Step 1: Authenticate#
Log in to the Supabase CLI if you haven't already:

```javascript
1supabase login
```


### Step 2: Connect your project#
Get the project ID associated with your function:

```javascript
1supabase projects list
```

If you haven't yet created a Supabase project, you can do so by visitingdatabase.new.

Linkyour local project to your remote Supabase project using the ID you just retrieved:

```javascript
1supabase link --project-ref your-project-id
```

Now you should have your local development environment connected to your production project.


### Step 3: Deploy Functions#
You can deploy all edge functions within thefunctionsfolder with a single command:

```javascript
1supabase functions deploy
```

Or deploy individual Edge Functions by specifying the function name:

```javascript
1supabase functions deploy hello-world
```


### Deploying public functions#
By default, Edge Functions require a valid JWT in the authorization header. If you want to deploy Edge Functions without Authorization checks (commonly used for Stripe webhooks), you can pass the--no-verify-jwtflag:

```javascript
1supabase functions deploy hello-world --no-verify-jwt
```

Be careful when using this flag, as it will allow anyone to invoke your Edge Function without a valid JWT. The Supabase client libraries automatically handle authorization.


### Step 4: Verify successful deployment#
🎉 Your function is now live!

When the deployment is successful, your function is automatically distributed to edge locations worldwide. Your edge functions is now running globally athttps://[YOUR_PROJECT_ID].supabase.co/functions/v1/hello-world.


### Step 5: Test your live function#
You can now invoke your Edge Function using the project'sANON_KEY, which can be found in theAPI settingsof the Supabase Dashboard. You can invoke it from within your app:

```javascript
1curl --request POST 'https://<project_id>.supabase.co/functions/v1/hello-world' \2  --header 'Authorization: Bearer ANON_KEY' \3  --header 'Content-Type: application/json' \4  --data '{ "name":"Functions" }'
```

Note that theSUPABASE_PUBLISHABLE_KEYis different in development and production. To get your production anon key, you can find it in your Supabase dashboard under Settings > API.

You should now see the expected response:

```javascript
1{ "message": "Hello Production!" }
```

You can also test the function through the Dashboard. To see how that works, check out theDashboard Quickstart guide.


### CI/CD deployment#
You can use popular CI / CD tools like GitHub Actions, Bitbucket, and GitLab CI to automate Edge Function deployments.


### GitHub Actions#
You can use the officialsetup-cliGitHub Actionto run Supabase CLI commands in your GitHub Actions.

The following GitHub Action deploys all Edge Functions any time code is merged into themainbranch:

```javascript
1name: Deploy Function23on:4  push:5    branches:6      - main7  workflow_dispatch:89jobs:10  deploy:11    runs-on: ubuntu-latest1213    env:14      SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}15      PROJECT_ID: your-project-id1617    steps:18      - uses: actions/checkout@v41920      - uses: supabase/setup-cli@v121        with:22          version: latest2324      - run: supabase functions deploy --project-ref $PROJECT_ID
```


### GitLab CI#
Here is the sample pipeline configuration to deploy via GitLab CI.

```javascript
1image: node:2023# List of stages for jobs, and their order of execution4stages:5  - setup6  - deploy78# This job runs in the setup stage, which runs first.9setup-npm:10  stage: setup11  script:12    - npm i supabase13  cache:14    paths:15      - node_modules/16  artifacts:17    paths:18      - node_modules/1920# This job runs in the deploy stage, which only starts when the job in the build stage completes successfully.21deploy-function:22  stage: deploy23  script:24    - npx supabase init25    - npx supabase functions deploy --debug26  services:27    - docker:dind28  variables:29    DOCKER_HOST: tcp://docker:2375
```


### Bitbucket Pipelines#
Here is the sample pipeline configuration to deploy via Bitbucket.

```javascript
1image: node:2023pipelines:4  default:5    - step:6        name: Setup7        caches:8          - node9        script:10          - npm i supabase11    - parallel:12        - step:13            name: Functions Deploy14            script:15              - npx supabase init16              - npx supabase functions deploy --debug17            services:18              - docker
```


### Function configuration#
Individual function configuration likeJWT verificationandimport map locationcan be set via theconfig.tomlfile.

```javascript
1[functions.hello-world]2verify_jwt = false
```

This ensures your function configurations are consistent across all environments and deployments.


### Example#
This example shows a GitHub Actions workflow that deploys all Edge Functions when code is merged into themainbranch.

```javascript
1name: Deploy Function23on:4  push:5    branches:6      - main7  workflow_dispatch:89jobs:10  deploy:11    runs-on: ubuntu-latest1213    env:14      SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}15      SUPABASE_PROJECT_ID: ${{ secrets.SUPABASE_PROJECT_ID }}1617    steps:18      - uses: actions/checkout@v31920      - uses: supabase/setup-cli@v121        with:22          version: latest2324      - run: supabase functions deploy --project-ref $SUPABASE_PROJECT_ID
```


================================================================================


# Development Environment
Source: https://supabase.com/docs/guides/functions/development-environment

Development Environment


### Set up your local development environment for Edge Functions.
Set up your local development environment for Edge Functions.

Before getting started, make sure you have the Supabase CLI installed. Check out theCLI installation guidefor installation methods and troubleshooting.


### Step 1: Install Deno CLI#
The Supabase CLI doesn't use the standard Deno CLI to serve functions locally. Instead, it uses its own Edge Runtime to keep the development and production environment consistent.

You can follow theDeno guidefor setting up your development environment with your favorite editor/IDE.

The benefit of installing Deno separately is that you can use the Deno LSP to improve your editor's autocompletion, type checking, and testing. You can also use Deno's built-in tools such asdeno fmt,deno lint, anddeno test.

After installing, you should have Deno installed and available in your terminal. Verify withdeno --version


### Step 2: Set up your editor#
Set up your editor environment for proper TypeScript support, autocompletion, and error detection.


### VSCode/Cursor (recommended)#
Install the Deno extensionfrom the VSCode marketplace

Option 1: Auto-generate (easiest)When runningsupabase init, selectywhen prompted "Generate VS Code settings for Deno? [y/N]"

Option 2: Manual setup

Create a.vscode/settings.jsonin your project root:

```javascript
1{2  "deno.enablePaths": ["./supabase/functions"],3  "deno.importMap": "./supabase/functions/import_map.json"4}
```

This configuration enables the Deno language server only for thesupabase/functionsfolder, while using VSCode's built-in JavaScript/TypeScript language server for all other files.


### Multi-root workspaces#
The standard.vscode/settings.jsonsetup works perfectly for projects where your Edge Functions live alongside your main application code. However, you might need multi-root workspaces if your development setup involves:

- Multiple repositories:Edge Functions in one repo, main app in another
- Microservices:Several services you need to develop in parallel

For this development workflow, createedge-functions.code-workspace:

```javascript
1{2  "folders": [3    {4      "name": "project-root",5      "path": "./"6    },7    {8      "name": "test-client",9      "path": "app"10    },11    {12      "name": "supabase-functions",13      "path": "supabase/functions"14    }15  ],16  "settings": {17    "files.exclude": {18      "node_modules/": true,19      "app/": true,20      "supabase/functions/": true21    },22    "deno.importMap": "./supabase/functions/import_map.json"23  }24}
```

You can find the complete example onGitHub.


### Recommended project structure#
It's recommended to organize your functions according to the following structure:

```javascript
1└── supabase2    ├── functions3    │   ├── import_map.json     # Top-level import map4    │   ├── _shared             # Shared code (underscore prefix)5    │   │   ├── supabaseAdmin.ts # Supabase client with SERVICE_ROLE key6    │   │   ├── supabaseClient.ts # Supabase client with ANON key7    │   │   └── cors.ts         # Reusable CORS headers8    │   ├── function-one        # Use hyphens for function names9    │   │   └── index.ts10    │   └── function-two11    │       └── index.ts12    ├── tests13    │   ├── function-one-test.ts14    │   └── function-two-test.ts15    ├── migrations16    └── config.toml
```

- Use "fat functions". Develop few, large functions by combining related functionality. This minimizes cold starts.
- Name functions with hyphens (-). This is the most URL-friendly approach
- Store shared code in_shared. Store any shared code in a folder prefixed with an underscore (_).
- Separate tests. Use a separate folder forUnit Teststhat includes the name of the function followed by a-testsuffix.


### Essential CLI commands#
Get familiar with the most commonly used CLI commands for developing and deploying Edge Functions.


### supabase start#
This command spins up your entire Supabase stack locally: database, auth, storage, and Edge Functions runtime. You're developing against the exact same environment you'll deploy to.


### supabase functions serve [function-name]#
Develop a specific function with hot reloading. Your functions run athttp://localhost:54321/functions/v1/[function-name]. When you save your file, you’ll see the changes instantly without having to wait.

Alternatively, usesupabase functions serveto serve all functions at once.


### supabase functions serve hello-world --no-verify-jwt#
If you want to serve an Edge Function without the default JWT verification. This is important for webhooks from Stripe, GitHub, etc. These services don't have your JWT tokens, so you need to skip auth verification.

Be careful when disabling JWT verification, as it allows anyone to call your function, so only use it for functions that are meant to be publicly accessible.


### supabase functions deploy hello-world#
Deploy the function when you’re ready


================================================================================


# Development tips
Source: https://supabase.com/docs/guides/functions/development-tips

Development tips


### Tips for getting started with Edge Functions.
Tips for getting started with Edge Functions.

Here are a few recommendations when you first start developing Edge Functions.


### Skipping authorization checks#
By default, Edge Functions require a valid JWT in the authorization header. If you want to use Edge Functions without Authorization checks (commonly used for Stripe webhooks), you can pass the--no-verify-jwtflag when serving your Edge Functions locally.

```javascript
1supabase functions serve hello-world --no-verify-jwt
```

Be careful when using this flag, as it will allow anyone to invoke your Edge Function without a valid JWT. The Supabase client libraries automatically handle authorization.


### Using HTTP methods#
Edge Functions supportGET,POST,PUT,PATCH,DELETE, andOPTIONS. A Function can be designed to perform different actions based on a request's HTTP method. See theexample on building a RESTful serviceto learn how to handle different HTTP methods in your Function.

HTML content is not supported.GETrequests that returntext/htmlwill be rewritten totext/plain.


### Naming Edge Functions#
We recommend using hyphens to name functions because hyphens are the most URL-friendly of all the naming conventions (snake_case, camelCase, PascalCase).


### Organizing your Edge Functions#
We recommend developing "fat functions". This means that you should develop few large functions, rather than many small functions. One common pattern when developing Functions is that you need to share code between two or more Functions. To do this, you can store any shared code in a folder prefixed with an underscore (_). We also recommend a separate folder forUnit Testsincluding the name of the function followed by a-testsuffix.
We recommend this folder structure:

```javascript
1└── supabase2    ├── functions3    │   ├── import_map.json # A top-level import map to use across functions.4    │   ├── _shared5    │   │   ├── supabaseAdmin.ts # Supabase client with SERVICE_ROLE key.6    │   │   └── supabaseClient.ts # Supabase client with ANON key.7    │   │   └── cors.ts # Reusable CORS headers.8    │   ├── function-one # Use hyphens to name functions.9    │   │   └── index.ts10    │   └── function-two11    │   │   └── index.ts12    │   └── tests13    │       └── function-one-test.ts14    │       └── function-two-test.ts15    ├── migrations16    └── config.toml
```


### Using config.toml#
Individual function configuration likeJWT verificationandimport map locationcan be set via theconfig.tomlfile.

```javascript
1[functions.hello-world]2verify_jwt = false3import_map = './import_map.json'
```


### Not using TypeScript#
When you create a new Edge Function, it will use TypeScript by default. However, it is possible to write and deploy Edge Functions using pure JavaScript.

Save your Function as a JavaScript file (e.g.index.js) and then update thesupabase/config.tomlas follows:

entrypointis available only in Supabase CLI version 1.215.0 or higher.

```javascript
1[functions.hello-world]2# other entries3entrypoint = './functions/hello-world/index.js' # path must be relative to config.toml
```

You can use any.ts,.js,.tsx,.jsxor.mjsfile as theentrypointfor a Function.


### Error handling#
Thesupabase-jslibrary provides several error types that you can use to handle errors that might occur when invoking Edge Functions:

```javascript
1import { FunctionsHttpError, FunctionsRelayError, FunctionsFetchError } from '@supabase/supabase-js'23const { data, error } = await supabase.functions.invoke('hello', {4  headers: { 'my-custom-header': 'my-custom-header-value' },5  body: { foo: 'bar' },6})78if (error instanceof FunctionsHttpError) {9  const errorMessage = await error.context.json()10  console.log('Function returned an error', errorMessage)11} else if (error instanceof FunctionsRelayError) {12  console.log('Relay error:', error.message)13} else if (error instanceof FunctionsFetchError) {14  console.log('Fetch error:', error.message)15}
```


### Database Functions vs Edge Functions#
For data-intensive operations we recommend usingDatabase Functions, which are executed within your database and can be called remotely using theREST and GraphQL API.

For use-cases which require low-latency we recommendEdge Functions, which are globally-distributed and can be written in TypeScript.


================================================================================


# File Storage
Source: https://supabase.com/docs/guides/functions/ephemeral-storage

File Storage


### Use persistent and ephemeral file storage
Use persistent and ephemeral file storage

Edge Functions provides two flavors of file storage:

- Persistent - backed by S3 protocol, can read/write from any S3 compatible bucket, including Supabase Storage
- Ephemeral - You can read and write files to the/tmpdirectory. Only suitable for temporary operations

You can use file storage to:

- Handle complex file transformations and workflows
- Do data migrations between projects
- Process user uploaded files and store them
- Unzip archives and process contents before saving to database


### Persistent Storage#
The persistent storage option is built on top of the S3 protocol. It allows you to mount any S3-compatible bucket, including Supabase Storage Buckets, as a directory for your Edge Functions.
You can perform operations such as reading and writing files to the mounted buckets as you would in a POSIX file system.

To access an S3 bucket from Edge Functions, you must set the following for environment variables in Edge Function Secrets.

- S3FS_ENDPOINT_URL
- S3FS_REGION
- S3FS_ACCESS_KEY_ID
- S3FS_SECRET_ACCESS_KEY

Follow this guideto enable and create an access key for Supabase Storage S3.

To access a file path in your mounted bucket from your Edge Function, use the prefix/s3/YOUR-BUCKET-NAME.

```javascript
1// read from S3 bucket2const data = await Deno.readFile('/s3/my-bucket/results.csv')34// make a directory5await Deno.mkdir('/s3/my-bucket/sub-dir')67// write to S3 bucket8await Deno.writeTextFile('/s3/my-bucket/demo.txt', 'hello world')
```


### Ephemeral storage#
Ephemeral storage will reset on each function invocation. This means the files you write during an invocation can only be read within the same invocation.

You can useDeno File System APIsor thenode:fsmodule to access the/tmppath.

```javascript
1Deno.serve(async (req) => {2  if (req.headers.get('content-type') !== 'application/zip') {3    return new Response('file must be a zip file', {4      status: 400,5    })6  }78  const uploadId = crypto.randomUUID()9  await Deno.writeFile('/tmp/' + uploadId, req.body)1011  // E.g. extract and process the zip file12  const zipFile = await Deno.readFile('/tmp/' + uploadId)13  // You could use a zip library to extract contents14  const extracted = await extractZip(zipFile)1516  // Or process the file directly17  console.log(`Processing zip file: ${uploadId}, size: ${zipFile.length} bytes`)18})
```


### Common use cases#

### Archive processing with background tasks#
You can use ephemeral storage withBackground Tasksto handle large file processing operations that exceed memory limits.

Imagine you have a Photo Album application that accepts photo uploads as zip files. A streaming implementation will run into memory limit errors with zip files exceeding 100MB, as it retains all archive files in memory simultaneously.

You can write the zip file to ephemeral storage first, then use a background task to extract and upload files to Supabase Storage. This way, you only read parts of the zip file to the memory.

```javascript
1import { BlobWriter, ZipReader } from 'https://deno.land/x/zipjs/index.js'2import { createClient } from 'jsr:@supabase/supabase-js@2'34const supabase = createClient(5  Deno.env.get('SUPABASE_URL'),6  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')7)89async function processZipFile(uploadId: string, filepath: string) {10  const file = await Deno.open(filepath, { read: true })11  const zipReader = new ZipReader(file.readable)12  const entries = await zipReader.getEntries()1314  await supabase.storage.createBucket(uploadId, { public: false })1516  await Promise.all(17    entries.map(async (entry) => {18      if (entry.directory) return1920      // Read file entry from temp storage21      const blobWriter = new BlobWriter()22      const blob = await entry.getData(blobWriter)2324      // Upload to permanent storage25      await supabase.storage.from(uploadId).upload(entry.filename, blob)2627      console.log('uploaded', entry.filename)28    })29  )3031  await zipReader.close()32}3334Deno.serve(async (req) => {35  const uploadId = crypto.randomUUID()36  const filepath = `/tmp/${uploadId}.zip`3738  // Write zip to ephemeral storage39  await Deno.writeFile(filepath, req.body)4041  // Process in background to avoid memory limits42  EdgeRuntime.waitUntil(processZipFile(uploadId, filepath))4344  return new Response(JSON.stringify({ uploadId }), {45    headers: { 'Content-Type': 'application/json' },46  })47})
```


### Image manipulation#
Custom image manipulation workflows usingmagick-wasm.

```javascript
1Deno.serve(async (req) => {2  // Save uploaded image to temp storage3  const imagePath = `/tmp/input-${crypto.randomUUID()}.jpg`4  await Deno.writeFile(imagePath, req.body)56  // Process image with magick-wasm7  const processedPath = `/tmp/output-${crypto.randomUUID()}.jpg`8  // ... image manipulation logic910  // Read processed image and return11  const processedImage = await Deno.readFile(processedPath)12  return new Response(processedImage, {13    headers: { 'Content-Type': 'image/jpeg' },14  })15})
```


### Using synchronous file APIs#
You can safely use the following synchronous Deno APIs (and their Node counterparts)during initial script evaluation:

- Deno.statSync
- Deno.removeSync
- Deno.writeFileSync
- Deno.writeTextFileSync
- Deno.readFileSync
- Deno.readTextFileSync
- Deno.mkdirSync
- Deno.makeTempDirSync
- Deno.readDirSync

Keep in mindthat the sync APIs are available only during initial script evaluation and aren’t supported in callbacks like HTTP handlers orsetTimeout.

```javascript
1Deno.statSync('...') // ✅23setTimeout(() => {4  Deno.statSync('...') // 💣 ERROR! Deno.statSync is blocklisted on the current context5})67Deno.serve(() => {8  Deno.statSync('...') // 💣 ERROR! Deno.statSync is blocklisted on the current context9})
```


### Limits#
There are no limits on S3 buckets you mount for Persistent storage.

Ephemeral Storage:

- Free projects: Up to 256MB of ephemeral storage
- Paid projects: Up to 512MB of ephemeral storage


================================================================================


# Error Handling
Source: https://supabase.com/docs/guides/functions/error-handling

Error Handling


### Implement proper error responses and client-side handling to create reliable applications.
Implement proper error responses and client-side handling to create reliable applications.


### Error handling#
Implementing the right error responses and client-side handling helps with debugging and makes your functions much easier to maintain in production.

Within your Edge Functions, return proper HTTP status codes and error messages:

```javascript
1Deno.serve(async (req) => {2  try {3    // Your function logic here4    const result = await processRequest(req)5    return new Response(JSON.stringify(result), {6      headers: { 'Content-Type': 'application/json' },7      status: 200,8    })9  } catch (error) {10    console.error('Function error:', error)11    return new Response(JSON.stringify({ error: error.message }), {12      headers: { 'Content-Type': 'application/json' },13      status: 500,14    })15  }16})
```

Best practices for function errors:

- Use the right HTTP status code for each situation. Return400for bad user input, 404 when something doesn't exist, 500 for server errors, etc. This helps with debugging and lets client apps handle different error types appropriately.
- Include helpful error messages in the response body
- Log errors to the console for debugging (visible in the Logs tab)


### Client-side error handling#
Within your client-side code, an Edge Function can throw three types of errors:

- FunctionsHttpError: Your function executed but returned an error (4xx/5xx status)
- FunctionsRelayError: Network issue between client and Supabase
- FunctionsFetchError: Function couldn't be reached at all

```javascript
1import { FunctionsHttpError, FunctionsRelayError, FunctionsFetchError } from '@supabase/supabase-js'23const { data, error } = await supabase.functions.invoke('hello', {4  headers: { 'my-custom-header': 'my-custom-header-value' },5  body: { foo: 'bar' },6})78if (error instanceof FunctionsHttpError) {9  const errorMessage = await error.context.json()10  console.log('Function returned an error', errorMessage)11} else if (error instanceof FunctionsRelayError) {12  console.log('Relay error:', error.message)13} else if (error instanceof FunctionsFetchError) {14  console.log('Fetch error:', error.message)15}
```

Make sure to handle the errors properly. Functions that fail silently are hard to debug, functions with clear error messages get fixed fast.


### Error monitoring#
You can see the production error logs in the Logs tab of your Supabase Dashboard.



For more information on Logging, check outthis guide.


================================================================================


# Generate Images with Amazon Bedrock
Source: https://supabase.com/docs/guides/functions/examples/amazon-bedrock-image-generator

Generate Images with Amazon Bedrock

Amazon Bedrockis a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon. Each model is accessible through a common API which implements a broad set of features to help build generative AI applications with security, privacy, and responsible AI in mind.

This guide will walk you through an example using the Amazon Bedrock JavaScript SDK in Supabase Edge Functions to generate images using theAmazon Titan Image Generator G1model.


### Setup#
- In your AWS console, navigate to Amazon Bedrock and under "Request model access", select the Amazon Titan Image Generator G1 model.
- In your Supabase project, create a.envfile in thesupabasedirectory with the following contents:

```javascript
1AWS_DEFAULT_REGION="<your_region>"2AWS_ACCESS_KEY_ID="<replace_your_own_credentials>"3AWS_SECRET_ACCESS_KEY="<replace_your_own_credentials>"4AWS_SESSION_TOKEN="<replace_your_own_credentials>"56# Mocked config files7AWS_SHARED_CREDENTIALS_FILE="./aws/credentials"8AWS_CONFIG_FILE="./aws/config"
```


### Configure Storage#
- [locally] Runsupabase start
- Open Studio URL:locally|hosted
- Navigate to Storage
- Click "New bucket"
- Create a new public bucket called "images"


### Code#
Create a new function in your project:

```javascript
1supabase functions new amazon-bedrock
```

And add the code to theindex.tsfile:

```javascript
1// We need to mock the file system for the AWS SDK to work.2import { prepareVirtualFile } from 'https://deno.land/x/mock_file@v1.1.2/mod.ts'34import { BedrockRuntimeClient, InvokeModelCommand } from 'npm:@aws-sdk/client-bedrock-runtime'5import { createClient } from 'npm:@supabase/supabase-js'6import { decode } from 'npm:base64-arraybuffer'78console.log('Hello from Amazon Bedrock!')910Deno.serve(async (req) => {11  prepareVirtualFile('./aws/config')12  prepareVirtualFile('./aws/credentials')1314  const client = new BedrockRuntimeClient({15    region: Deno.env.get('AWS_DEFAULT_REGION') ?? 'us-west-2',16    credentials: {17      accessKeyId: Deno.env.get('AWS_ACCESS_KEY_ID') ?? '',18      secretAccessKey: Deno.env.get('AWS_SECRET_ACCESS_KEY') ?? '',19      sessionToken: Deno.env.get('AWS_SESSION_TOKEN') ?? '',20    },21  })2223  const { prompt, seed } = await req.json()24  console.log(prompt)25  const input = {26    contentType: 'application/json',27    accept: '*/*',28    modelId: 'amazon.titan-image-generator-v1',29    body: JSON.stringify({30      taskType: 'TEXT_IMAGE',31      textToImageParams: { text: prompt },32      imageGenerationConfig: {33        numberOfImages: 1,34        quality: 'standard',35        cfgScale: 8.0,36        height: 512,37        width: 512,38        seed: seed ?? 0,39      },40    }),41  }4243  const command = new InvokeModelCommand(input)44  const response = await client.send(command)45  console.log(response)4647  if (response.$metadata.httpStatusCode === 200) {48    const { body, $metadata } = response4950    const textDecoder = new TextDecoder('utf-8')51    const jsonString = textDecoder.decode(body.buffer)52    const parsedData = JSON.parse(jsonString)53    console.log(parsedData)54    const image = parsedData.images[0]5556    const supabaseClient = createClient(57      // Supabase API URL - env var exported by default.58      Deno.env.get('SUPABASE_URL')!,59      // Supabase API ANON KEY - env var exported by default.60      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!61    )6263    const { data: upload, error: uploadError } = await supabaseClient.storage64      .from('images')65      .upload(`${$metadata.requestId ?? ''}.png`, decode(image), {66        contentType: 'image/png',67        cacheControl: '3600',68        upsert: false,69      })70    if (!upload) {71      return Response.json(uploadError)72    }73    const { data } = supabaseClient.storage.from('images').getPublicUrl(upload.path!)74    return Response.json(data)75  }7677  return Response.json(response)78})
```


### Run the function locally#
```javascript
1curl -i --location --request POST 'http://127.0.0.1:54321/functions/v1/amazon-bedrock' \2    --header 'Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0' \3    --header 'Content-Type: application/json' \4    --data '{"prompt":"A beautiful picture of a bird"}'
```


### Deploy to your hosted project#
```javascript
1supabase link2supabase functions deploy amazon-bedrock3supabase secrets set --env-file supabase/.env
```

You've now deployed a serverless function that uses AI to generate and upload images to your Supabase storage bucket.


================================================================================


# Custom Auth Emails with React Email and Resend
Source: https://supabase.com/docs/guides/functions/examples/auth-send-email-hook-react-email-resend

Custom Auth Emails with React Email and Resend

Use thesend email hookto send custom auth emails withReact EmailandResendin Supabase Edge Functions.

Prefer to jump straight to the code?Check out the example on GitHub.


### Prerequisites#
To get the most out of this guide, you’ll need to:

- Create a Resend API key
- Verify your domain

Make sure you have the latest version of theSupabase CLIinstalled.


### 1. Create Supabase function#
Create a new function locally:

```javascript
1supabase functions new send-email
```


### 2. Edit the handler function#
Paste the following code into theindex.tsfile:

```javascript
1import React from 'npm:react@18.3.1'2import { Webhook } from 'https://esm.sh/standardwebhooks@1.0.0'3import { Resend } from 'npm:resend@4.0.0'4import { renderAsync } from 'npm:@react-email/components@0.0.22'5import { MagicLinkEmail } from './_templates/magic-link.tsx'67const resend = new Resend(Deno.env.get('RESEND_API_KEY') as string)8const hookSecret = (Deno.env.get('SEND_EMAIL_HOOK_SECRET') as string).replace('v1,whsec_', '')910Deno.serve(async (req) => {11  if (req.method !== 'POST') {12    return new Response('not allowed', { status: 400 })13  }1415  const payload = await req.text()16  const headers = Object.fromEntries(req.headers)17  const wh = new Webhook(hookSecret)18  try {19    const {20      user,21      email_data: { token, token_hash, redirect_to, email_action_type },22    } = wh.verify(payload, headers) as {23      user: {24        email: string25      }26      email_data: {27        token: string28        token_hash: string29        redirect_to: string30        email_action_type: string31        site_url: string32        token_new: string33        token_hash_new: string34      }35    }3637    const html = await renderAsync(38      React.createElement(MagicLinkEmail, {39        supabase_url: Deno.env.get('SUPABASE_URL') ?? '',40        token,41        token_hash,42        redirect_to,43        email_action_type,44      })45    )4647    const { error } = await resend.emails.send({48      from: 'welcome <onboarding@resend.dev>',49      to: [user.email],50      subject: 'Supa Custom MagicLink!',51      html,52    })53    if (error) {54      throw error55    }56  } catch (error) {57    console.log(error)58    return new Response(59      JSON.stringify({60        error: {61          http_code: error.code,62          message: error.message,63        },64      }),65      {66        status: 401,67        headers: { 'Content-Type': 'application/json' },68      }69    )70  }7172  const responseHeaders = new Headers()73  responseHeaders.set('Content-Type', 'application/json')74  return new Response(JSON.stringify({}), {75    status: 200,76    headers: responseHeaders,77  })78})
```


### 3. Create React Email templates#
Create a new folder_templatesand create a new filemagic-link.tsxwith the following code:

```javascript
1import {2  Body,3  Container,4  Head,5  Heading,6  Html,7  Link,8  Preview,9  Text,10} from 'npm:@react-email/components@0.0.22'11import * as React from 'npm:react@18.3.1'1213interface MagicLinkEmailProps {14  supabase_url: string15  email_action_type: string16  redirect_to: string17  token_hash: string18  token: string19}2021export const MagicLinkEmail = ({22  token,23  supabase_url,24  email_action_type,25  redirect_to,26  token_hash,27}: MagicLinkEmailProps) => (28  <Html>29    <Head />30    <Preview>Log in with this magic link</Preview>31    <Body style={main}>32      <Container style={container}>33        <Heading style={h1}>Login</Heading>34        <Link35          href={`${supabase_url}/auth/v1/verify?token=${token_hash}&type=${email_action_type}&redirect_to=${redirect_to}`}36          target="_blank"37          style={{38            ...link,39            display: 'block',40            marginBottom: '16px',41          }}42        >43          Click here to log in with this magic link44        </Link>45        <Text style={{ ...text, marginBottom: '14px' }}>46          Or, copy and paste this temporary login code:47        </Text>48        <code style={code}>{token}</code>49        <Text50          style={{51            ...text,52            color: '#ababab',53            marginTop: '14px',54            marginBottom: '16px',55          }}56        >57          If you didn&apos;t try to login, you can safely ignore this email.58        </Text>59        <Text style={footer}>60          <Link61            href="https://demo.vercel.store/"62            target="_blank"63            style={{ ...link, color: '#898989' }}64          >65            ACME Corp66          </Link>67          , the famouse demo corp.68        </Text>69      </Container>70    </Body>71  </Html>72)7374export default MagicLinkEmail7576const main = {77  backgroundColor: '#ffffff',78}7980const container = {81  paddingLeft: '12px',82  paddingRight: '12px',83  margin: '0 auto',84}8586const h1 = {87  color: '#333',88  fontFamily:89    "-apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue', sans-serif",90  fontSize: '24px',91  fontWeight: 'bold',92  margin: '40px 0',93  padding: '0',94}9596const link = {97  color: '#2754C5',98  fontFamily:99    "-apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue', sans-serif",100  fontSize: '14px',101  textDecoration: 'underline',102}103104const text = {105  color: '#333',106  fontFamily:107    "-apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue', sans-serif",108  fontSize: '14px',109  margin: '24px 0',110}111112const footer = {113  color: '#898989',114  fontFamily:115    "-apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue', sans-serif",116  fontSize: '12px',117  lineHeight: '22px',118  marginTop: '12px',119  marginBottom: '24px',120}121122const code = {123  display: 'inline-block',124  padding: '16px 4.5%',125  width: '90.5%',126  backgroundColor: '#f4f4f4',127  borderRadius: '5px',128  border: '1px solid #eee',129  color: '#333',130}
```

You can find a selection of React Email templates in theReact Email Examples.


### 4. Deploy the Function#
Deploy function to Supabase:

```javascript
1supabase functions deploy send-email --no-verify-jwt
```

Note down the function URL, you will need it in the next step!


### 5. Configure the Send Email Hook#
- Go to theAuth Hookssection of the Supabase dashboard and create a new "Send Email hook".
- Select HTTPS as the hook type.
- Paste the function URL in the "URL" field.
- Click "Generate Secret" to generate your webhook secret and note it down.
- Click "Create" to save the hook configuration.

Store these secrets in your.envfile.

```javascript
1RESEND_API_KEY=your_resend_api_key2SEND_EMAIL_HOOK_SECRET="v1,whsec_<base64_secret>"
```

You can generate the secret in theAuth Hookssection of the Supabase dashboard.

Set the secrets from the.envfile:

```javascript
1supabase secrets set --env-file supabase/functions/.env
```

Now your Supabase Edge Function will be triggered anytime an Auth Email needs to be sent to the user!


### More resources#
- Send Email Hooks
- Auth Hooks


================================================================================


# CAPTCHA support with Cloudflare Turnstile
Source: https://supabase.com/docs/guides/functions/examples/cloudflare-turnstile

CAPTCHA support with Cloudflare Turnstile

Cloudflare Turnstileis a friendly, free CAPTCHA replacement, and it works seamlessly with Supabase Edge Functions to protect your forms.View on GitHub.


### Setup#
- Follow these steps to set up a new site:https://developers.cloudflare.com/turnstile/get-started/
- Add the Cloudflare Turnstile widget to your site:https://developers.cloudflare.com/turnstile/get-started/client-side-rendering/


### Code#
Create a new function in your project:

```javascript
1supabase functions new cloudflare-turnstile
```

And add the code to theindex.tsfile:

```javascript
1import { corsHeaders } from '@supabase/supabase-js/cors' // v2.95.0+23console.log('Hello from Cloudflare Trunstile!')45function ips(req: Request) {6  return req.headers.get('x-forwarded-for')?.split(/\s*,\s*/)7}89Deno.serve(async (req) => {10  // This is needed if you're planning to invoke your function from a browser.11  if (req.method === 'OPTIONS') {12    return new Response('ok', { headers: corsHeaders })13  }1415  const { token } = await req.json()16  const clientIps = ips(req) || ['']17  const ip = clientIps[0]1819  // Validate the token by calling the20  // "/siteverify" API endpoint.21  let formData = new FormData()22  formData.append('secret', Deno.env.get('CLOUDFLARE_SECRET_KEY') ?? '')23  formData.append('response', token)24  formData.append('remoteip', ip)2526  const url = 'https://challenges.cloudflare.com/turnstile/v0/siteverify'27  const result = await fetch(url, {28    body: formData,29    method: 'POST',30  })3132  const outcome = await result.json()33  console.log(outcome)34  if (outcome.success) {35    return new Response('success', { headers: corsHeaders })36  }37  return new Response('failure', { headers: corsHeaders })38})
```


### Deploy the server-side validation Edge Functions#
- https://developers.cloudflare.com/turnstile/get-started/server-side-validation/

```javascript
1supabase functions deploy cloudflare-turnstile2supabase secrets set CLOUDFLARE_SECRET_KEY=your_secret_key
```


### Invoke the function from your site#
```javascript
1const { data, error } = await supabase.functions.invoke('cloudflare-turnstile', {2  body: { token },3})
```


================================================================================


# Building a Discord Bot
Source: https://supabase.com/docs/guides/functions/examples/discord-bot

Building a Discord Bot


### Create an application on Discord Developer portal#
A new application is created which will hold our Slash Command. Don't close the tab as we need information from this application page throughout our development.

Before we can write some code, we need to curl a discord endpoint to register a Slash Command in our app.

FillDISCORD_BOT_TOKENwith the token available in theBotsection andCLIENT_IDwith the ID available on theGeneral Informationsection of the page and run the command on your terminal.

```javascript
1BOT_TOKEN='replace_me_with_bot_token'2CLIENT_ID='replace_me_with_client_id'3curl -X POST \4-H 'Content-Type: application/json' \5-H "Authorization: Bot $BOT_TOKEN" \6-d '{"name":"hello","description":"Greet a person","options":[{"name":"name","description":"The name of the person","type":3,"required":true}]}' \7"https://discord.com/api/v8/applications/$CLIENT_ID/commands"
```

This will register a Slash Command namedhellothat accepts a parameter namednameof type string.


### Code#
```javascript
1// Sift is a small routing library that abstracts away details like starting a2// listener on a port, and provides a simple function (serve) that has an API3// to invoke a function for a specific path.4import { json, serve, validateRequest } from 'https://deno.land/x/sift@0.6.0/mod.ts'5// TweetNaCl is a cryptography library that we use to verify requests6// from Discord.7import nacl from 'https://cdn.skypack.dev/tweetnacl@v1.0.3?dts'89enum DiscordCommandType {10  Ping = 1,11  ApplicationCommand = 2,12}1314// For all requests to "/" endpoint, we want to invoke home() handler.15serve({16  '/discord-bot': home,17})1819// The main logic of the Discord Slash Command is defined in this function.20async function home(request: Request) {21  // validateRequest() ensures that a request is of POST method and22  // has the following headers.23  const { error } = await validateRequest(request, {24    POST: {25      headers: ['X-Signature-Ed25519', 'X-Signature-Timestamp'],26    },27  })28  if (error) {29    return json({ error: error.message }, { status: error.status })30  }3132  // verifySignature() verifies if the request is coming from Discord.33  // When the request's signature is not valid, we return a 401 and this is34  // important as Discord sends invalid requests to test our verification.35  const { valid, body } = await verifySignature(request)36  if (!valid) {37    return json(38      { error: 'Invalid request' },39      {40        status: 401,41      }42    )43  }4445  const { type = 0, data = { options: [] } } = JSON.parse(body)46  // Discord performs Ping interactions to test our application.47  // Type 1 in a request implies a Ping interaction.48  if (type === DiscordCommandType.Ping) {49    return json({50      type: 1, // Type 1 in a response is a Pong interaction response type.51    })52  }5354  // Type 2 in a request is an ApplicationCommand interaction.55  // It implies that a user has issued a command.56  if (type === DiscordCommandType.ApplicationCommand) {57    const { value } = data.options.find(58      (option: { name: string; value: string }) => option.name === 'name'59    )60    return json({61      // Type 4 responds with the below message retaining the user's62      // input at the top.63      type: 4,64      data: {65        content: `Hello, ${value}!`,66      },67    })68  }6970  // We will return a bad request error as a valid Discord request71  // shouldn't reach here.72  return json({ error: 'bad request' }, { status: 400 })73}7475/** Verify whether the request is coming from Discord. */76async function verifySignature(request: Request): Promise<{ valid: boolean; body: string }> {77  const PUBLIC_KEY = Deno.env.get('DISCORD_PUBLIC_KEY')!78  // Discord sends these headers with every request.79  const signature = request.headers.get('X-Signature-Ed25519')!80  const timestamp = request.headers.get('X-Signature-Timestamp')!81  const body = await request.text()82  const valid = nacl.sign.detached.verify(83    new TextEncoder().encode(timestamp + body),84    hexToUint8Array(signature),85    hexToUint8Array(PUBLIC_KEY)86  )8788  return { valid, body }89}9091/** Converts a hexadecimal string to Uint8Array. */92function hexToUint8Array(hex: string) {93  return new Uint8Array(hex.match(/.{1,2}/g)!.map((val) => parseInt(val, 16)))94}
```


### Deploy the slash command handler#
```javascript
1supabase functions deploy discord-bot --no-verify-jwt2supabase secrets set DISCORD_PUBLIC_KEY=your_public_key
```

Navigate to your Function details in the Supabase Dashboard to get your Endpoint URL.


### Configure Discord application to use our URL as interactions endpoint URL#
The application is now ready. Let's proceed to the next section to install it.


### Install the slash command on your Discord server#
So to use thehelloSlash Command, we need to install our Greeter application on our Discord server. Here are the steps:

Open Discord, type/Promiseand pressEnter.


### Run locally#
```javascript
1supabase functions serve discord-bot --no-verify-jwt --env-file ./supabase/.env.local2ngrok http 54321
```


================================================================================


# Streaming Speech with ElevenLabs
Source: https://supabase.com/docs/guides/functions/examples/elevenlabs-generate-speech-stream

Streaming Speech with ElevenLabs


### Generate and stream speech through Supabase Edge Functions. Store speech in Supabase Storage and cache responses via built-in CDN.
Generate and stream speech through Supabase Edge Functions. Store speech in Supabase Storage and cache responses via built-in CDN.


### Introduction#
In this tutorial you will learn how to build an edge API to generate, stream, store, and cache speech using Supabase Edge Functions, Supabase Storage, andElevenLabs text to speech API.

Find theexample project on GitHub.


### Requirements#
- An ElevenLabs account with anAPI key.
- ASupabaseaccount (you can sign up for a free account viadatabase.new).
- TheSupabase CLIinstalled on your machine.
- TheDeno runtimeinstalled on your machine and optionallysetup in your favourite IDE.


### Setup#

### Create a Supabase project locally#
After installing theSupabase CLI, run the following command to create a new Supabase project locally:

```javascript
1supabase init
```


### Configure the storage bucket#
You can configure the Supabase CLI to automatically generate a storage bucket by adding this configuration in theconfig.tomlfile:

```javascript
1[storage.buckets.audio]2public = false3file_size_limit = "50MiB"4allowed_mime_types = ["audio/mp3"]5objects_path = "./audio"
```

Upon runningsupabase startthis will create a new storage bucket in your local Supabase project. Should you want to push this to your hosted Supabase project, you can runsupabase seed buckets --linked.


### Configure background tasks for Supabase Edge Functions#
To use background tasks in Supabase Edge Functions when developing locally, you need to add the following configuration in theconfig.tomlfile:

```javascript
1[edge_runtime]2policy = "per_worker"
```

When running withper_workerpolicy, Function won't auto-reload on edits. You will need to manually restart it by runningsupabase functions serve.


### Create a Supabase Edge Function for speech generation#
Create a new Edge Function by running the following command:

```javascript
1supabase functions new text-to-speech
```

If you're using VS Code or Cursor, selectywhen the CLI prompts "Generate VS Code settings for Deno? [y/N]"!


### Set up the environment variables#
Within thesupabase/functionsdirectory, create a new.envfile and add the following variables:

```javascript
1# Find / create an API key at https://elevenlabs.io/app/settings/api-keys2ELEVENLABS_API_KEY=your_api_key
```


### Dependencies#
The project uses a couple of dependencies:

- The@supabase/supabase-jslibrary to interact with the Supabase database.
- The ElevenLabsJavaScript SDKto interact with the text-to-speech API.
- The open-sourceobject-hashto generate a hash from the request parameters.

Since Supabase Edge Function uses theDeno runtime, you don't need to install the dependencies, rather you canimportthem via thenpm:prefix.


### Code the Supabase Edge Function#
In your newly createdsupabase/functions/text-to-speech/index.tsfile, add the following code:

```javascript
1// Setup type definitions for built-in Supabase Runtime APIs2import 'jsr:@supabase/functions-js/edge-runtime.d.ts'3import { createClient } from 'npm:@supabase/supabase-js@2'4import { ElevenLabsClient } from 'npm:elevenlabs@1.52.0'5import * as hash from 'npm:object-hash'67const supabase = createClient(8  Deno.env.get('SUPABASE_URL')!,9  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!10)1112const client = new ElevenLabsClient({13  apiKey: Deno.env.get('ELEVENLABS_API_KEY'),14})1516// Upload audio to Supabase Storage in a background task17async function uploadAudioToStorage(stream: ReadableStream, requestHash: string) {18  const { data, error } = await supabase.storage19    .from('audio')20    .upload(`${requestHash}.mp3`, stream, {21      contentType: 'audio/mp3',22    })2324  console.log('Storage upload result', { data, error })25}2627Deno.serve(async (req) => {28  // To secure your function for production, you can for example validate the request origin,29  // or append a user access token and validate it with Supabase Auth.30  console.log('Request origin', req.headers.get('host'))31  const url = new URL(req.url)32  const params = new URLSearchParams(url.search)33  const text = params.get('text')34  const voiceId = params.get('voiceId') ?? 'JBFqnCBsd6RMkjVDRZzb'3536  const requestHash = hash.MD5({ text, voiceId })37  console.log('Request hash', requestHash)3839  // Check storage for existing audio file40  const { data } = await supabase.storage.from('audio').createSignedUrl(`${requestHash}.mp3`, 60)4142  if (data) {43    console.log('Audio file found in storage', data)44    const storageRes = await fetch(data.signedUrl)45    if (storageRes.ok) return storageRes46  }4748  if (!text) {49    return new Response(JSON.stringify({ error: 'Text parameter is required' }), {50      status: 400,51      headers: { 'Content-Type': 'application/json' },52    })53  }5455  try {56    console.log('ElevenLabs API call')57    const response = await client.textToSpeech.convertAsStream(voiceId, {58      output_format: 'mp3_44100_128',59      model_id: 'eleven_multilingual_v2',60      text,61    })6263    const stream = new ReadableStream({64      async start(controller) {65        for await (const chunk of response) {66          controller.enqueue(chunk)67        }68        controller.close()69      },70    })7172    // Branch stream to Supabase Storage73    const [browserStream, storageStream] = stream.tee()7475    // Upload to Supabase Storage in the background76    EdgeRuntime.waitUntil(uploadAudioToStorage(storageStream, requestHash))7778    // Return the streaming response immediately79    return new Response(browserStream, {80      headers: {81        'Content-Type': 'audio/mpeg',82      },83    })84  } catch (error) {85    console.log('error', { error })86    return new Response(JSON.stringify({ error: error.message }), {87      status: 500,88      headers: { 'Content-Type': 'application/json' },89    })90  }91})
```


### Run locally#
To run the function locally, run the following commands:

```javascript
1supabase start
```

Once the local Supabase stack is up and running, run the following command to start the function and observe the logs:

```javascript
1supabase functions serve
```


### Try it out#
Navigate tohttp://127.0.0.1:54321/functions/v1/text-to-speech?text=hello%20worldto hear the function in action.

Afterwards, navigate tohttp://127.0.0.1:54323/project/default/storage/buckets/audioto see the audio file in your local Supabase Storage bucket.


### Deploy to Supabase#
If you haven't already, create a new Supabase account atdatabase.newand link the local project to your Supabase account:

```javascript
1supabase link
```

Once done, run the following command to deploy the function:

```javascript
1supabase functions deploy
```


### Set the function secrets#
Now that you have all your secrets set locally, you can run the following command to set the secrets in your Supabase project:

```javascript
1supabase secrets set --env-file supabase/functions/.env
```


### Test the function#
The function is designed in a way that it can be used directly as a source for an<audio>element.

```javascript
1<audio2  src="https://${SUPABASE_PROJECT_REF}.supabase.co/functions/v1/text-to-speech?text=Hello%2C%20world!&voiceId=JBFqnCBsd6RMkjVDRZzb"3  controls4/>
```

You can find an example frontend implementation in the complete code example onGitHub.


================================================================================


# Transcription Telegram Bot
Source: https://supabase.com/docs/guides/functions/examples/elevenlabs-transcribe-speech

Transcription Telegram Bot


### Build a Telegram bot that transcribes audio and video messages in 99 languages using TypeScript with Deno in Supabase Edge Functions.
Build a Telegram bot that transcribes audio and video messages in 99 languages using TypeScript with Deno in Supabase Edge Functions.


### Introduction#
In this tutorial you will learn how to build a Telegram bot that transcribes audio and video messages in 99 languages using TypeScript and the ElevenLabs Scribe model via thespeech to text API.

To check out what the end result will look like, you can test out thet.me/ElevenLabsScribeBot

Find theexample project on GitHub.


### Requirements#
- An ElevenLabs account with anAPI key.
- ASupabaseaccount (you can sign up for a free account viadatabase.new).
- TheSupabase CLIinstalled on your machine.
- TheDeno runtimeinstalled on your machine and optionallysetup in your favourite IDE.
- ATelegramaccount.


### Setup#

### Register a Telegram bot#
Use theBotFatherto create a new Telegram bot. Run the/newbotcommand and follow the instructions to create a new bot. At the end, you will receive your secret bot token. Note it down securely for the next step.




### Create a Supabase project locally#
After installing theSupabase CLI, run the following command to create a new Supabase project locally:

```javascript
1supabase init
```


### Create a database table to log the transcription results#
Next, create a new database table to log the transcription results:

```javascript
1supabase migrations new init
```

This will create a new migration file in thesupabase/migrationsdirectory. Open the file and add the following SQL:

```javascript
1CREATE TABLE IF NOT EXISTS transcription_logs (2  id BIGSERIAL PRIMARY KEY,3  file_type VARCHAR NOT NULL,4  duration INTEGER NOT NULL,5  chat_id BIGINT NOT NULL,6  message_id BIGINT NOT NULL,7  username VARCHAR,8  transcript TEXT,9  language_code VARCHAR,10  created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,11  error TEXT12);1314ALTER TABLE transcription_logs ENABLE ROW LEVEL SECURITY;
```


### Create a Supabase Edge Function to handle Telegram webhook requests#
Next, create a new Edge Function to handle Telegram webhook requests:

```javascript
1supabase functions new scribe-bot
```

If you're using VS Code or Cursor, selectywhen the CLI prompts "Generate VS Code settings for Deno? [y/N]"!


### Set up the environment variables#
Within thesupabase/functionsdirectory, create a new.envfile and add the following variables:

```javascript
1# Find / create an API key at https://elevenlabs.io/app/settings/api-keys2ELEVENLABS_API_KEY=your_api_key34# The bot token you received from the BotFather.5TELEGRAM_BOT_TOKEN=your_bot_token67# A random secret chosen by you to secure the function.8FUNCTION_SECRET=random_secret
```


### Dependencies#
The project uses a couple of dependencies:

- The open-sourcegrammY Frameworkto handle the Telegram webhook requests.
- The@supabase/supabase-jslibrary to interact with the Supabase database.
- The ElevenLabsJavaScript SDKto interact with the speech-to-text API.

Since Supabase Edge Function uses theDeno runtime, you don't need to install the dependencies, rather you canimportthem via thenpm:prefix.


### Code the Telegram bot#
In your newly createdscribe-bot/index.tsfile, add the following code:

```javascript
1import { Bot, webhookCallback } from 'https://deno.land/x/grammy@v1.34.0/mod.ts'2import 'jsr:@supabase/functions-js/edge-runtime.d.ts'3import { createClient } from 'npm:@supabase/supabase-js@2'4import { ElevenLabsClient } from 'npm:elevenlabs@1.50.5'56console.log(`Function "elevenlabs-scribe-bot" up and running!`)78const elevenLabsClient = new ElevenLabsClient({9  apiKey: Deno.env.get('ELEVENLABS_API_KEY') || '',10})1112const supabase = createClient(13  Deno.env.get('SUPABASE_URL') || '',14  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') || ''15)1617async function scribe({18  fileURL,19  fileType,20  duration,21  chatId,22  messageId,23  username,24}: {25  fileURL: string26  fileType: string27  duration: number28  chatId: number29  messageId: number30  username: string31}) {32  let transcript: string | null = null33  let languageCode: string | null = null34  let errorMsg: string | null = null35  try {36    const sourceFileArrayBuffer = await fetch(fileURL).then((res) => res.arrayBuffer())37    const sourceBlob = new Blob([sourceFileArrayBuffer], {38      type: fileType,39    })4041    const scribeResult = await elevenLabsClient.speechToText.convert({42      file: sourceBlob,43      model_id: 'scribe_v1',44      tag_audio_events: false,45    })4647    transcript = scribeResult.text48    languageCode = scribeResult.language_code4950    // Reply to the user with the transcript51    await bot.api.sendMessage(chatId, transcript, {52      reply_parameters: { message_id: messageId },53    })54  } catch (error) {55    errorMsg = error.message56    console.log(errorMsg)57    await bot.api.sendMessage(chatId, 'Sorry, there was an error. Please try again.', {58      reply_parameters: { message_id: messageId },59    })60  }61  // Write log to Supabase.62  const logLine = {63    file_type: fileType,64    duration,65    chat_id: chatId,66    message_id: messageId,67    username,68    language_code: languageCode,69    error: errorMsg,70  }71  console.log({ logLine })72  await supabase.from('transcription_logs').insert({ ...logLine, transcript })73}7475const telegramBotToken = Deno.env.get('TELEGRAM_BOT_TOKEN')76const bot = new Bot(telegramBotToken || '')77const startMessage = `Welcome to the ElevenLabs Scribe Bot\\! I can transcribe speech in 99 languages with super high accuracy\\!78    \nTry it out by sending or forwarding me a voice message, video, or audio file\\!79    \n[Learn more about Scribe](https://elevenlabs.io/speech-to-text) or [build your own bot](https://elevenlabs.io/docs/cookbooks/speech-to-text/telegram-bot)\\!80  `81bot.command('start', (ctx) => ctx.reply(startMessage.trim(), { parse_mode: 'MarkdownV2' }))8283bot.on([':voice', ':audio', ':video'], async (ctx) => {84  try {85    const file = await ctx.getFile()86    const fileURL = `https://api.telegram.org/file/bot${telegramBotToken}/${file.file_path}`87    const fileMeta = ctx.message?.video ?? ctx.message?.voice ?? ctx.message?.audio8889    if (!fileMeta) {90      return ctx.reply('No video|audio|voice metadata found. Please try again.')91    }9293    // Run the transcription in the background.94    EdgeRuntime.waitUntil(95      scribe({96        fileURL,97        fileType: fileMeta.mime_type!,98        duration: fileMeta.duration,99        chatId: ctx.chat.id,100        messageId: ctx.message?.message_id!,101        username: ctx.from?.username || '',102      })103    )104105    // Reply to the user immediately to let them know we received their file.106    return ctx.reply('Received. Scribing...')107  } catch (error) {108    console.error(error)109    return ctx.reply(110      'Sorry, there was an error getting the file. Please try again with a smaller file!'111    )112  }113})114115const handleUpdate = webhookCallback(bot, 'std/http')116117Deno.serve(async (req) => {118  try {119    const url = new URL(req.url)120    if (url.searchParams.get('secret') !== Deno.env.get('FUNCTION_SECRET')) {121      return new Response('not allowed', { status: 405 })122    }123124    return await handleUpdate(req)125  } catch (err) {126    console.error(err)127  }128})
```


### Deploy to Supabase#
If you haven't already, create a new Supabase account atdatabase.newand link the local project to your Supabase account:

```javascript
1supabase link
```


### Apply the database migrations#
Run the following command to apply the database migrations from thesupabase/migrationsdirectory:

```javascript
1supabase db push
```

Navigate to thetable editorin your Supabase dashboard and you should see and emptytranscription_logstable.



Lastly, run the following command to deploy the Edge Function:

```javascript
1supabase functions deploy --no-verify-jwt scribe-bot
```

Navigate to theEdge Functions viewin your Supabase dashboard and you should see thescribe-botfunction deployed. Make a note of the function URL as you'll need it later, it should look something likehttps://<project-ref>.functions.supabase.co/scribe-bot.




### Set up the webhook#
Set your bot's webhook URL tohttps://<PROJECT_REFERENCE>.functions.supabase.co/telegram-bot(Replacing<...>with respective values). In order to do that, run a GET request to the following URL (in your browser, for example):

```javascript
1https://api.telegram.org/bot<TELEGRAM_BOT_TOKEN>/setWebhook?url=https://<PROJECT_REFERENCE>.supabase.co/functions/v1/scribe-bot?secret=<FUNCTION_SECRET>
```

Note that theFUNCTION_SECRETis the secret you set in your.envfile.




### Set the function secrets#
Now that you have all your secrets set locally, you can run the following command to set the secrets in your Supabase project:

```javascript
1supabase secrets set --env-file supabase/functions/.env
```


### Test the bot#
Finally you can test the bot by sending it a voice message, audio or video file.



After you see the transcript as a reply, navigate back to your table editor in the Supabase dashboard and you should see a new row in yourtranscription_logstable.




================================================================================


# GitHub Actions
Source: https://supabase.com/docs/guides/functions/examples/github-actions

GitHub Actions

Use the Supabase CLI together with GitHub Actions to automatically deploy our Supabase Edge Functions.View on GitHub.

```javascript
1name: Deploy Function23on:4  push:5    branches:6      - main7  workflow_dispatch:89jobs:10  deploy:11    runs-on: ubuntu-latest1213    env:14      SUPABASE_ACCESS_TOKEN: YOUR_SUPABASE_ACCESS_TOKEN15      PROJECT_ID: YOUR_SUPABASE_PROJECT_ID1617    steps:18      - uses: actions/checkout@v41920      - uses: supabase/setup-cli@v121        with:22          version: latest2324      - run: supabase functions deploy --project-ref $PROJECT_ID
```

Since Supabase CLIv1.62.0you can deploy all functions with a single command.

Individual function configuration likeJWT verificationandimport map locationcan be set via theconfig.tomlfile.

```javascript
1[functions.hello-world]2verify_jwt = false
```


================================================================================


# Image Manipulation
Source: https://supabase.com/docs/guides/functions/examples/image-manipulation

Image Manipulation

Supabase Storage hasout-of-the-box supportfor the most common image transformations and optimizations you need.
If you need to do anything custom beyond what Supabase Storage provides, you can use Edge Functions to write custom image manipulation scripts.

In this example, we will usemagick-wasmto perform image manipulations.magick-wasmis the WebAssembly port of the popular ImageMagick library and supports processing over 100 file formats.

Edge Functions currently doesn't support image processing libraries such asSharp, which depend on native libraries. Only WASM-based libraries are supported.


### Prerequisites#
Make sure you have the latest version of theSupabase CLIinstalled.


### Create the Edge Function#
Create a new function locally:

```javascript
1supabase functions new image-blur
```


### Write the function#
In this example, we are implementing a function allowing users to upload an image and get a blurred thumbnail.

Here's the implementation inindex.tsfile:

```javascript
1// This is an example showing how to use Magick WASM to do image manipulations in Edge Functions.2//3import {4  ImageMagick,5  initializeImageMagick,6  MagickFormat,7} from "npm:@imagemagick/magick-wasm@0.0.30";89const wasmBytes = await Deno.readFile(10  new URL(11    "magick.wasm",12    import.meta.resolve("npm:@imagemagick/magick-wasm@0.0.30"),13  ),14);15await initializeImageMagick(16  wasmBytes,17);1819Deno.serve(async (req) => {20  const formData = await req.formData();21  const content = await formData.get("file").bytes();2223  let result = ImageMagick.read(24    content,25    (img): Uint8Array => {26      // resize the image27      img.resize(500, 300);28      // add a blur of 60x529      img.blur(60, 5);3031      return img.write(32        (data) => data,33      );34    },35  );3637  return new Response(38    result,39    { headers: { "Content-Type": "image/png" } },40  );41});
```


### Test it locally#
You can test the function locally by running:

```javascript
1supabase start2supabase functions serve --no-verify-jwt
```

Then, make a request usingcurlor your favorite API testing tool.

```javascript
1curl --location '<http://localhost:54321/functions/v1/image-blur>' \\2--form 'file=@"/path/to/image.png"'3--output '/path/to/output.png'
```

If you open theoutput.pngfile you will find a transformed version of your original image.


### Deploy to your hosted project#
Now, let's deploy the function to your Supabase project.

```javascript
1supabase link2supabase functions deploy image-blur
```

Hosted Edge Functions havelimitson memory and CPU usage.

If you try to perform complex image processing or handle large images (> 5MB) your function may return a resource limit exceeded error.


================================================================================


# Building an MCP Server with mcp-lite
Source: https://supabase.com/docs/guides/functions/examples/mcp-server-mcp-lite

Building an MCP Server with mcp-lite

TheModel Context Protocol(MCP) enables Large Language Models (LLMs) to interact with external tools and data sources. Withmcp-lite, you can build lightweight MCP servers that run on Supabase Edge Functions, giving your AI assistants the ability to execute custom tools at the edge.

This guide shows you how to scaffold, develop, and deploy an MCP server using mcp-lite on Supabase Edge Functions.


### What is mcp-lite?#
mcp-liteis a lightweight, zero-dependency TypeScript framework for building MCP servers. It works everywhere the Fetch API is available, including Node, Bun, Cloudflare Workers, Deno, and Supabase Edge Functions.


### Why Supabase Edge Functions + mcp-lite?#
This combination offers several advantages:

- Zero cold starts: Edge Functions stay warm for fast responses
- Global distribution: Deploy once and run everywhere
- Direct database access: Connect directly to your Supabase Postgres
- Minimal footprint: mcp-lite has zero runtime dependencies
- Full type safety: TypeScript support in Deno
- Simple deployment: One command to production


### Prerequisites#
You need:

- Docker(to run Supabase locally)
- Deno(Supabase Edge Functions runtime)
- Supabase CLI


### Create a new MCP server#
Starting withcreate-mcp-lite@0.3.0, you can scaffold a complete MCP server that runs on Supabase Edge Functions:

```javascript
1npm create mcp-lite@latest
```

When prompted, selectSupabase Edge Functions (MCP server)from the template options.

The template creates a focused structure for Edge Functions development:

```javascript
1my-mcp-server/2├── supabase/3│   ├── config.toml                    # Minimal Supabase config (Edge Functions only)4│   └── functions/5│       └── mcp-server/6│           ├── index.ts               # MCP server implementation7│           └── deno.json              # Deno imports and configuration8├── package.json9└── tsconfig.json
```


### Understanding the project structure#

### Minimal config.toml#
The template includes a minimalconfig.tomlthat runs only Edge Functions - no database, storage, or Studio UI. This keeps your local setup lightweight:

```javascript
1# Minimal config for running only Edge Functions (no DB, storage, or studio)2project_id = "starter-mcp-supabase"34[api]5enabled = true6port = 5432178[edge_runtime]9enabled = true10policy = "per_worker"11deno_version = 2
```

You can always add more services as needed.


### Two Hono apps pattern#
The template uses a specific pattern required by Supabase Edge Functions:

```javascript
1// Root handler - matches the function name2const app = new Hono()34// MCP protocol handler5const mcpApp = new Hono()67mcpApp.get('/', (c) => {8  return c.json({9    message: 'MCP Server on Supabase Edge Functions',10    endpoints: {11      mcp: '/mcp',12      health: '/health',13    },14  })15})1617mcpApp.all('/mcp', async (c) => {18  const response = await httpHandler(c.req.raw)19  return response20})2122// Mount at /mcp-server (the function name)23app.route('/mcp-server', mcpApp)
```

This is required because Supabase routes all requests to/<function-name>/*. The outerapphandles the function-level routing, whilemcpApphandles your actual MCP endpoints.


### Deno import maps#
The template uses Deno's import maps indeno.jsonto manage dependencies:

```javascript
1{2  "compilerOptions": {3    "lib": ["deno.window", "deno.ns"],4    "strict": true5  },6  "imports": {7    "hono": "npm:hono@^4.6.14",8    "mcp-lite": "npm:mcp-lite@0.8.2",9    "zod": "npm:zod@^4.1.12"10  }11}
```

This gives you npm package access while staying in the Deno ecosystem.


### Local development#

### Start Supabase#
Navigate to your project directory and start Supabase services:

```javascript
1supabase start
```


### Serve your function#
In a separate terminal, serve your MCP function locally:

```javascript
1supabase functions serve --no-verify-jwt mcp-server
```

Or use the npm script (which runs the same command):

```javascript
1npm run dev
```

Your MCP server is available at:

```javascript
1http://localhost:54321/functions/v1/mcp-server/mcp
```


### Testing your server#
Test the MCP server by adding it to your Claude Code, Claude Desktop, Cursor, or your preferred MCP client.

Using Claude Code:

```javascript
1claude mcp add my-mcp-server -t http http://localhost:54321/functions/v1/mcp-server/mcp
```

You can also test it using the MCP inspector:

```javascript
1npx @modelcontextprotocol/inspector
```

Then add the MCP endpoint URL in the inspector UI.


### How it works#
The MCP server setup is straightforward:

```javascript
1import { McpServer, StreamableHttpTransport } from 'mcp-lite'2import { z } from 'zod'34// Create MCP server instance5const mcp = new McpServer({6  name: 'starter-mcp-supabase-server',7  version: '1.0.0',8  schemaAdapter: (schema) => z.toJSONSchema(schema as z.ZodType),9})1011// Define a tool12mcp.tool('sum', {13  description: 'Adds two numbers together',14  inputSchema: z.object({15    a: z.number(),16    b: z.number(),17  }),18  handler: (args: { a: number; b: number }) => ({19    content: [{ type: 'text', text: String(args.a + args.b) }],20  }),21})2223// Bind to HTTP transport24const transport = new StreamableHttpTransport()25const httpHandler = transport.bind(mcp)
```


### Adding more tools#
Extend your MCP server by adding tools directly to themcpinstance. Here's an example of adding a database search tool:

```javascript
1mcp.tool('searchDatabase', {2  description: 'Search your Supabase database',3  inputSchema: z.object({4    table: z.string(),5    query: z.string(),6  }),7  handler: async (args) => {8    // Access Supabase client here9    // const { data } = await supabase.from(args.table).select('*')10    return {11      content: [{ type: 'text', text: `Searching ${args.table}...` }],12    }13  },14})
```

You can add tools that:

- Query your Supabase database
- Access Supabase Storage for file operations
- Call external APIs
- Process data with custom logic
- Integrate with other Supabase features


### Deploy to production#
When ready, deploy to Supabase's global edge network:

```javascript
1supabase functions deploy --no-verify-jwt mcp-server
```

Or use the npm script:

```javascript
1npm run deploy
```

Your MCP server will be live at:

```javascript
1https://your-project-ref.supabase.co/functions/v1/mcp-server/mcp
```


### Authentication considerations#
The template uses--no-verify-jwtfor quick development. This means authentication is not enforced by Supabase's JWT layer.

For production, you should implement authentication at the MCP server level following theMCP Authorization specification. This gives you control over who can access your MCP tools.


### Security best practices#
When deploying MCP servers:

- Don't expose sensitive data: Use the server in development environments with non-production data
- Implement authentication: Add proper authentication for production deployments
- Validate inputs: Always validate and sanitize tool inputs
- Limit tool scope: Only expose tools that are necessary for your use case
- Monitor usage: Track tool calls and monitor for unusual activity

For more security guidance, see theMCP security guide.


### What's next#
With your MCP server running on Supabase Edge Functions, you can:

- Connect it to your Supabase database for data-driven tools
- Use Supabase Auth to secure your endpoints
- Access Supabase Storage for file operations
- Deploy to multiple regions automatically
- Scale to handle production traffic
- Integrate with AI assistants like Claude, Cursor, or custom MCP clients


### Resources#
- mcp-lite on GitHub
- Model Context Protocol Spec
- Supabase Edge Functions Docs
- Deno Runtime Documentation
- Fiberplane tutorial


================================================================================


# Generating OG Images
Source: https://supabase.com/docs/guides/functions/examples/og-image

Generating OG Images

Generate Open Graph images with Deno and Supabase Edge Functions.View on GitHub.


### Code#
Create ahandler.tsxfile to construct the OG image in React:

```javascript
1import React from 'https://esm.sh/react@18.2.0'2import { ImageResponse } from 'https://deno.land/x/og_edge@0.0.4/mod.ts'34export default function handler(req: Request) {5  return new ImageResponse(6    (7      <div8        style={{9          width: '100%',10          height: '100%',11          display: 'flex',12          alignItems: 'center',13          justifyContent: 'center',14          fontSize: 128,15          background: 'lavender',16        }}17      >18        Hello OG Image!19      </div>20    )21  )22}
```

Create anindex.tsfile to execute the handler on incoming requests:

```javascript
1import handler from './handler.tsx'23console.log('Hello from og-image Function!')45Deno.serve(handler)
```


================================================================================


# Sending Push Notifications
Source: https://supabase.com/docs/guides/functions/examples/push-notifications

Sending Push Notifications

Push notifications are an important part of any mobile app. They allow you to send notifications to your users even when they are not using your app. This guide will show you how to send push notifications to different mobile app frameworks from your Supabase edge functions.

Expomakes implementing push notifications easy. All the hassle with device information and communicating with Firebase Cloud Messaging (FCM) or Apple Push Notification Service (APNs) is done behind the scenes. This allows you to treat Android and iOS notifications in the same way and save time both on the frontend and backend.

Find the example code onGitHub.


### Supabase setup#
- Create a new Supabase project.
- Link your project:supabase link --project-ref your-supabase-project-ref
- Start Supabase locally:supabase start
- Push up the schema:supabase db push(schema is defined insupabase/migrations)


### Expo setup#
To utilize Expo's push notification service, you must configure your app by installing a set of libraries, implementing functions to handle notifications, and setting up credentials for Android and iOS. Follow the officialExpo Push Notifications Setup Guideto get the credentials for Android and iOS. This project usesExpo's EAS buildservice to simplify this part.


### Enhanced security for push notifications#

### Deploy the Supabase Edge Function#
The database webhook handler to send push notifications is located insupabase/functions/push/index.ts. Deploy the function to your linked project and set theEXPO_ACCESS_TOKENsecret.

```javascript
1import { createClient } from 'npm:@supabase/supabase-js@2'23console.log('Hello from Functions!')45interface Notification {6  id: string7  user_id: string8  body: string9}1011interface WebhookPayload {12  type: 'INSERT' | 'UPDATE' | 'DELETE'13  table: string14  record: Notification15  schema: 'public'16  old_record: null | Notification17}1819const supabase = createClient(20  Deno.env.get('SUPABASE_URL')!,21  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!22)2324Deno.serve(async (req) => {25  const payload: WebhookPayload = await req.json()26  const { data } = await supabase27    .from('profiles')28    .select('expo_push_token')29    .eq('id', payload.record.user_id)30    .single()3132  const res = await fetch('https://exp.host/--/api/v2/push/send', {33    method: 'POST',34    headers: {35      'Content-Type': 'application/json',36      Authorization: `Bearer ${Deno.env.get('EXPO_ACCESS_TOKEN')}`,37    },38    body: JSON.stringify({39      to: data?.expo_push_token,40      sound: 'default',41      body: payload.record.body,42    }),43  }).then((res) => res.json())4445  return new Response(JSON.stringify(res), {46    headers: { 'Content-Type': 'application/json' },47  })48})
```


### Create the database webhook#
Navigate to theDatabase Webhooks settingsin your Supabase Dashboard.


### Send push notification#

================================================================================


# Rate Limiting Edge Functions
Source: https://supabase.com/docs/guides/functions/examples/rate-limiting

Rate Limiting Edge Functions

Redisis an open source (BSD licensed), in-memory data structure store used as a database, cache, message broker, and streaming engine. It is optimized for atomic operations like incrementing a value, for example for a view counter or rate limiting. We can even rate limit based on the user ID from Supabase Auth!

Upstashprovides an HTTP/REST based Redis client which is ideal for serverless use-cases and therefore works well with Supabase Edge Functions.

Find the code onGitHub.


================================================================================


# Taking Screenshots with Puppeteer
Source: https://supabase.com/docs/guides/functions/examples/screenshots

Taking Screenshots with Puppeteer

Puppeteeris a handy tool to programmatically take screenshots and generate PDFs. However, trying to do so in Edge Functions can be challenging due to the size restrictions. Luckily there is aserverless browser offering availablethat we can connect to via WebSockets.

Find the code onGitHub.


================================================================================


# Semantic Search
Source: https://supabase.com/docs/guides/functions/examples/semantic-search

Semantic Search


### Semantic Search with pgvector and Supabase Edge Functions
Semantic Search with pgvector and Supabase Edge Functions

Semantic searchinterprets the meaning behind user queries rather than exactkeywords. It uses machine learning to capture the intent and context behind the query, handling language nuances like synonyms, phrasing variations, and word relationships.

Since Supabase Edge Runtimev1.36.0you can run thegte-smallmodelnatively within Supabase Edge Functions without any external dependencies! This allows you to generate text embeddings without calling any external APIs!

In this tutorial you're implementing three parts:

You can find the complete example code onGitHub


### Create the database table and webhook#
Given thefollowing table definition:

```javascript
1create extension if not exists vector with schema extensions;23create table embeddings (4  id bigint primary key generated always as identity,5  content text not null,6  embedding extensions.vector (384)7);8alter table embeddings enable row level security;910create index on embeddings using hnsw (embedding vector_ip_ops);
```

You can deploy thefollowing edge functionas adatabase webhookto generate the embeddings for any text content inserted into the table:

```javascript
1const model = new Supabase.ai.Session('gte-small')23Deno.serve(async (req) => {4  const payload: WebhookPayload = await req.json()5  const { content, id } = payload.record67  // Generate embedding.8  const embedding = await model.run(content, {9    mean_pool: true,10    normalize: true,11  })1213  // Store in database.14  const { error } = await supabase15    .from('embeddings')16    .update({ embedding: JSON.stringify(embedding) })17    .eq('id', id)18  if (error) console.warn(error.message)1920  return new Response('ok')21})
```


### Create a Database Function and RPC#
With the embeddings now stored in your Postgres database table, you can query them from Supabase Edge Functions by utilizingRemote Procedure Calls (RPC).

Given thefollowing Postgres Function:

```javascript
1-- Matches document sections using vector similarity search on embeddings2--3-- Returns a setof embeddings so that we can use PostgREST resource embeddings (joins with other tables)4-- Additional filtering like limits can be chained to this function call5create or replace function query_embeddings(embedding extensions.vector(384), match_threshold float)6returns setof embeddings7language plpgsql8as $$9begin10  return query11  select *12  from embeddings1314  -- The inner product is negative, so we negate match_threshold15  where embeddings.embedding <#> embedding < -match_threshold1617  -- Our embeddings are normalized to length 1, so cosine similarity18  -- and inner product will produce the same query results.19  -- Using inner product which can be computed faster.20  --21  -- For the different distance functions, see https://github.com/pgvector/pgvector22  order by embeddings.embedding <#> embedding;23end;24$$;
```


### Query vectors in Supabase Edge Functions#
You can usesupabase-jsto first generate the embedding for the search term and then invoke the Postgres function to find the relevant results from your stored embeddings, right from yourSupabase Edge Function:

```javascript
1const model = new Supabase.ai.Session('gte-small')23Deno.serve(async (req) => {4  const { search } = await req.json()5  if (!search) return new Response('Please provide a search param!')6  // Generate embedding for search term.7  const embedding = await model.run(search, {8    mean_pool: true,9    normalize: true,10  })1112  // Query embeddings.13  const { data: result, error } = await supabase14    .rpc('query_embeddings', {15      embedding,16      match_threshold: 0.8,17    })18    .select('content')19    .limit(3)20  if (error) {21    return Response.json(error)22  }2324  return Response.json({ search, result })25})
```

You now have AI powered semantic search set up without any external dependencies! Just you, pgvector, and Supabase Edge Functions!


================================================================================


# Sending Emails
Source: https://supabase.com/docs/guides/functions/examples/send-emails

Sending Emails

Sending emails from Edge Functions using theResend API.


### Prerequisites#
To get the most out of this guide, you’ll need to:

- Create an API key
- Verify your domain

Make sure you have the latest version of theSupabase CLIinstalled.


### 1. Create Supabase function#
Create a new function locally:

```javascript
1supabase functions new resend
```

Store theRESEND_API_KEYin your.envfile.


### 2. Edit the handler function#
Paste the following code into theindex.tsfile:

```javascript
1const RESEND_API_KEY = Deno.env.get('RESEND_API_KEY')23const handler = async (_request: Request): Promise<Response> => {4  const res = await fetch('https://api.resend.com/emails', {5    method: 'POST',6    headers: {7      'Content-Type': 'application/json',8      Authorization: `Bearer ${RESEND_API_KEY}`,9    },10    body: JSON.stringify({11      from: 'onboarding@resend.dev',12      to: 'delivered@resend.dev',13      subject: 'hello world',14      html: '<strong>it works!</strong>',15    }),16  })1718  const data = await res.json()1920  return new Response(JSON.stringify(data), {21    status: 200,22    headers: {23      'Content-Type': 'application/json',24    },25  })26}2728Deno.serve(handler)
```


### 3. Deploy and send email#
Run function locally:

```javascript
1supabase start2supabase functions serve --no-verify-jwt --env-file .env
```

Test it:http://localhost:54321/functions/v1/resend

Deploy function to Supabase:

```javascript
1supabase functions deploy resend --no-verify-jwt
```

When you deploy to Supabase, make sure that yourRESEND_API_KEYis set inEdge Function Secrets Management

Open the endpoint URL to send an email:


### 4. Try it yourself#
Find the complete example onGitHub.


================================================================================


# Monitoring with Sentry
Source: https://supabase.com/docs/guides/functions/examples/sentry-monitoring

Monitoring with Sentry

Add theSentry Deno SDKto your Supabase Edge Functions to track exceptions and get notified of errors or performance issues.


### Prerequisites#
- Create a Sentry account.
- Make sure you have the latest version of theSupabase CLIinstalled.


### 1. Create Supabase function#
Create a new function locally:

```javascript
1supabase functions new sentryfied
```


### 2. Add the Sentry Deno SDK#
Handle exceptions within your function and send them to Sentry.

```javascript
1import * as Sentry from 'https://deno.land/x/sentry/index.mjs'23Sentry.init({4  // https://docs.sentry.io/product/sentry-basics/concepts/dsn-explainer/#where-to-find-your-dsn5  dsn: SENTRY_DSN,6  defaultIntegrations: false,7  // Performance Monitoring8  tracesSampleRate: 1.0,9  // Set sampling rate for profiling - this is relative to tracesSampleRate10  profilesSampleRate: 1.0,11})1213// Set region and execution_id as custom tags14Sentry.setTag('region', Deno.env.get('SB_REGION'))15Sentry.setTag('execution_id', Deno.env.get('SB_EXECUTION_ID'))1617Deno.serve(async (req) => {18  try {19    const { name } = await req.json()20    // This will throw, as `name` in our example call will be `undefined`21    const data = {22      message: `Hello ${name}!`,23    }2425    return new Response(JSON.stringify(data), { headers: { 'Content-Type': 'application/json' } })26  } catch (e) {27    Sentry.captureException(e)28    // Flush Sentry before the running process closes29    await Sentry.flush(2000)30    return new Response(JSON.stringify({ msg: 'error' }), {31      status: 500,32      headers: { 'Content-Type': 'application/json' },33    })34  }35})
```


### 3. Deploy and test#
Run function locally:

```javascript
1supabase start2supabase functions serve --no-verify-jwt
```

Test it:http://localhost:54321/functions/v1/sentryfied

Deploy function to Supabase:

```javascript
1supabase functions deploy sentryfied --no-verify-jwt
```


### 4. Try it yourself#
Find the complete example onGitHub.


### Working with scopes#
Sentry Deno SDK currently do not supportDeno.serveinstrumentation, which means that there is no scope separation between requests. Because of that, when the Edge Functions runtime is reused between multiple requests, all globally captured breadcrumbs and contextual data will be shared, which is not the desired behavior. To work around this, all default integrations in the example code above are disabled, and you should be relying onwithScopeto encapsulate all Sentry SDK API calls, orpass context directlyto thecaptureExceptionorcaptureMessagecalls.


================================================================================


# Slack Bot Mention Edge Function
Source: https://supabase.com/docs/guides/functions/examples/slack-bot-mention

Slack Bot Mention Edge Function

The Slack Bot Mention Edge Function allows you to process mentions in Slack and respond accordingly.


### Configuring Slack apps#
For your bot to seamlessly interact with Slack, you'll need to configure Slack Apps:


### Creating the Edge Function#
Deploy the following code as an Edge function using the CLI:

```javascript
1supabase --project-ref nacho_slacker secrets \2set SLACK_TOKEN=<TEST-TOKEN-HERE>
```

Here's the code of the Edge Function, you can change the response to handle the text received:

```javascript
1import { WebClient } from 'https://deno.land/x/slack_web_api@6.7.2/mod.js'23const slackBotToken = Deno.env.get('SLACK_TOKEN') ?? ''4const botClient = new WebClient(slackBotToken)56console.log(`Slack URL verification function up and running!`)7Deno.serve(async (req) => {8  try {9    const reqBody = await req.json()10    console.log(JSON.stringify(reqBody, null, 2))11    const { token, challenge, type, event } = reqBody1213    if (type == 'url_verification') {14      return new Response(JSON.stringify({ challenge }), {15        headers: { 'Content-Type': 'application/json' },16        status: 200,17      })18    } else if (event.type == 'app_mention') {19      const { user, text, channel, ts } = event20      // Here you should process the text received and return a response:21      const response = await botClient.chat.postMessage({22        channel: channel,23        text: `Hello <@${user}>!`,24        thread_ts: ts,25      })26      return new Response('ok', { status: 200 })27    }28  } catch (error) {29    return new Response(JSON.stringify({ error: error.message }), {30      headers: { 'Content-Type': 'application/json' },31      status: 500,32    })33  }34})
```


================================================================================


# Handling Stripe Webhooks
Source: https://supabase.com/docs/guides/functions/examples/stripe-webhooks

Handling Stripe Webhooks

Handling signed Stripe Webhooks with Edge Functions.View on GitHub.

```javascript
1// Follow this setup guide to integrate the Deno language server with your editor:2// https://deno.land/manual/getting_started/setup_your_environment3// This enables autocomplete, go to definition, etc.45// Import via bare specifier thanks to the import_map.json file.6import Stripe from 'https://esm.sh/stripe@14?target=denonext'78const stripe = new Stripe(Deno.env.get('STRIPE_API_KEY') as string, {9  // This is needed to use the Fetch API rather than relying on the Node http10  // package.11  apiVersion: '2024-11-20'12})13// This is needed in order to use the Web Crypto API in Deno.14const cryptoProvider = Stripe.createSubtleCryptoProvider()1516console.log('Hello from Stripe Webhook!')1718Deno.serve(async (request) => {19  const signature = request.headers.get('Stripe-Signature')2021  // First step is to verify the event. The .text() method must be used as the22  // verification relies on the raw request body rather than the parsed JSON.23  const body = await request.text()24  let receivedEvent25  try {26    receivedEvent = await stripe.webhooks.constructEventAsync(27      body,28      signature!,29      Deno.env.get('STRIPE_WEBHOOK_SIGNING_SECRET')!,30      undefined,31      cryptoProvider32    )33  } catch (err) {34    return new Response(err.message, { status: 400 })35  }36  console.log(`🔔 Event received: ${receivedEvent.id}`)37  return new Response(JSON.stringify({ ok: true }), { status: 200 })38});
```


================================================================================


# Building a Telegram Bot
Source: https://supabase.com/docs/guides/functions/examples/telegram-bot

Building a Telegram Bot

Handle Telegram Bot Webhooks with thegrammY framework. grammY is an open source Telegram Bot Framework which makes it easy to handle and respond to incoming messages.View on GitHub.


================================================================================


# Upstash Redis
Source: https://supabase.com/docs/guides/functions/examples/upstash-redis

Upstash Redis

A Redis counter example that stores ahashof function invocation count per region. Find the code onGitHub.


### Redis database setup#
Create a Redis database using theUpstash ConsoleorUpstash CLI.

Select theGlobaltype to minimize the latency from all edge locations. Copy theUPSTASH_REDIS_REST_URLandUPSTASH_REDIS_REST_TOKENto your .env file.

You'll find them underDetails > REST API > .env.

```javascript
1cp supabase/functions/upstash-redis-counter/.env.example supabase/functions/upstash-redis-counter/.env
```


### Code#
Make sure you have the latest version of theSupabase CLI installed.

Create a new function in your project:

```javascript
1supabase functions new upstash-redis-counter
```

And add the code to theindex.tsfile:

```javascript
1import { Redis } from 'https://deno.land/x/upstash_redis@v1.19.3/mod.ts'23console.log(`Function "upstash-redis-counter" up and running!`)45Deno.serve(async (_req) => {6  try {7    const redis = new Redis({8      url: Deno.env.get('UPSTASH_REDIS_REST_URL')!,9      token: Deno.env.get('UPSTASH_REDIS_REST_TOKEN')!,10    })1112    const deno_region = Deno.env.get('DENO_REGION')13    if (deno_region) {14      // Increment region counter15      await redis.hincrby('supa-edge-counter', deno_region, 1)16    } else {17      // Increment localhost counter18      await redis.hincrby('supa-edge-counter', 'localhost', 1)19    }2021    // Get all values22    const counterHash: Record<string, number> | null = await redis.hgetall('supa-edge-counter')23    const counters = Object.entries(counterHash!)24      .sort(([, a], [, b]) => b - a) // sort desc25      .reduce((r, [k, v]) => ({ total: r.total + v, regions: { ...r.regions, [k]: v } }), {26        total: 0,27        regions: {},28      })2930    return new Response(JSON.stringify({ counters }), { status: 200 })31  } catch (error) {32    return new Response(JSON.stringify({ error: error.message }), { status: 200 })33  }34})
```


### Run locally#
```javascript
1supabase start2supabase functions serve --no-verify-jwt --env-file supabase/functions/upstash-redis-counter/.env
```

Navigate tohttp://localhost:54321/functions/v1/upstash-redis-counter.


### Deploy#
```javascript
1supabase functions deploy upstash-redis-counter --no-verify-jwt2supabase secrets set --env-file supabase/functions/upstash-redis-counter/.env
```


================================================================================


# Function Configuration
Source: https://supabase.com/docs/guides/functions/function-configuration

Function Configuration


### Configure individual function behavior. Customize authentication, dependencies, and other settings per function.
Configure individual function behavior. Customize authentication, dependencies, and other settings per function.


### Configuration#
By default, all your Edge Functions have the same settings. In real applications, however, you might need different behaviors between functions.

For example:

- Stripe webhooksneed to be publicly accessible (Stripe doesn't have your user tokens)
- User profile APIsshould require authentication
- Some functionsmight need special dependencies or different file types

To enable these per-function rules, createsupabase/config.tomlin your project root:

```javascript
1# Disables authentication for the Stripe webhook.2[functions.stripe-webhook]3verify_jwt = false45# Custom dependencies for this specific function6[functions.image-processor]7import_map = './functions/image-processor/import_map.json'89# Custom entrypoint for legacy function using JavaScript10[functions.legacy-processor]11entrypoint = './functions/legacy-processor/index.js
```

This configuration tell Supabase that thestripe-webhookfunction doesn't require a valid JWT, theimage-processorfunction uses a custom import map, andlegacy-processoruses a custom entrypoint.

You set these rules once and never worry about them again. Deploy your functions knowing that the security and behavior is exactly what each endpoint needs.

To see more generalconfig.tomloptions, check outthis guide.


### Skipping authorization checks#
By default, Edge Functions require a valid JWT in the authorization header. If you want to use Edge Functions without Authorization checks (commonly used for Stripe webhooks), you can configure this in yourconfig.toml:

```javascript
1[functions.stripe-webhook]2verify_jwt = false
```

You can also pass the--no-verify-jwtflag when serving your Edge Functions locally:

```javascript
1supabase functions serve hello-world --no-verify-jwt
```

Be careful when using this flag, as it will allow anyone to invoke your Edge Function without a valid JWT. The Supabase client libraries automatically handle authorization.


### Custom entrypoints#
entrypointis available only in Supabase CLI version 1.215.0 or higher.

When you create a new Edge Function, it will use TypeScript by default. However, it is possible to write and deploy Edge Functions using pure JavaScript.

Save your Function as a JavaScript file (e.g.index.js) update thesupabase/config.toml:

```javascript
1[functions.hello-world]2entrypoint = './index.js' # path must be relative to config.toml
```

You can use any.ts,.js,.tsx,.jsxor.mjsfile as the entrypoint for a Function.


================================================================================


# Routing
Source: https://supabase.com/docs/guides/functions/http-methods

Routing


### Handle different request types in a single function to create efficient APIs.
Handle different request types in a single function to create efficient APIs.


### Overview#
Edge Functions supportGET,POST,PUT,PATCH,DELETE, andOPTIONS. This means you can build complete REST APIs in a single function:

```javascript
1Deno.serve(async (req) => {2  const { method, url } = req3  const { pathname } = new URL(url)45  // Route based on method and path6  if (method === 'GET' && pathname === '/users') {7    return getAllUsers()8  } else if (method === 'POST' && pathname === '/users') {9    return createUser(req)10  }1112  return new Response('Not found', { status: 404 })13})
```

Edge Functions allow you to build APIs without needing separate functions for each endpoint. This reduces cold starts and simplifies deployment while keeping your code organized.

HTML content is not supported.GETrequests that returntext/htmlwill be rewritten totext/plain. Edge Functions are designed for APIs and data processing, not serving web pages. Use Supabase for your backend API and your favorite frontend framework for HTML.


### Example#
Here's a full example of a RESTful API built with Edge Functions.

```javascript
1// Follow this setup guide to integrate the Deno language server with your editor:2// https://deno.land/manual/getting_started/setup_your_environment3// This enables autocomplete, go to definition, etc.45import { createClient, SupabaseClient } from 'npm:supabase-js@2'6// New approach (v2.95.0+)7import { corsHeaders } from 'jsr:@supabase/supabase-js@2/cors'8// For older versions, use hardcoded headers:9// const corsHeaders = {10//   'Access-Control-Allow-Origin': '*',11//   'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',12//   'Access-Control-Allow-Methods': 'POST, GET, OPTIONS, PUT, DELETE',13// }1415interface Task {16  name: string17  status: number18}1920async function getTask(supabaseClient: SupabaseClient, id: string) {21  const { data: task, error } = await supabaseClient.from('tasks').select('*').eq('id', id)22  if (error) throw error2324  return new Response(JSON.stringify({ task }), {25    headers: { ...corsHeaders, 'Content-Type': 'application/json' },26    status: 200,27  })28}2930async function getAllTasks(supabaseClient: SupabaseClient) {31  const { data: tasks, error } = await supabaseClient.from('tasks').select('*')32  if (error) throw error3334  return new Response(JSON.stringify({ tasks }), {35    headers: { ...corsHeaders, 'Content-Type': 'application/json' },36    status: 200,37  })38}3940async function deleteTask(supabaseClient: SupabaseClient, id: string) {41  const { error } = await supabaseClient.from('tasks').delete().eq('id', id)42  if (error) throw error4344  return new Response(JSON.stringify({}), {45    headers: { ...corsHeaders, 'Content-Type': 'application/json' },46    status: 200,47  })48}4950async function updateTask(supabaseClient: SupabaseClient, id: string, task: Task) {51  const { error } = await supabaseClient.from('tasks').update(task).eq('id', id)52  if (error) throw error5354  return new Response(JSON.stringify({ task }), {55    headers: { ...corsHeaders, 'Content-Type': 'application/json' },56    status: 200,57  })58}5960async function createTask(supabaseClient: SupabaseClient, task: Task) {61  const { error } = await supabaseClient.from('tasks').insert(task)62  if (error) throw error6364  return new Response(JSON.stringify({ task }), {65    headers: { ...corsHeaders, 'Content-Type': 'application/json' },66    status: 200,67  })68}6970Deno.serve(async (req) => {71  const { url, method } = req7273  // This is needed if you're planning to invoke your function from a browser.74  if (method === 'OPTIONS') {75    return new Response('ok', { headers: corsHeaders })76  }7778  try {79    // Create a Supabase client with the Auth context of the logged in user.80    const supabaseClient = createClient(81      // Supabase API URL - env var exported by default.82      Deno.env.get('SUPABASE_URL') ?? '',83      // Supabase API ANON KEY - env var exported by default.84      Deno.env.get('SUPABASE_ANON_KEY') ?? '',85      // Create client with Auth context of the user that called the function.86      // This way your row-level-security (RLS) policies are applied.87      {88        global: {89          headers: { Authorization: req.headers.get('Authorization')! },90        },91      }92    )9394    // For more details on URLPattern, check https://developer.mozilla.org/en-US/docs/Web/API/URL_Pattern_API95    const taskPattern = new URLPattern({ pathname: '/restful-tasks/:id' })96    const matchingPath = taskPattern.exec(url)97    const id = matchingPath ? matchingPath.pathname.groups.id : null9899    let task = null100    if (method === 'POST' || method === 'PUT') {101      const body = await req.json()102      task = body.task103    }104105    // call relevant method based on method and id106    switch (true) {107      case id && method === 'GET':108        return getTask(supabaseClient, id as string)109      case id && method === 'PUT':110        return updateTask(supabaseClient, id as string, task)111      case id && method === 'DELETE':112        return deleteTask(supabaseClient, id as string)113      case method === 'POST':114        return createTask(supabaseClient, task)115      case method === 'GET':116        return getAllTasks(supabaseClient)117      default:118        return getAllTasks(supabaseClient)119    }120  } catch (error) {121    console.error(error)122123    return new Response(JSON.stringify({ error: error.message }), {124      headers: { ...corsHeaders, 'Content-Type': 'application/json' },125      status: 400,126    })127  }128})
```


================================================================================


# Type-Safe SQL with Kysely
Source: https://supabase.com/docs/guides/functions/kysely-postgres

Type-Safe SQL with Kysely

Supabase Edge Functions canconnect directly to your Postgres databaseto execute SQL queries.Kyselyis a type-safe and autocompletion-friendly typescript SQL query builder.

Combining Kysely with Deno Postgres gives you a convenient developer experience for interacting directly with your Postgres database.


### Code#
Find the example onGitHub

Get your database connection credentials from the project'sConnectpaneland store them in an.envfile:

```javascript
1DB_HOSTNAME=2DB_PASSWORD=3DB_SSL_CERT="-----BEGIN CERTIFICATE-----4GET YOUR CERT FROM YOUR PROJECT DASHBOARD5-----END CERTIFICATE-----"
```

Create aDenoPostgresDriver.tsfile to manage the connection to Postgres viadeno-postgres:

```javascript
1import {2  CompiledQuery,3  DatabaseConnection,4  Driver,5  PostgresCursorConstructor,6  QueryResult,7  TransactionSettings,8} from 'https://esm.sh/kysely@0.23.4'9import { freeze, isFunction } from 'https://esm.sh/kysely@0.23.4/dist/esm/util/object-utils.js'10import { extendStackTrace } from 'https://esm.sh/kysely@0.23.4/dist/esm/util/stack-trace-utils.js'11import { Pool, PoolClient } from 'https://deno.land/x/postgres@v0.17.0/mod.ts'1213export interface PostgresDialectConfig {14  pool: Pool | (() => Promise<Pool>)15  cursor?: PostgresCursorConstructor16  onCreateConnection?: (connection: DatabaseConnection) => Promise<void>17}1819const PRIVATE_RELEASE_METHOD = Symbol()2021export class PostgresDriver implements Driver {22  readonly #config: PostgresDialectConfig23  readonly #connections = new WeakMap<PoolClient, DatabaseConnection>()24  #pool?: Pool2526  constructor(config: PostgresDialectConfig) {27    this.#config = freeze({ ...config })28  }2930  async init(): Promise<void> {31    this.#pool = isFunction(this.#config.pool) ? await this.#config.pool() : this.#config.pool32  }3334  async acquireConnection(): Promise<DatabaseConnection> {35    const client = await this.#pool!.connect()36    let connection = this.#connections.get(client)3738    if (!connection) {39      connection = new PostgresConnection(client, {40        cursor: this.#config.cursor ?? null,41      })42      this.#connections.set(client, connection)4344      // The driver must take care of calling `onCreateConnection` when a new45      // connection is created. The `pg` module doesn't provide an async hook46      // for the connection creation. We need to call the method explicitly.47      if (this.#config?.onCreateConnection) {48        await this.#config.onCreateConnection(connection)49      }50    }5152    return connection53  }5455  async beginTransaction(56    connection: DatabaseConnection,57    settings: TransactionSettings58  ): Promise<void> {59    if (settings.isolationLevel) {60      await connection.executeQuery(61        CompiledQuery.raw(`start transaction isolation level ${settings.isolationLevel}`)62      )63    } else {64      await connection.executeQuery(CompiledQuery.raw('begin'))65    }66  }6768  async commitTransaction(connection: DatabaseConnection): Promise<void> {69    await connection.executeQuery(CompiledQuery.raw('commit'))70  }7172  async rollbackTransaction(connection: DatabaseConnection): Promise<void> {73    await connection.executeQuery(CompiledQuery.raw('rollback'))74  }7576  async releaseConnection(connection: PostgresConnection): Promise<void> {77    connection[PRIVATE_RELEASE_METHOD]()78  }7980  async destroy(): Promise<void> {81    if (this.#pool) {82      const pool = this.#pool83      this.#pool = undefined84      await pool.end()85    }86  }87}8889interface PostgresConnectionOptions {90  cursor: PostgresCursorConstructor | null91}9293class PostgresConnection implements DatabaseConnection {94  #client: PoolClient95  #options: PostgresConnectionOptions9697  constructor(client: PoolClient, options: PostgresConnectionOptions) {98    this.#client = client99    this.#options = options100  }101102  async executeQuery<O>(compiledQuery: CompiledQuery): Promise<QueryResult<O>> {103    try {104      const result = await this.#client.queryObject<O>(compiledQuery.sql, [105        ...compiledQuery.parameters,106      ])107108      if (109        result.command === 'INSERT' ||110        result.command === 'UPDATE' ||111        result.command === 'DELETE'112      ) {113        const numAffectedRows = BigInt(result.rowCount || 0)114115        return {116          numUpdatedOrDeletedRows: numAffectedRows,117          numAffectedRows,118          rows: result.rows ?? [],119        } as any120      }121122      return {123        rows: result.rows ?? [],124      }125    } catch (err) {126      throw extendStackTrace(err, new Error())127    }128  }129130  async *streamQuery<O>(131    _compiledQuery: CompiledQuery,132    chunkSize: number133  ): AsyncIterableIterator<QueryResult<O>> {134    if (!this.#options.cursor) {135      throw new Error(136        "'cursor' is not present in your postgres dialect config. It's required to make streaming work in postgres."137      )138    }139140    if (!Number.isInteger(chunkSize) || chunkSize <= 0) {141      throw new Error('chunkSize must be a positive integer')142    }143144    // stream not available145    return null146  }147148  [PRIVATE_RELEASE_METHOD](): void {149    this.#client.release()150  }151}
```

Create anindex.tsfile to execute a query on incoming requests:

```javascript
1import { serve } from 'https://deno.land/std@0.175.0/http/server.ts'2import { Pool } from 'https://deno.land/x/postgres@v0.17.0/mod.ts'3import {4  Kysely,5  Generated,6  PostgresAdapter,7  PostgresIntrospector,8  PostgresQueryCompiler,9} from 'https://esm.sh/kysely@0.23.4'10import { PostgresDriver } from './DenoPostgresDriver.ts'1112console.log(`Function "kysely-postgres" up and running!`)1314interface AnimalTable {15  id: Generated<bigint>16  animal: string17  created_at: Date18}1920// Keys of this interface are table names.21interface Database {22  animals: AnimalTable23}2425// Create a database pool with one connection.26const pool = new Pool(27  {28    tls: { caCertificates: [Deno.env.get('DB_SSL_CERT')!] },29    database: 'postgres',30    hostname: Deno.env.get('DB_HOSTNAME'),31    user: 'postgres',32    port: 5432,33    password: Deno.env.get('DB_PASSWORD'),34  },35  136)3738// You'd create one of these when you start your app.39const db = new Kysely<Database>({40  dialect: {41    createAdapter() {42      return new PostgresAdapter()43    },44    createDriver() {45      return new PostgresDriver({ pool })46    },47    createIntrospector(db: Kysely<unknown>) {48      return new PostgresIntrospector(db)49    },50    createQueryCompiler() {51      return new PostgresQueryCompiler()52    },53  },54})5556serve(async (_req) => {57  try {58    // Run a query59    const animals = await db.selectFrom('animals').select(['id', 'animal', 'created_at']).execute()6061    // Neat, it's properly typed \o/62    console.log(animals[0].created_at.getFullYear())6364    // Encode the result as pretty printed JSON65    const body = JSON.stringify(66      animals,67      (key, value) => (typeof value === 'bigint' ? value.toString() : value),68      269    )7071    // Return the response with the correct content type header72    return new Response(body, {73      status: 200,74      headers: {75        'Content-Type': 'application/json; charset=utf-8',76      },77    })78  } catch (err) {79    console.error(err)80    return new Response(String(err?.message ?? err), { status: 500 })81  }82})
```


================================================================================


# Limits
Source: https://supabase.com/docs/guides/functions/limits

Limits


### Limits applied Edge Functions in Supabase's hosted platform.
Limits applied Edge Functions in Supabase's hosted platform.


### Runtime limits#
- Maximum Memory: 256MB
- Maximum Duration (Wall clock limit):
This is the duration an Edge Function worker will stay active. During this period, a worker can serve multiple requests or process background tasks.Free plan: 150sPaid plans: 400s
- Maximum CPU Time: 2s (Amount of actual time spent on the CPU per request - does not include async I/O.)
- Request idle timeout: 150s (If an Edge Function doesn't send a response before the timeout, 504 Gateway Timeout will be returned)

- Free plan: 150s
- Paid plans: 400s


### Platform limits#
- Maximum Function Size: 20MB (After bundling using CLI)
- Maximum no. of Functions per project:Free: 100Pro: 500Team: 1000Enterprise: Unlimited
- Maximum log message length: 10,000 characters
- Log event threshold: 100 events per 10 seconds

- Free: 100
- Pro: 500
- Team: 1000
- Enterprise: Unlimited


### Secrets#
- Maximum number of secrets per project:100
- Secret name length: up to256characters
- Maximum secret size:48 KiB(24,576characters)
- Names must NOT start with the prefixSUPABASE_(this prefix is reserved).


### Other limits & restrictions#
- Outgoing connections to ports25and587are not allowed.
- Serving of HTML content is only supported withcustom domains(OtherwiseGETrequests that returntext/htmlwill be rewritten totext/plain).
- Web Worker API (or NodevmAPI) are not available.
- Static files cannot be deployed using the API flag. You need to build them withDocker on the CLI.
- Node Libraries that require multithreading are not supported. Examples:libvips,sharp.


================================================================================


# Logging
Source: https://supabase.com/docs/guides/functions/logging

Logging


### Monitor your Edge Functions with logging to track execution, debug issues, and optimize performance.
Monitor your Edge Functions with logging to track execution, debug issues, and optimize performance.

Logs are provided for each function invocation, locally and in hosted environments.


### Accessing logs#

### Production#
Access logs from the Functions section of your Dashboard:

- Invocations:Request/Response data including headers, body, status codes, and execution duration. Filter by date, time, or status code.
- Logs:Platform events, uncaught exceptions, and custom log messages. Filter by timestamp, level, or message content.




### Development#
Whendeveloping locallyyou will see error messages and console log statements printed to your local terminal window.


### Log event types#

### Automatic logs#
Your functions automatically capture several types of events:

- Uncaught exceptions: Uncaught exceptions thrown by a function during execution are automatically logged. You can see the error message and stack trace in the Logs tool.
- Custom log events: You can useconsole.log,console.error, andconsole.warnin your code to emit custom log events. These events also appear in the Logs tool.
- Boot and Shutdown Logs: The Logs tool extends its coverage to include logs for the boot and shutdown of functions.


### Custom logs#
You can add your own log messages using standard console methods:

```javascript
1Deno.serve(async (req) => {2  try {3    const { name } = await req.json()45    if (!name) {6      // Log a warning message7      console.warn('Empty name parameter received')8    }910    // Log a message11    console.log(`Processing request for: ${name}`)1213    const data = {14      message: `Hello ${name || 'Guest'}!`,15    }1617    return new Response(JSON.stringify(data), {18      headers: { 'Content-Type': 'application/json' },19    })20  } catch (error) {21    // Log an error message22    console.error(`Request processing failed: ${error.message}`)23    return new Response(JSON.stringify({ error: 'Internal Server Error' }), {24      status: 500,25      headers: { 'Content-Type': 'application/json' },26    })27  }28})
```

A custom log message can contain up to 10,000 characters. A function can log up to 100 events within a 10 second period.


### Logging tips#

### Logging request headers#
When debugging Edge Functions, a common mistake is to try to log headers to the developer console via code like this:

```javascript
1// ❌ This doesn't work as expected23Deno.serve(async (req) => {4  console.log(`Headers: ${JSON.stringify(req.headers)}`) // Outputs: "{}"5})
```

Thereq.headersobject appears empty because Headers objects don't store data in enumerable JavaScript properties, making them opaque toJSON.stringify().

Instead, you have to convert headers to a plain object first, for example usingObject.fromEntries.

```javascript
1// ✅ This works correctly2Deno.serve(async (req) => {3  const headersObject = Object.fromEntries(req.headers)4  const headersJson = JSON.stringify(headersObject, null, 2)56  console.log(`Request headers:\n${headersJson}`)7})
```

This results in something like:

```javascript
1Request headers: {2    "accept": "*/*",3    "accept-encoding": "gzip",4    "authorization": "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InN1cGFuYWNobyIsInJvbGUiOiJhbm9uIiwieW91IjoidmVyeSBzbmVha3ksIGh1aD8iLCJpYXQiOjE2NTQ1NDA5MTYsImV4cCI6MTk3MDExNjkxNn0.cwBbk2tq-fUcKF1S0jVKkOAG2FIQSID7Jjvff5Do99Y",5    "cdn-loop": "cloudflare; subreqs=1",6    "cf-ew-via": "15",7    "cf-ray": "8597a2fcc558a5d7-GRU",8    "cf-visitor": "{\"scheme\":\"https\"}",9    "cf-worker": "supabase.co",10    "content-length": "20",11    "content-type": "application/x-www-form-urlencoded",12    "host": "edge-runtime.supabase.com",13    "my-custom-header": "abcd",14    "user-agent": "curl/8.4.0",15    "x-deno-subhost": "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiIsImtpZCI6InN1cGFiYXNlIn0.eyJkZXBsb3ltZW50X2lkIjoic3VwYW5hY2hvX2M1ZGQxMWFiLTFjYmUtNDA3NS1iNDAxLTY3ZTRlZGYxMjVjNV8wMDciLCJycGNfcm9vdCI6Imh0dHBzOi8vc3VwYWJhc2Utb3JpZ2luLmRlbm8uZGV2L3YwLyIsImV4cCI6MTcwODYxMDA4MiwiaWF0IjoxNzA4NjA5MTgyfQ.-fPid2kEeEM42QHxWeMxxv2lJHZRSkPL-EhSH0r_iV4",16    "x-forwarded-host": "edge-runtime.supabase.com",17    "x-forwarded-port": "443",18    "x-forwarded-proto": "https"19}
```


================================================================================


# Pricing
Source: https://supabase.com/docs/guides/functions/pricing

Pricing

$2per 1 million invocations. You are only charged for usage exceeding your subscription
plan's quota.

For a detailed explanation of how charges are calculated, refer toManage Edge Function Invocations usage.


================================================================================


# Getting Started with Edge Functions
Source: https://supabase.com/docs/guides/functions/quickstart

Getting Started with Edge Functions


### Learn how to create, test, and deploy your first Edge Function using the Supabase CLI.
Learn how to create, test, and deploy your first Edge Function using the Supabase CLI.

Before getting started, make sure you have theSupabase CLI installed. Check out theCLI installation guidefor installation methods and troubleshooting.

You can also create and deploy functions directly from the Supabase Dashboard. Check out ourDashboard Quickstart guide.


### Step 1: Create or configure your project#
If you don't have a project yet, initialize a new Supabase project in your current directory.

```javascript
1supabase init my-edge-functions-project2cd my-edge-functions-project
```

Or, if you already have a project locally, navigate to your project directory. If your project hasn't been configured for Supabase yet, make sure to run thesupabase initcommand.

```javascript
1cd your-existing-project2supabase init # Initialize Supabase, if you haven't already
```

After this step, you should have a project directory with asupabasefolder containingconfig.tomland an emptyfunctionsdirectory.


### Step 2: Create your first function#
Within your project, generate a new Edge Function with a basic template:

```javascript
1supabase functions new hello-world
```

This creates a new function atsupabase/functions/hello-world/index.tswith this starter code:

```javascript
1Deno.serve(async (req) => {2  const { name } = await req.json()3  const data = {4    message: `Hello ${name}!`,5  }67  return new Response(JSON.stringify(data), { headers: { 'Content-Type': 'application/json' } })8})
```

This function accepts a JSON payload with anamefield and returns a greeting message.

After this step, you should have a new file atsupabase/functions/hello-world/index.tscontaining the starter Edge Function code.


### Step 3: Test your function locally#
Start the local development server to test your function:

```javascript
1supabase start  # Start all Supabase services2supabase functions serve hello-world
```

Thesupabase startcommand downloads Docker images, which can take a few minutes initially.

Function not starting locally?

- Make sure Docker is running
- Runsupabase stopthensupabase startto restart services

Port already in use?

- Check what's running withsupabase status
- Stop other Supabase instances withsupabase stop

Your function is now running athttp://localhost:54321/functions/v1/hello-world. Hot reloading is enabled, which means that the server will automatically reload when you save changes to your function code.

After this step, you should have all Supabase services running locally, and your Edge Function serving at the local URL. Keep these terminal windows open.


### Step 4: Send a test request#
Open a new terminal and test your function with curl:

Need yourSUPABASE_PUBLISHABLE_KEY?

Runsupabase statusto see your local anon key and other credentials.

```javascript
1curl -i --location --request POST 'http://localhost:54321/functions/v1/hello-world' \2  --header 'Authorization: Bearer SUPABASE_PUBLISHABLE_KEY' \3  --header 'Content-Type: application/json' \4  --data '{"name":"Functions"}'
```

After running this curl command, you should see:

```javascript
1{ "message": "Hello Functions!" }
```

You can also try different inputs. Change"Functions"to"World"in the curl command and run it again to see the response change.

After this step, you should have successfully tested your Edge Function locally and received a JSON response with your greeting message.


### Step 5: Connect to your Supabase project#
To deploy your function globally, you need to connect your local project to a Supabase project.

Create one atdatabase.new.

First, login to the CLI if you haven't already, and authenticate with Supabase. This opens your browser to authenticate with Supabase; complete the login process in your browser.

```javascript
1supabase login
```

Next, list your Supabase projects to find your project ID:

```javascript
1supabase projects list
```

Next, copy your project ID from the output, then connect your local project to your remote Supabase project. ReplaceYOUR_PROJECT_IDwith the ID from the previous step.

```javascript
1supabase link --project-ref [YOUR_PROJECT_ID]
```

After this step, you should have your local project authenticated and linked to your remote Supabase project. You can verify this by runningsupabase status.


### Step 6: Deploy to production#
Deploy your function to Supabase's global edge network:

```javascript
1supabase functions deploy hello-world23# If you want to deploy all functions, run the `deploy` command without specifying a function name:4supabase functions deploy
```

The CLI automatically falls back to API-based deployment if Docker isn't available. You can also explicitly use API deployment with the--use-apiflag:

```javascript
1supabase functions deploy hello-world --use-api
```

If you want to skip JWT verification, you can add the--no-verify-jwtflag for webhooks that don't need authentication:

```javascript
1supabase functions deploy hello-world --no-verify-jwt
```

Use--no-verify-jwtcarefully.It allows anyone to invoke your function without authentication!

When the deployment is successful, your function is automatically distributed to edge locations worldwide.

Now, you should have your Edge Function deployed and running globally athttps://[YOUR_PROJECT_ID].supabase.co/functions/v1/hello-world.


### Step 7: Test your live function#
🎉 Your function is now live! Test it with your project's anon key:

```javascript
1curl --request POST 'https://[YOUR_PROJECT_ID].supabase.co/functions/v1/hello-world' \2  --header 'Authorization: Bearer SUPABASE_PUBLISHABLE_KEY' \3  --header 'Content-Type: application/json' \4  --data '{"name":"Production"}'
```

Expected response:

```javascript
1{ "message": "Hello Production!" }
```

TheSUPABASE_PUBLISHABLE_KEYis different in development and production. To get your production anon key, you can find it in your Supabase dashboard underSettings > API.

Finally, you should have a fully deployed Edge Function that you can call from anywhere in the world.


### Usage#
Now that your function is deployed, you can invoke it from within your app:

```javascript
1import { createClient } from '@supabase/supabase-js'23const supabase = createClient('https://[YOUR_PROJECT_ID].supabase.co', 'YOUR_ANON_KEY')45const { data, error } = await supabase.functions.invoke('hello-world', {6  body: { name: 'JavaScript' },7})89console.log(data) // { message: "Hello JavaScript!" }
```


================================================================================


# Getting Started with Edge Functions (Dashboard)
Source: https://supabase.com/docs/guides/functions/quickstart-dashboard

Getting Started with Edge Functions (Dashboard)


### Learn how to create, test, and deploy your first Edge Function using the Supabase Dashboard.
Learn how to create, test, and deploy your first Edge Function using the Supabase Dashboard.

Supabase allows you to create Supabase Edge Functions directly from the Supabase Dashboard, making it easy to deploy functions without needing to set up a local development environment. The Edge Functions editor in the Dashboard has built-in syntax highlighting and type-checking for Deno and Supabase-specific APIs.

This guide will walk you through creating, testing, and deploying your first Edge Function using the Supabase Dashboard. You'll have a working function running globally in under 10 minutes.

You can also create and deploy functions using the Supabase CLI. Check out ourCLI Quickstart guide.

You'll need a Supabase project to get started. If you don't have one yet, create a new project atdatabase.new.


### Step 1: Navigate to the Edge Functions tab#
Navigate to your Supabase project dashboard and locate the Edge Functions section:

You'll see the Edge Functions overview page where you can manage all your functions.


### Step 2: Create your first function#
Click the"Deploy a new function"button and select"Via Editor"to create a function directly in the dashboard.

The dashboard offers several pre-built templates for common use cases, such as Stripe Webhooks, OpenAI proxying, uploading files to Supabase Storage, and sending emails.

For this guide, we’ll select the"Hello World"template. If you’d rather start from scratch, you can ignore the pre-built templates.


### Step 3: Customize your function code#
The dashboard will load your chosen template in the code editor. Here's what the "Hello World" template looks like:

If needed, you can modify this code directly in the browser editor. The function accepts a JSON payload with anamefield and returns a greeting message.


### Step 4: Deploy your function#
Once you're happy with your function code:

🚀 Your function is now automatically distributed to edge locations worldwide, running athttps://YOUR_PROJECT_ID.supabase.co/functions/v1/hello-world


### Step 5: Test your function#
Supabase has built-in tools for testing your Edge Functions from the Dashboard. You can execute your Edge Function with different request payloads, headers, and query parameters. The built-in tester returns the response status, headers, and body.

On your function's details page:

- HTTP Method: POST (or whatever your function expects)
- Headers: Add any required headers likeContent-Type: application/json
- Query Parameters: Add URL parameters if needed
- Request Body: Add your JSON payload
- Authorization: Change the authorization token (anon key or user key)

Click"Send Request"to test your function.

In this example, we successfully tested our Hello World function by sending a JSON payload with a name field, and received the expected greeting message back.


### Step 6: Get your function URL and keys#
Your function is now live at:

```javascript
1https://YOUR_PROJECT_ID.supabase.co/functions/v1/hello-world
```

To invoke this Edge Function from within your application, you'll need API keys. Navigate toSettings > API Keysin your dashboard to find:

- Anon Key- For client-side requests (safe to use in browsers with RLS enabled)
- Service Role Key- For server-side requests (keep this secret! bypasses RLS)

If you’d like to update the deployed function code, click on the function you want to edit, modify the code as needed, then click Deploy updates. This will overwrite the existing deployment with the newly edited function code.

There is currentlyno version controlfor edits! The Dashboard's Edge Function editor currently does not support version control, versioning, or rollbacks. We recommend using it only for quick testing and prototypes.


### Usage#
Now that your function is deployed, you can invoke it from within your app:

```javascript
1import { createClient } from '@supabase/supabase-js'23const supabase = createClient('https://[YOUR_PROJECT_ID].supabase.co', 'YOUR_ANON_KEY')45const { data, error } = await supabase.functions.invoke('hello-world', {6  body: { name: 'JavaScript' },7})89console.log(data) // { message: "Hello JavaScript!" }
```


### Deploy via Assistant#
You can also use Supabase's AI Assistant to generate and deploy functions automatically.

Go to your project >Deploy a new function>Via AI Assistant.

Describe what you want your function to do in the prompt

ClickDeployand the Assistant will create and deploy the function for you.


### Download Edge Functions#
Now that your function is deployed, you can access it from your local development environment. To use your Edge Function code within your local development environment, you can download your function source code either through the dashboard, or the CLI.


### Dashboard#

### CLI#
Before getting started, make sure you have theSupabase CLI installed. Check out theCLI installation guidefor installation methods and troubleshooting.

```javascript
1# Link your project to your local environment2supabase link --project-ref [project-ref]34# List all functions in the linked project5supabase functions list67# Download a function8supabase functions download hello-world
```

At this point, your function has been downloaded to your local environment. Make the required changes, and redeploy when you're ready.

```javascript
1# Run a function locally2supabase functions serve hello-world34# Redeploy when you're ready with your changes5supabase functions deploy hello-world
```


================================================================================


# Regional Invocations
Source: https://supabase.com/docs/guides/functions/regional-invocation

Regional Invocations


### Execute Edge Functions in specific regions for optimal performance.
Execute Edge Functions in specific regions for optimal performance.

Edge Functions automatically execute in the region closest to the user making the request. This reduces network latency and provides faster responses.

However, if your function performs intensive database or storage operations, executing in the same region as your database often provides better performance:

- Bulk database operations:Adding or editing many records
- File uploads:Processing large files or multiple uploads
- Complex queries:Operations requiring multiple database round trips


### Available regions#
The following regions are supported:

Asia Pacific:

- ap-northeast-1(Tokyo)
- ap-northeast-2(Seoul)
- ap-south-1(Mumbai)
- ap-southeast-1(Singapore)
- ap-southeast-2(Sydney)

North America:

- ca-central-1(Canada Central)
- us-east-1(N. Virginia)
- us-west-1(N. California)
- us-west-2(Oregon)

Europe:

- eu-central-1(Frankfurt)
- eu-west-1(Ireland)
- eu-west-2(London)
- eu-west-3(Paris)

South America:

- sa-east-1(São Paulo)


### Usage#
You can specify the region programmatically using the Supabase Client library, or using thex-regionHTTP header.

```javascript
1import { createClient, FunctionRegion } from '@supabase/supabase-js'23const { data, error } = await supabase.functions.invoke('function-name', {4  ...5  region: FunctionRegion.UsEast1, // Execute in us-east-1 region6})
```

In case you cannot add thex-regionheader to the request (e.g.: CORS requests, Webhooks), you can useforceFunctionRegionquery parameter.

You can verify the execution region by looking at thex-sb-edge-regionHTTP header in the response. You can also find it as metadata inEdge Function Logs.


### Region runtime information#
Functions have access to the following environment variables:

SB_REGION: The AWS region function was invoked

This is useful if you have read replicate and want to Postgres connect to a different replicate according of the Region.


### Region outages#
When you explicitly specify a region via thex-regionheader, requests will NOT be automatically
re-routed to another region.

During outages, consider temporarily changing to a different region.

Test your function's performance with and without regional specification to determine if the benefits outweigh automatic region selection.


================================================================================


# Handling Routing in Functions
Source: https://supabase.com/docs/guides/functions/routing

Handling Routing in Functions


### Handle custom routing within Edge Functions.
Handle custom routing within Edge Functions.

Usually, an Edge Function is written to perform a single action (e.g. write a record to the database). However, if your app's logic is split into multiple Edge Functions, requests to each action may seem slower.

Each Edge Function needs to be booted before serving a request (known as cold starts). If an action is performed less frequently (e.g. deleting a record), there is a high chance of that function experiencing a cold start.

One way to reduce cold starts and increase performance is to combine multiple actions into a single Edge Function. This way only one instance needs to be booted and it can handle multiple requests to different actions.

This allows you to:

- Reduce cold starts by combining multiple actions into one function
- Build complete REST APIs in a single function
- Improve performance by keeping one instance warm for multiple endpoints

For example, we can use a single Edge Function to create a typical CRUD API (create, read, update, delete records).

To combine multiple endpoints into a single Edge Function, you can use web application frameworks such asExpress,Oak, orHono.


### Basic routing example#
Here's a simple hello world example using some popular web frameworks:

```javascript
1import { Hono } from 'jsr:@hono/hono'23const app = new Hono()45app.post('/hello-world', async (c) => {6  const { name } = await c.req.json()7  return new Response(`Hello ${name}!`)8})910app.get('/hello-world', (c) => {11  return new Response('Hello World!')12})1314Deno.serve(app.fetch)
```

Within Edge Functions, paths should always be prefixed with the function name (in this casehello-world).


### Using route parameters#
You can use route parameters to capture values at specific URL segments (e.g./tasks/:taskId/notes/:noteId).

Keep in mind paths must be prefixed by function name. Route parameters can only be used after the function name prefix.

```javascript
1interface  {2  : string3  : string4}56let : [] = []78const  = new <string, (: ) => <>>()910async function (): <> {11  return new (.())12}1314async function (: string): <> {15  const  = .(() => . === )16  if () {17    return new (.())18  } else {19    return new ('Task not found', { : 404 })20  }21}2223async function (: ): <> {24  const  = .().(36).(7)25  const  = { , : '' }26  .()27  return new (.(), { : 201 })28}2930async function (: string, : ): <> {31  const  = .(() => . === )32  if ( !== -1) {33    [] = { ...[] }34    return new (.([]))35  } else {36    return new ('Task not found', { : 404 })37  }38}3940async function (: string): <> {41  const  = .(() => . === )42  if ( !== -1) {43    .(, 1)44    return new ('Task deleted successfully')45  } else {46    return new ('Task not found', { : 404 })47  }48}4950.(async () => {51  const  = new (.)52  const  = .53  // Extract the last part of the path as the command54  const  = ..('/').()55  // Assuming the last part of the path is the task ID56  const  = 57  try {58    switch () {59      case 'GET':60        if () {61          return ()62        } else {63          return ()64        }65      case 'POST':66        return ()67      case 'PUT':68        if () {69          return (, )70        } else {71          return new ('Bad Request', { : 400 })72        }73      case 'DELETE':74        if () {75          return ()76        } else {77          return new ('Bad Request', { : 400 })78        }79      default:80        return new ('Method Not Allowed', { : 405 })81    }82  } catch () {83    return new (`Internal Server Error: ${}`, { : 500 })84  }85})
```


### URL Patterns API#
If you prefer not to use a web framework, you can directly useURL Pattern APIwithin your Edge Functions to implement routing.

This works well for small apps with only a couple of routes:

```javascript
1// ...23    )45    // For more details on URLPattern, check https://developer.mozilla.org/en-US/docs/Web/API/URL_Pattern_API6    const taskPattern = new URLPattern({ pathname: '/restful-tasks/:id' })7    const matchingPath = taskPattern.exec(url)8    const id = matchingPath ? matchingPath.pathname.groups.id : null910    let task = null11    if (method === 'POST' || method === 'PUT') {12      const body = await req.json()13      task = body.task14    }1516    // call relevant method based on method and id17    switch (true) {18      case id && method === 'GET':19        return getTask(supabaseClient, id as string)20      case id && method === 'PUT':21        return updateTask(supabaseClient, id as string, task)22      case id && method === 'DELETE':23        return deleteTask(supabaseClient, id as string)24      case method === 'POST':25        return createTask(supabaseClient, task)26      case method === 'GET':27        return getAllTasks(supabaseClient)2829    // ...
```


================================================================================


# Scheduling Edge Functions
Source: https://supabase.com/docs/guides/functions/schedule-functions

Scheduling Edge Functions

The hosted Supabase Platform supports thepg_cronextension, a recurring job scheduler in Postgres.

In combination with thepg_netextension, this allows us to invoke Edge Functions periodically on a set schedule.

To access the auth token securely for your Edge Function call, we recommend storing them inSupabase Vault.


### Examples#

### Invoke an Edge Function every minute#
Storeproject_urlandanon_keyin Supabase Vault:

```javascript
1select vault.create_secret('https://project-ref.supabase.co', 'project_url');2select vault.create_secret('YOUR_SUPABASE_PUBLISHABLE_KEY', 'publishable_key');
```

Make a POST request to a Supabase Edge Function every minute:

```javascript
1select2  cron.schedule(3    'invoke-function-every-minute',4    '* * * * *', -- every minute5    $$6    select7      net.http_post(8          url:= (select decrypted_secret from vault.decrypted_secrets where name = 'project_url') || '/functions/v1/function-name',9          headers:=jsonb_build_object(10            'Content-type', 'application/json',11            'Authorization', 'Bearer ' || (select decrypted_secret from vault.decrypted_secrets where name = 'anon_key')12          ),13          body:=concat('{"time": "', now(), '"}')::jsonb14      ) as request_id;15    $$16  );
```


### Resources#
- pg_netextension
- pg_cronextension


================================================================================


# Environment Variables
Source: https://supabase.com/docs/guides/functions/secrets

Environment Variables


### Manage sensitive data securely across environments.
Manage sensitive data securely across environments.


### Default secrets#
Edge Functions have access to these secrets by default:

- SUPABASE_URL: The API gateway for your Supabase project
- SUPABASE_ANON_KEY: Theanonkey for your Supabase API. This is safe to use in a browser when you have Row Level Security enabled
- SUPABASE_SERVICE_ROLE_KEY: Theservice_rolekey for your Supabase API. This is safe to use in Edge Functions, but it should NEVER be used in a browser. This key will bypass Row Level Security
- SUPABASE_DB_URL: The URL for your Postgres database. You can use this to connect directly to your database

In a hosted environment, functions have access to the following environment variables:

- SB_REGION: The region function was invoked
- SB_EXECUTION_ID: A UUID of function instance (isolate)
- DENO_DEPLOYMENT_ID: Version of the function code ({project_ref}_{function_id}_{version})


### Accessing environment variables#
You can access environment variables using Deno's built-in handler, and passing it the name of the environment variable you’d like to access.

```javascript
1..('NAME_OF_SECRET')
```

For example, in a function:

```javascript
1import { createClient } from 'npm:@supabase/supabase-js@2'23// For user-facing operations (respects RLS)4const supabase = createClient(5  Deno.env.get('SUPABASE_URL')!,6  Deno.env.get('SUPABASE_ANON_KEY')!7)89// For admin operations (bypasses RLS)10const supabaseAdmin = createClient(11  Deno.env.get('SUPABASE_URL')!,12  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!13)
```


### Local secrets#
In development, you can load environment variables in two ways:

```javascript
1supabase functions serve --env-file .env.local
```

Never check your.envfiles into Git! Instead, add the path to this file to your.gitignore.

We can automatically access the secrets in our Edge Functions through Deno’s handler

```javascript
1const secretKey = Deno.env.get('STRIPE_SECRET_KEY')
```

Now we can invoke our function locally. If you're using the default.envfile atsupabase/functions/.env, it's automatically loaded:

```javascript
1supabase functions serve hello-world
```

Or you can specify a custom.envfile with the--env-fileflag:

```javascript
1supabase functions serve hello-world --env-file .env.local
```

This is useful for managing different environments (development, staging, etc.).


### Production secrets#
You will also need to set secrets for your production Edge Functions. You can do this via the Dashboard or using the CLI.

Using the Dashboard:

Note that you can paste multiple secrets at a time.

Using the CLI

You can create a.envfile to help deploy your secrets to production

```javascript
1# .env2STRIPE_SECRET_KEY=sk_live_...
```

Never check your.envfiles into Git! Instead, add the path to this file to your.gitignore.

You can push all the secrets from the.envfile to your remote project usingsupabase secrets set. This makes the environment visible in the dashboard as well.

```javascript
1supabase secrets set --env-file .env
```

Alternatively, this command also allows you to set production secrets individually rather than storing them in a.envfile.

```javascript
1supabase secrets set STRIPE_SECRET_KEY=sk_live_...
```

To see all the secrets which you have set remotely, you can usesupabase secrets list

```javascript
1supabase secrets list
```

You don't need to re-deploy after setting your secrets. They're available immediately in your
functions.


================================================================================


# Status codes
Source: https://supabase.com/docs/guides/functions/status-codes

Status codes


### Understand HTTP status codes returned by Edge Functions to properly debug issues and handle responses.
Understand HTTP status codes returned by Edge Functions to properly debug issues and handle responses.


### Success Responses#

### 2XX Success#
Your Edge Function executed successfully and returned a valid response. This includes any status code in the 200-299 range that your function explicitly returns.


### 3XX Redirect#
Your Edge Function used theResponse.redirect()API to redirect the client to a different URL. This is a normal response when implementing authentication flows or URL forwarding.


### Client Errors#
These errors indicate issues with the request itself, which typically require changing how the function is called.


### 401 Unauthorized#
Cause:The Edge Function has JWT verification enabled, but the request was made with an invalid or missing JWT token.

Solution:

- Ensure you're passing a valid JWT token in theAuthorizationheader
- Check that your token hasn't expired
- For webhooks or public endpoints, consider disabling JWT verification


### 404 Not Found#
Cause:The requested Edge Function doesn't exist or the URL path is incorrect.

Solution:

- Verify the function name and project reference in your request URL
- Check that the function has been deployed successfully


### 405 Method Not Allowed#
Cause:You're using an unsupported HTTP method. Edge Functions only support:GET,POST,PUT,PATCH,DELETE, andOPTIONS.

Solution:Update your request to use a supported HTTP method.


### Server Errors#
These errors indicate issues with the function execution or underlying platform.


### 500 Internal Server Error#
Cause:Your Edge Function threw an uncaught exception (WORKER_ERROR).

Common causes:

- Unhandled JavaScript errors in your function code
- Missing error handling for async operations
- Invalid JSON parsing

Solution:Check your Edge Function logs to identify the specific error and add proper error handling to your code.

```javascript
1// ✅ Good error handling2try {3  const result = await someAsyncOperation()4  return new Response(JSON.stringify(result))5} catch (error) {6  console.error('Function error:', error)7  return new Response('Internal error', { status: 500 })8}
```

You can see the output in theEdge Function Logs.


### 503 Service Unavailable#
Cause:Your Edge Function failed to start (BOOT_ERROR).

Common causes:

- Syntax errors preventing the function from loading
- Import errors or missing dependencies
- Invalid function configuration

Solution:Check your Edge Function logs and verify your function code can be executed locally withsupabase functions serve.


### 504 Gateway Timeout#
Cause:Your Edge Function didn't respond within therequest timeout limit.

Common causes:

- Long-running database queries
- Slow external API calls
- Infinite loops or blocking operations

Solution:

- Optimize slow operations
- Add timeout handling to external requests
- Consider breaking large operations into smaller chunks


### 546 Resource Limit (Custom Error Code)#
Cause:Your Edge Function execution was stopped due to exceeding resource limits (WORKER_LIMIT). Edge Function logs should provide whichresource limitwas exceeded.

Common causes:

- Memory usage exceeded available limits
- CPU time exceeded execution quotas
- Too many concurrent operations

Solution:Check your Edge Function logs to see which resource limit was exceeded, then optimize your function accordingly.


================================================================================


# Integrating with Supabase Storage
Source: https://supabase.com/docs/guides/functions/storage-caching

Integrating with Supabase Storage

Edge Functions work seamlessly withSupabase Storage. This allows you to:

- Upload generated content directly from your functions
- Implement cache-first patterns for better performance
- Serve files with built-in CDN capabilities


### Basic file operations#
Use the Supabase client to upload files directly from your Edge Functions. You'll need the service role key for server-side storage operations:

```javascript
1import { createClient } from 'npm:@supabase/supabase-js@2'23Deno.serve(async (req) => {4  const supabaseAdmin = createClient(5    Deno.env.get('SUPABASE_URL') ?? '',6    Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''7  )89  // Generate your content10  const fileContent = await generateImage()1112  // Upload to storage13  const { data, error } = await supabaseAdmin.storage14    .from('images')15    .upload(`generated/${filename}.png`, fileContent.body!, {16      contentType: 'image/png',17      cacheControl: '3600',18      upsert: false,19    })2021  if (error) {22    throw error23  }2425  return new Response(JSON.stringify({ path: data.path }))26})
```

Always use theSUPABASE_SERVICE_ROLE_KEYfor server-side operations. Never expose this key in client-side code!


### Cache-first pattern#
Check storage before generating new content to improve performance:

```javascript
1const STORAGE_URL = 'https://your-project.supabase.co/storage/v1/object/public/images'23Deno.serve(async (req) => {4  const url = new URL(req.url)5  const username = url.searchParams.get('username')67  try {8    // Try to get existing file from storage first9    const storageResponse = await fetch(`${STORAGE_URL}/avatars/${username}.png`)1011    if (storageResponse.ok) {12      // File exists in storage, return it directly13      return storageResponse14    }1516    // File doesn't exist, generate it17    const generatedImage = await generateAvatar(username)1819    // Upload to storage for future requests20    const { error } = await supabaseAdmin.storage21      .from('images')22      .upload(`avatars/${username}.png`, generatedImage.body!, {23        contentType: 'image/png',24        cacheControl: '86400', // Cache for 24 hours25      })2627    if (error) {28      console.error('Upload failed:', error)29    }3031    return generatedImage32  } catch (error) {33    return new Response('Error processing request', { status: 500 })34  }35})
```


================================================================================


# Testing your Edge Functions
Source: https://supabase.com/docs/guides/functions/unit-test

Testing your Edge Functions


### Writing Unit Tests for Edge Functions using Deno Test
Writing Unit Tests for Edge Functions using Deno Test

Testing is an essential step in the development process to ensure the correctness and performance of your Edge Functions.


### Testing in Deno#
Deno has a built-in test runner that you can use for testing JavaScript or TypeScript code. You can read theofficial documentationfor more information and details about the available testing functions.


### Folder structure#
We recommend creating your testing in asupabase/functions/testsdirectory, using the same name as the Function followed by-test.ts:

```javascript
1└── supabase2    ├── functions3    │   ├── function-one4    │   │   └── index.ts5    │   └── function-two6    │   │   └── index.ts7    │   └── tests8    │       └── function-one-test.ts  # Tests for function-one9    │       └── function-two-test.ts  # Tests for function-two10    └── config.toml
```


### Example#
The following script is a good example to get started with testing your Edge Functions:

```javascript
1// Import required libraries and modules2import { assert, assertEquals } from 'jsr:@std/assert@1'3import { createClient, SupabaseClient } from 'npm:@supabase/supabase-js@2'45// Will load the .env file to Deno.env6import 'jsr:@std/dotenv/load'78// Set up the configuration for the Supabase client9const supabaseUrl = Deno.env.get('SUPABASE_URL') ?? ''10const supabaseKey = Deno.env.get('SUPABASE_PUBLISHABLE_KEY') ?? ''11const options = {12  auth: {13    autoRefreshToken: false,14    persistSession: false,15    detectSessionInUrl: false,16  },17}1819// Test the creation and functionality of the Supabase client20const testClientCreation = async () => {21  var client: SupabaseClient = createClient(supabaseUrl, supabaseKey, options)2223  // Verify if the Supabase URL and key are provided24  if (!supabaseUrl) throw new Error('supabaseUrl is required.')25  if (!supabaseKey) throw new Error('supabaseKey is required.')2627  // Test a simple query to the database28  const { data: table_data, error: table_error } = await client29    .from('my_table')30    .select('*')31    .limit(1)32  if (table_error) {33    throw new Error('Invalid Supabase client: ' + table_error.message)34  }35  assert(table_data, 'Data should be returned from the query.')36}3738// Test the 'hello-world' function39const testHelloWorld = async () => {40  var client: SupabaseClient = createClient(supabaseUrl, supabaseKey, options)4142  // Invoke the 'hello-world' function with a parameter43  const { data: func_data, error: func_error } = await client.functions.invoke('hello-world', {44    body: { name: 'bar' },45  })4647  // Check for errors from the function invocation48  if (func_error) {49    throw new Error('Invalid response: ' + func_error.message)50  }5152  // Log the response from the function53  console.log(JSON.stringify(func_data, null, 2))5455  // Assert that the function returned the expected result56  assertEquals(func_data.message, 'Hello bar!')57}5859// Register and run the tests60Deno.test('Client Creation Test', testClientCreation)61Deno.test('Hello-world Function Test', testHelloWorld)
```

This test case consists of two parts.

- We import various testing functions from the Deno standard library, includingassert,assertExists, andassertEquals.
- We import thecreateClientandSupabaseClientclasses from the@supabase/supabase-jslibrary to interact with the Supabase client.
- We define the necessary configuration for the Supabase client, including the Supabase URL, API key, and authentication options.
- ThetestClientCreationfunction tests the creation of a Supabase client instance and queries the database for data from a table. It verifies that data is returned from the query.
- ThetestHelloWorldfunction tests the "Hello-world" Edge Function by invoking it using the Supabase client'sfunctions.invokemethod. It checks if the response message matches the expected greeting.
- We run the tests using theDeno.testfunction, providing a descriptive name for each test case and the corresponding test function.

Make sure to replace the placeholders (supabaseUrl,supabaseKey,my_table) with the actual values relevant to your Supabase setup.


### Running Edge Functions locally#
To locally test and debug Edge Functions, you can utilize the Supabase CLI. Let's explore how to run Edge Functions locally using the Supabase CLI:

Ensure that the Supabase server is running by executing the following command:

```javascript
1supabase start
```

In your terminal, use the following command to serve the Edge Functions locally:

```javascript
1supabase functions serve
```

This command starts a local server that runs your Edge Functions, enabling you to test and debug them in a development environment.

Create the environment variables file:

```javascript
1# creates the file2touch .env3# adds the SUPABASE_URL secret4echo "SUPABASE_URL=http://localhost:54321" >> .env5# adds the SUPABASE_PUBLISHABLE_KEY secret6echo "SUPABASE_PUBLISHABLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0" >> .env7# Alternatively, you can open it in your editor:8open .env
```

To run the tests, use the following command in your terminal:

```javascript
1deno test --allow-all supabase/functions/tests/function-one-test.ts
```


### Resources#
- Full guide on Testing Supabase Edge Functions onMansueli's tips


================================================================================


# Using Wasm modules
Source: https://supabase.com/docs/guides/functions/wasm

Using Wasm modules


### Use WebAssembly in Edge Functions.
Use WebAssembly in Edge Functions.

Edge Functions supports runningWebAssembly (Wasm)modules. WebAssembly is useful if you want to optimize code that's slower to run in JavaScript or require low-level manipulation.

This allows you to:

- Optimize performance-critical code beyond JavaScript capabilities
- Port existing libraries from other languages (C, C++, Rust) to JavaScript
- Access low-level system operations not available in JavaScript

For example, libraries likemagick-wasmport existing C libraries to WebAssembly for complex image processing.


### Writing a Wasm module#
You can use different languages and SDKs to write Wasm modules. For this tutorial, we will write a simple Wasm module in Rust that adds two numbers.

Follow thisguide on writing Wasm modules in Rustto setup your dev environment.


### Create a new Edge Function
Create a new Edge Function calledwasm-add

```javascript
1supabase functions new wasm-add
```


### Create a new Cargo project
Create a new Cargo project for the Wasm module inside the function's directory:

```javascript
1cd supabase/functions/wasm-add2cargo new --lib add-wasm
```


### Add the Wasm module code
Add the following code toadd-wasm/src/lib.rs.

```javascript
1use wasm_bindgen::prelude::*;23#[wasm_bindgen]4pub fn add(a: u32, b: u32) -> u32 {5    a + b6}
```


### Update the Cargo.toml file
Update theadd-wasm/Cargo.tomlto include thewasm-bindgendependency.

```javascript
1[package]2name = "add-wasm"3version = "0.1.0"4description = "A simple wasm module that adds two numbers"5license = "MIT/Apache-2.0"6edition = "2021"78[lib]9crate-type = ["cdylib"]1011[dependencies]12wasm-bindgen = "0.2"
```


### Build the Wasm module
Build the package by running:

```javascript
1wasm-pack build --target deno
```

This will produce a Wasm binary file insideadd-wasm/pkgdirectory.


### Calling the Wasm module from the Edge Function#
Update your Edge Function to call the add function from the Wasm module:

```javascript
1import { add } from "./add-wasm/pkg/add_wasm.js";23Deno.serve(async (req) => {4  const { a, b } = await req.json();5  return new Response(6    JSON.stringify({ result: add(a, b) }),7    { headers: { "Content-Type": "application/json" } },8  );9});
```

Supabase Edge Functions currently use Deno 1.46. FromDeno 2.1, importing Wasm moduleswill require even less boilerplate code.


### Bundle and deploy#
Before deploying, ensure the Wasm module is bundled with your function by defining it insupabase/config.toml:

- You will need update Supabase CLI to 2.7.0 or higher for thestatic_filessupport.
- Static files cannot be deployed using the--use-apiAPI flag. You need to build them withDocker on the CLI.

```javascript
1[functions.wasm-add]2static_files = [ "./functions/wasm-add/add-wasm/pkg/*"]
```

Deploy the function by running:

```javascript
1supabase functions deploy wasm-add
```


================================================================================


# Handling WebSockets
Source: https://supabase.com/docs/guides/functions/websockets

Handling WebSockets


### Handle WebSocket connections in Edge Functions.
Handle WebSocket connections in Edge Functions.

Edge Functions supports hosting WebSocket servers that can facilitate bi-directional communications with browser clients.

This allows you to:

- Build real-time applications like chat or live updates
- Create WebSocket relay servers for external APIs
- Establish both incoming and outgoing WebSocket connections


### Creating WebSocket servers#
Here are some basic examples of setting up WebSocket servers using Deno and Node.js APIs.

```javascript
1Deno.serve((req) => {2  const upgrade = req.headers.get('upgrade') || ''34  if (upgrade.toLowerCase() != 'websocket') {5    return new Response("request isn't trying to upgrade to WebSocket.", { status: 400 })6  }78  const { socket, response } = Deno.upgradeWebSocket(req)910  socket.onopen = () => console.log('socket opened')11  socket.onmessage = (e) => {12    console.log('socket message:', e.data)13    socket.send(new Date().toString())14  }1516  socket.onerror = (e) => console.log('socket errored:', e.message)17  socket.onclose = () => console.log('socket closed')1819  return response20})
```


### Outbound WebSockets#
You can also establish an outbound WebSocket connection to another server from an Edge Function.

Combining it with incoming WebSocket servers, it's possible to use Edge Functions as a WebSocket proxy, for example as arelay serverfor theOpenAI Realtime API.

```javascript
1import { createServer } from "node:http";2import { WebSocketServer } from "npm:ws";3import { RealtimeClient } from "https://raw.githubusercontent.com/openai/openai-realtime-api-beta/refs/heads/main/lib/client.js";45// ...67const OPENAI_API_KEY = Deno.env.get("OPENAI_API_KEY");89const server = createServer();10// Since we manually created the HTTP server,11// turn on the noServer mode.12const wss = new WebSocketServer({ noServer: true });1314wss.on("connection", async (ws) => {15  console.log("socket opened");16  if (!OPENAI_API_KEY) {17    throw new Error("OPENAI_API_KEY is not set");18  }19  // Instantiate new client20  console.log(`Connecting with key "${OPENAI_API_KEY.slice(0, 3)}..."`);21  const client = new RealtimeClient({ apiKey: OPENAI_API_KEY });2223  // Relay: OpenAI Realtime API Event -> Browser Event24  client.realtime.on("server.*", (event) => {25    console.log(`Relaying "${event.type}" to Client`);26    ws.send(JSON.stringify(event));27  });28  client.realtime.on("close", () => ws.close());2930  // Relay: Browser Event -> OpenAI Realtime API Event31  // We need to queue data waiting for the OpenAI connection32  const messageQueue = [];33  const messageHandler = (data) => {34    try {35      const event = JSON.parse(data);36      console.log(`Relaying "${event.type}" to OpenAI`);37      client.realtime.send(event.type, event);38    } catch (e) {39      console.error(e.message);40      console.log(`Error parsing event from client: ${data}`);41    }42  };4344  ws.on("message", (data) => {45    if (!client.isConnected()) {46      messageQueue.push(data);47    } else {48      messageHandler(data);49    }50  });51  ws.on("close", () => client.disconnect());5253  // Connect to OpenAI Realtime API54  try {55    console.log(`Connecting to OpenAI...`);56    await client.connect();57  } catch (e) {58    console.log(`Error connecting to OpenAI: ${e.message}`);59    ws.close();60    return;61  }62  console.log(`Connected to OpenAI successfully!`);63  while (messageQueue.length) {64    messageHandler(messageQueue.shift());65  }66});6768server.on("upgrade", (req, socket, head) => {69  wss.handleUpgrade(req, socket, head, (ws) => {70    wss.emit("connection", ws, req);71  });72});7374server.listen(8080);
```


### Authentication#
WebSocket browser clients don't have the option to send custom headers. Because of this, Edge Functions won't be able to perform the usual authorization header check to verify the JWT.

You can skip the default authorization header checks by explicitly providing--no-verify-jwtwhen serving and deploying functions.

To authenticate the user making WebSocket requests, you can pass the JWT in URL query params or via a custom protocol.

```javascript
1import { createClient } from 'npm:@supabase/supabase-js@2'23const supabase = createClient(4  Deno.env.get('SUPABASE_URL'),5  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')6)78Deno.serve((req) => {9  const upgrade = req.headers.get('upgrade') || ''10  if (upgrade.toLowerCase() != 'WebSocket') {11    return new Response("request isn't trying to upgrade to WebSocket.", { status: 400 })12  }1314  // Please be aware query params may be logged in some logging systems.15  const url = new URL(req.url)16  const jwt = url.searchParams.get('jwt')1718  if (!jwt) {19    console.error('Auth token not provided')20    return new Response('Auth token not provided', { status: 403 })21  }2223  const { error, data } = await supabase.auth.getClaims()2425  if (error) {26    console.error(error)27    return new Response('Invalid token provided', { status: 403 })28  }2930  if (!data.user) {31    console.error('user is not authenticated')32    return new Response('User is not authenticated', { status: 403 })33  }3435  const { socket, response } = Deno.upgradeWebSocket(req)3637  socket.onopen = () => console.log('socket opened')38  socket.onmessage = (e) => {39    console.log('socket message:', e.data)40    socket.send(new Date().toString())41  }4243  socket.onerror = (e) => console.log('socket errored:', e.message)44  socket.onclose = () => console.log('socket closed')4546  return response47})
```

The maximum duration is capped based on the wall-clock, CPU, and memory limits. The Function will shutdown when it reaches one of theselimits.


### Testing WebSockets locally#
When testing Edge Functions locally with Supabase CLI, the instances are terminated automatically after a request is completed. This will prevent keeping WebSocket connections open.

To prevent that, you can update thesupabase/config.tomlwith the following settings:

```javascript
1[edge_runtime]2policy = "per_worker"
```

When running withper_workerpolicy, Function won't auto-reload on edits. You will need to manually restart it by runningsupabase functions serve.


================================================================================


# Realtime
Source: https://supabase.com/docs/guides/realtime

Realtime


### Send and receive messages to connected clients.
Send and receive messages to connected clients.

Supabase provides a globally distributedRealtimeservice with the following features:

- Broadcast: Send low-latency messages between clients. Perfect for real-time messaging, database changes, cursor tracking, game events, and custom notifications.
- Presence: Track and synchronize user state across clients. Ideal for showing who's online, or active participants.
- Postgres Changes: Listen to database changes in real-time.


### What can you build?#
- Chat applications- Real-time messaging with typing indicators and online presence
- Collaborative tools- Document editing, whiteboards, and shared workspaces
- Live dashboards- Real-time data visualization and monitoring
- Multiplayer games- Synchronized game state and player interactions
- Social features- Live notifications, reactions, and user activity feeds

Check theGetting Startedguide to get started.


### Examples#
Multiplayer.dev

Chat

Avatar Stack

Realtime Cursor


### Resources#
Find the source code and documentation in the Supabase GitHub repository.

Supabase Realtime

Realtime: Multiplayer Edition


================================================================================


# Realtime Architecture
Source: https://supabase.com/docs/guides/realtime/architecture

Realtime Architecture

Realtime is a globally distributed Elixir cluster. Clients can connect to any node in the cluster via WebSockets and send messages to any other client connected to the cluster.

Realtime is written inElixir, which compiles toErlang, and utilizes many tools thePhoenix Frameworkprovides out of the box.


### Elixir & Phoenix#
Phoenix is fast and able to handle millions of concurrent connections.

Phoenix can handle many concurrent connections because Elixir provides lightweight processes (not OS processes) to work with.

Client-facing WebSocket servers need to handle many concurrent connections. Elixir & Phoenix let the Supabase Realtime cluster do this easily.


### Channels#
Channels are implemented usingPhoenix Channelswhich usesPhoenix.PubSubwith the defaultPhoenix.PubSub.PG2adapter.

The PG2 adapter utilizes Erlangprocess groupsto implement the PubSub model where a publisher can send messages to many subscribers.


### Global cluster#
Presence is an in-memory key-value store backed by a CRDT. When a user is connected to the cluster the state of that user is sent to all connected Realtime nodes.

Broadcast lets you send a message from any connected client to a Channel. Any other client connected to that same Channel will receive that message.

This works globally. A client connected to a Realtime node in the United States can send a message to another client connected to a node in Singapore. Connect two clients to the same Realtime Channel and they'll all receive the same messages.

Broadcast is useful for getting messages to users in the same location very quickly. If a group of clients are connected to a node in Singapore, the message only needs to go to that Realtime node in Singapore and back down. If users are close to a Realtime node they'll get Broadcast messages in the time it takes to ping the cluster.

Thanks to the Realtime cluster, you (an amazing Supabase user) don't have to think about which regions your clients are connected to.

If you're using Broadcast, Presence, or streaming database changes, messages will always get to your users via the shortest path possible.


### Connecting to a database#
Realtime allows you to listen to changes from your Postgres database. When a new client connects to Realtime and initializes thepostgres_changesRealtime Extension the cluster will connect to your Postgres database and start streaming changes from a replication slot.

Realtime knows the region your database is in, and connects to it from the closest region possible.

Every Realtime region has at least two nodes so if one node goes offline the other node should reconnect and start streaming changes again.


### Broadcast from Postgres#
Realtime Broadcast sends messages when changes happen in your database. Behind the scenes, Realtime creates a publication on therealtime.messagestable. It then reads the Write-Ahead Log (WAL) file for this table, and sends a message whenever an insert happens. Messages are sent as JSON packages over WebSockets.

Therealtime.messagestable is partitioned by day. This allows old messages to be deleted performantly, by dropping old partitions. Partitions are retained for 3 days before being deleted.

Broadcast usesRealtime Authorizationby default to protect your data.


### Streaming the Write-Ahead Log#
A Postgres logical replication slot is acquired when connecting to your database.

Realtime delivers changes by polling the replication slot and appending channel subscription IDs to each wal record.

Subscription IDs are Erlang processes representing underlying sockets on the cluster. These IDs are globally unique and messages to processes are routed automatically by the Erlang virtual machine.

After receiving results from the polling query, with subscription IDs appended, Realtime delivers records to those clients.


================================================================================


# Realtime Authorization
Source: https://supabase.com/docs/guides/realtime/authorization

Realtime Authorization

You can control client access to RealtimeBroadcastandPresenceby adding Row Level Security policies to therealtime.messagestable. Each RLS policy can map to a specific action a client can take:

- Control which clients can broadcast to a Channel
- Control which clients can receive broadcasts from a Channel
- Control which clients can publish their presence to a Channel
- Control which clients can receive messages about the presence of other clients

Realtime Authorization is in Public Beta. To use Authorization for your Realtime Channels, usesupabase-jsversionv2.44.0or later.

To enforce private channels you need to disable the 'Allow public access' setting inRealtime Settings


### How it works#
Realtime uses themessagestable in your database'srealtimeschema to generate access policies for your clients when they connect to a Channel topic.

By creating RLS policies on therealtime.messagestable you can control the access users have to a Channel topic, and features within a Channel topic.

The validation is done when the user connects. When their WebSocket connection is established and a Channel topic is joined, their permissions are calculated based on:

- The RLS policies on therealtime.messagestable
- The user information sent as part of theirAuth JWT
- The request headers
- The Channel topic the user is trying to connect to

When Realtime generates a policy for a client it performs a query on therealtime.messagestable and then rolls it back. Realtime does not store any messages in yourrealtime.messagestable.

Using Realtime Authorization involves two steps:

- In your database, create RLS policies on therealtime.messages
- In your client, instantiate the Realtime Channel with theconfigoptionprivate: true

Increased RLS complexity can impact database performance and connection time, leading to higher connection latency and decreased join rates.


### Accessing request information#

### realtime.topic#
You can use therealtime.topichelper function when writing RLS policies. It returns the Channel topic the user is attempting to connect to.

```javascript
1create policy "authenticated can read all messages on topic"2on "realtime"."messages"3for select4to authenticated5using (6  (select realtime.topic()) = 'room-1'7);
```


### JWT claims#
The user claims can be accessed using thecurrent_settingfunction. The claims are available as a JSON object in therequest.jwt.claimssetting.

```javascript
1create policy "authenticated with supabase.io email can read all"2on "realtime"."messages"3for select4to authenticated5using (6  -- Only users with the email claim ending with @supabase.io7  (((current_setting('request.jwt.claims'))::json ->> 'email') ~~ '%@supabase.io')8);
```


### Examples#
The following examples use this schema:

```javascript
1create table public.rooms (2    id bigint generated by default as identity primary key,3    topic text not null unique4);56alter table public.rooms enable row level security;78create table public.profiles (9  id uuid not null references auth.users on delete cascade,10  email text NOT NULL,1112  primary key (id)13);1415alter table public.profiles enable row level security;1617create table public.rooms_users (18  user_id uuid references auth.users (id),19  room_topic text references public.rooms (topic),20  created_at timestamptz default current_timestamp21);2223alter table public.rooms_users enable row level security;
```


### Broadcast#
Theextensionfield on therealtime.messagestable records the message type. For Broadcast messages, the value ofrealtime.messages.extensionisbroadcast. You can check for this in your RLS policies.


### Allow a user to join (and read) a Broadcast topic#
To join a Broadcast Channel, a user must have at least one read or write permission on the Channel topic.

Here, we allow reads (selects) for users who are linked to the requested topic within the relationship tablepublic.room_users:

```javascript
1create policy "authenticated can receive broadcast"2on "realtime"."messages"3for select4to authenticated5using (6exists (7    select8      user_id9    from10      rooms_users11    where12      user_id = (select auth.uid())13      and topic = (select realtime.topic())14      and realtime.messages.extension in ('broadcast')15  )16);
```

Then, to join a topic with RLS enabled, instantiate the Channel with theprivateoption set totrue.

```javascript
1const  = .('room-1', {2  : { : true },3})456  .('broadcast', { : 'test' }, () => .())7  .((, ) => {8    if ( === 'SUBSCRIBED') {9      .('Connected!')10    } else {11      .()12    }13  })
```


### Allow a user to send a Broadcast message#
To authorize sending Broadcast messages, create a policy forinsertwhere the value ofrealtime.messages.extensionisbroadcast.

Here, we allow writes (sends) for users who are linked to the requested topic within the relationship tablepublic.room_users:

```javascript
1create policy "authenticated can send broadcast on topic"2on "realtime"."messages"3for insert4to authenticated5with check (6  exists (7    select8      user_id9    from10      rooms_users11    where12      user_id = (select auth.uid())13      and topic = (select realtime.topic())14      and realtime.messages.extension in ('broadcast')15  )16);
```


### Presence#
Theextensionfield on therealtime.messagestable records the message type. For Presence messages, the value ofrealtime.messages.extensionispresence. You can check for this in your RLS policies.


### Allow users to listen to Presence messages on a Channel#
Create a policy forselectonrealtime.messageswhererealtime.messages.extensionispresence.

```javascript
1create policy "authenticated can listen to presence in topic"2on "realtime"."messages"3for select4to authenticated5using (6  exists (7    select8      user_id9    from10      rooms_users11    where12      user_id = (select auth.uid())13      and topic = (select realtime.topic())14      and realtime.messages.extension in ('presence')15  )16);
```


### Allow users to send Presence messages on a channel#
To update the Presence status for a user create a policy forinsertonrealtime.messageswhere the value ofrealtime.messages.extensionispresence.

```javascript
1create policy "authenticated can track presence on topic"2on "realtime"."messages"3for insert4to authenticated5with check (6  exists (7    select8      user_id9    from10      rooms_users11    where12      user_id = (select auth.uid())13      and name = (select realtime.topic())14      and realtime.messages.extension in ('presence')15  )16);
```


### Presence and Broadcast#
Authorize both Presence and Broadcast by including both extensions in thewherefilter.


### Broadcast and Presence read#
Authorize Presence and Broadcast read in one RLS policy.

```javascript
1create policy "authenticated can listen to broadcast and presence on topic"2on "realtime"."messages"3for select4to authenticated5using (6  exists (7    select8      user_id9    from10      rooms_users11    where12      user_id = (select auth.uid())13      and topic = (select realtime.topic())14      and realtime.messages.extension in ('broadcast', 'presence')15  )16);
```


### Broadcast and Presence write#
Authorize Presence and Broadcast write in one RLS policy.

```javascript
1create policy "authenticated can send broadcast and presence on topic"2on "realtime"."messages"3for insert4to authenticated5with check (6  exists (7    select8      user_id9    from10      rooms_users11    where12      user_id = (select auth.uid())13      and name = (select realtime.topic())14      and realtime.messages.extension in ('broadcast', 'presence')15  )16);
```


### Interaction with Postgres Changes#
Realtime Postgres Changes are separate from Channel authorization. TheprivateChannel option does not apply to Postgres Changes.

When using Postgres Changes with RLS, database records are sent only to clients who are allowed to read them based on your RLS policies.


### Updating RLS policies#
Client access policies are cached for the duration of the connection. Your database is not queried for every Channel message.

Realtime updates the access policy cache for a client based on your RLS policies when:

- A client connects to Realtime and subscribes to a Channel
- A new JWT is sent to Realtime from a client via theaccess_tokenmessage

If a new JWT is never received on the Channel, the client will be disconnected when the JWT expires.

Make sure to keep the JWT expiration window short.


================================================================================


# Benchmarks
Source: https://supabase.com/docs/guides/realtime/benchmarks

Benchmarks


### Scalability Benchmarks for Supabase Realtime.
Scalability Benchmarks for Supabase Realtime.

This guide explores the scalability of Realtime's features: Broadcast, Presence, and Postgres Changes.


### Methodology#
- The benchmarks are conducted using k6, an open-source load testing tool, against a Realtime Cluster deployed on AWS.
- The cluster configurations use 2-6 nodes, tested in both single-region and multi-region setups, all connected to a single Supabase project.
- The load generators (k6 servers) are deployed on AWS to minimize network latency impact on the results.
- Tests are executed with a full load from the start without warm-up runs.

The metrics collected include: message throughput, latency percentiles, CPU and memory utilization, and connection success rates. Note that performance in production environments may vary based on factors such as network conditions, hardware specifications, and specific usage patterns.


### Workloads#
The proposed workloads are designed to demonstrate Supabase Realtime's throughput and scalability. These benchmarks focus on core functionality and common usage patterns. The benchmarking results include the following workloads:


### Results#

### Broadcast: Using WebSockets#
This workload evaluates the system's capacity to handle multiple concurrent WebSocket connections and sending Broadcast messages via the WebSocket. Each virtual user (VU) in the test:

- Establishes and maintains a WebSocket connection
- Joins two distinct channels:An echo channel (1 user per channel) for direct message reflectionA broadcast channel (6 users per channel) for group communication
- Generates traffic by sending 2 messages per second to each joined channel for 10 minutes

- An echo channel (1 user per channel) for direct message reflection
- A broadcast channel (6 users per channel) for group communication




### Broadcast: Using the database#
This workload evaluates the system's capacity to send Broadcast messages from the database using therealtime.broadcast_changesfunction. Each virtual user (VU) in the test:

- Establishes and maintains a WebSocket connection
- Joins a distinct channel:A single channel (100 users per channel) for group communication
- Database has a trigger set to runrealtime.broadcast_changeson every insert
- Database triggers 10_000 inserts per second

- A single channel (100 users per channel) for group communication




### Broadcast: Impact of payload size#
This workload tests the system's performance with different message payload sizes to understand how data volume affects throughput and latency. Each virtual user (VU) follows the same connection pattern as the broadcast test, but with varying message sizes:

- Establishes and maintains a WebSocket connection
- Joins two distinct channels:An echo channel (1 user per channel) for direct message reflectionA broadcast channel (6 users per channel) for group communication
- Sends messages with payloads of 1KB, 10KB, and 50KB
- Generates traffic by sending 2 messages per second to each joined channel for 5 minutes

- An echo channel (1 user per channel) for direct message reflection
- A broadcast channel (6 users per channel) for group communication


### 1KB payload#



### 10KB payload#



### 50KB payload#


Note: The final column shows results with reduced load (2,000 users) for the 50KB payload test, demonstrating how the system performs with larger payloads under different concurrency levels.


### Broadcast: Scalability scenarios#
This workload demonstrates Realtime's capability to handle high-scale scenarios with a large number of concurrent users and broadcast channels. The test simulates a scenario where each user participates in group communications with periodic message broadcasts. Each virtual user (VU):

- Establishes and maintains a WebSocket connection (30-120 minutes)
- Joins 2 broadcast channels
- Sends 1 message per minute to each joined channel
- Each message is broadcast to 100 other users




### Realtime Auth#
This workload demonstrates Realtime's capability to handle large amounts of new connections per second and channel joins per second with Authentication Row Level Security (RLS) enabled for these channels. The test simulates a scenario where large volumes of users connect to realtime and participate in auth protected communications. Each virtual user (VU):

- Establishes and maintains a WebSocket connection (2.5 minutes)
- Joins 2 broadcast channels
- Sends 1 message per minute to each joined channel
- Each message is broadcast to 100 other users




### Postgres Changes#
Realtime systems usually require forethought because of their scaling dynamics. For thePostgres Changesfeature, every change event must be checked to see if the subscribed user has access. For instance, if you have 100 users subscribed to a table where you make a single insert, it will then trigger 100 "reads": one for each user.

There can be a database bottleneck which limits message throughput. If your database cannot authorize the changes rapidly enough, the changes will be delayed until you receive a timeout.

Database changes are processed on a single thread to maintain the change order. That means compute upgrades don't have a large effect on the performance of Postgres change subscriptions. You can estimate the expected maximum throughput for your database below.

If you are using Postgres Changes at scale, you should consider using a separate "public" table without RLS and filters. Alternatively, you can use Realtime server-side only and then re-stream the changes to your clients using a Realtime Broadcast.

Enter your database settings to estimate the maximum throughput for your instance:

Don't forget to run your own benchmarks to make sure that the performance is acceptable for your use case.

Supabase continues to make improvements to Realtime's Postgres Changes. If you are uncertain about your use case performance, reach out using theSupport Form. The support team can advise on the best solution for each use-case.


================================================================================


# Broadcast
Source: https://supabase.com/docs/guides/realtime/broadcast

Broadcast


### Send low-latency messages using the client libs, REST, or your Database.
Send low-latency messages using the client libs, REST, or your Database.

You can use Realtime Broadcast to send low-latency messages between users. Messages can be sent using the client libraries, REST APIs, or directly from your database.


### How Broadcast works#
The way Broadcast works changes based on the channel you are using:

- From REST API will receive an HTTP request which then will be sent via WebSocket to connected clients
- From Client libraries we have an established WebSocket connection and we use that to send a message to the server which then will be sent via WebSocket to connected clients
- From Database we add a new entry torealtime.messageswhere we have logical replication set to listen for changes which then will be sent via WebSocket to connected clients

The public flag (the last argument inrealtime.send(payload, event, topic, is_private))only affects who can subscribe to the topic not who can read messages from the database.

- Public (false) → Anyone can subscribe to that topic without authentication
- Private (true) → Only authenticated clients can subscribe to that topic

However, regardless of whether it's public or private, the Realtime service connects to your database as the authenticated Supabase Admin role.

For Authorization we insert a message and try to read it, and rollback the transaction to verify that the RLS policies set by the user are being respected by the user joining the channel, but this message isn't sent to the user. You can read more about it in theAuthorization docs.


### Subscribe to messages#
You can use the Supabase client libraries to receive Broadcast messages.


### Initialize the client#
Get the Project URL and key fromthe project'sConnectdialog.

Supabase is changing the way keys work to improve project security and developer experience. You canread the full announcement, but in the transition period, you can use both the currentanonandservice_rolekeys and the new publishable key with the formsb_publishable_xxxwhich will replace the older keys.

In most cases, you can get the correct key fromthe Project'sConnectdialog, but if you want a specific key, you can find all keys inthe API Keys section of a Project's Settings page:

- For legacy keys, copy theanonkey for client-side operations and theservice_rolekey for server-side operations from theLegacy API Keystab.
- For new keys, open theAPI Keystab, if you don't have a publishable key already, clickCreate new API Keys, and copy the value from thePublishable keysection.

```javascript
1import {  } from '@supabase/supabase-js'23const  = 'https://<project>.supabase.co'4const  = '<sb_publishable_... or anon key>'56const  = (, )
```


### Receiving Broadcast messages#
You can provide a callback for thebroadcastchannel to receive messages. This example will receive anybroadcastmessages that are sent totest-channel:

```javascript
1// Join a room/topic. Can be anything except for 'realtime'.2const  = .('test-channel')34// Simple function to log any messages we receive5function () {6  .()7}89// Subscribe to the Channel1011  .(12    'broadcast',13    { : 'shout' }, // Listen for "shout". Can be "*" to listen to all events14    () => ()15  )16  .()
```


### Send messages#

### Broadcast using the client libraries#
You can use the Supabase client libraries to send Broadcast messages.

```javascript
1const  = .('test-channel')23/**4 * Sending a message before subscribing will use HTTP5 */67  .({8    : 'broadcast',9    : 'shout',10    : { : 'Hi' },11  })12  .(() => .())131415/**16 * Sending a message after subscribing will use Websockets17 */18.(() => {19  if ( !== 'SUBSCRIBED') {20    return null21  }2223  .({24    : 'broadcast',25    : 'shout',26    : { : 'Hi' },27  })28})
```


### Broadcast from the Database#
This feature is in Public Beta.Submit a support ticketif you have any issues.

All the messages sent using Broadcast from the Database are stored inrealtime.messagestable and will be deleted after 3 days.

You can send messages directly from your database using therealtime.send()function:

```javascript
1select2  realtime.send(3    jsonb_build_object('hello', 'world'), -- JSONB Payload4    'event', -- Event name5    'topic', -- Topic6    false -- Public / Private flag7  );
```

The realtime.send function in the database includes a flag that determines whether the broadcast is private or public, and client channels also have the same configuration. For broadcasts to work correctly, these settings must match a public broadcast will only reach public channels, and a private broadcast will only reach private ones.

By default, all database broadcasts are private, meaning clients must authenticate to receive them. If the database sends a public message but the client subscribes to a private channel, the message won't be delivered since private channels only accept signed, authenticated messages.

It's a common use case to broadcast messages when a record is created, updated, or deleted. We provide a helper function specific to this use case,realtime.broadcast_changes(). For more details, check out theSubscribing to Database Changesguide.


### Broadcast using the REST API#
You can send a Broadcast message by making an HTTP request to Realtime servers.

```javascript
1curl -v \2-H 'apikey: <SUPABASE_TOKEN>' \3-H 'Content-Type: application/json' \4--data-raw '{5  "messages": [6    {7      "topic": "test",8      "event": "event",9      "payload": { "test": "test" }10    }11  ]12}' \13'https://<PROJECT_REF>.supabase.co/realtime/v1/api/broadcast'
```


### Broadcast options#
You can pass configuration options while initializing the Supabase Client.


### Self-send messages#
By default, broadcast messages are only sent to other clients. You can broadcast messages back to the sender by setting Broadcast'sselfparameter totrue.

```javascript
1const myChannel = supabase.channel('room-2', {2  config: {3    broadcast: { self: true },4  },5})67myChannel.on(8  'broadcast',9  { event: 'test-my-messages' },10  (payload) => console.log(payload)11)1213myChannel.subscribe((status) => {14  if (status !== 'SUBSCRIBED') { return }15  myChannel.send({16    type: 'broadcast',17    event: 'test-my-messages',18    payload: { message: 'talking to myself' },19  })20})
```


### Acknowledge messages#
You can confirm that the Realtime servers have received your message by setting Broadcast'sackconfig totrue.

```javascript
1const  = .('room-3', {2  : {3    : { : true },4  },5})67.(async () => {8  if ( !== 'SUBSCRIBED') { return }910  const  = await .({11    : 'broadcast',12    : 'acknowledge',13    : {},14  })1516  .('serverResponse', )17})
```

Use this to guarantee that the server has received the message before resolvingchannelD.send's promise. If theackconfig is not set totruewhen creating the channel, the promise returned bychannelD.sendwill resolve immediately.


### Send messages using REST calls#
You can also send a Broadcast message by making an HTTP request to Realtime servers. This is useful when you want to send messages from your server or client without having to first establish a WebSocket connection.

This is currently available only in the Supabase JavaScript client version 2.37.0 and later.

```javascript
1const channel = supabase.channel('test-channel')23// No need to subscribe to channel45channel6  .send({7    type: 'broadcast',8    event: 'test',9    payload: { message: 'Hi' },10  })11  .then((resp) => console.log(resp))1213// Remember to clean up the channel1415supabase.removeChannel(channel)
```


### Trigger broadcast messages from your database#

### How it works#
Broadcast Changes allows you to trigger messages from your database. To achieve it Realtime is directly reading your WAL (Write Append Log) file using a publication against therealtime.messagestable so whenever a new insert happens a message is sent to connected users.

It uses partitioned tables per day which allows the deletion your previous messages in a performant way by dropping the physical tables of this partitioned table. Tables older than 3 days old are deleted.

Broadcasting from the database works like a client-side broadcast, using WebSockets to send JSON packages.Realtime Authorizationis required and enabled by default to protect your data.

The database broadcast feature provides two functions to help you send messages:

- realtime.sendwill insert a message into realtime.messages without a specific format.
- realtime.broadcast_changeswill insert a message with the required fields to emit database changes to clients. This helps you set up triggers on your tables to emit changes.


### Broadcasting a message from your database#
Therealtime.sendfunction provides the most flexibility by allowing you to broadcast messages from your database without a specific format. This allows you to use database broadcast for messages that aren't necessarily tied to the shape of a Postgres row change.

```javascript
1SELECT realtime.send (2	'{}'::jsonb, -- JSONB Payload3	'event', -- Event name4	'topic', -- Topic5	FALSE -- Public / Private flag6);
```


### Broadcast record changes#

### Setup realtime authorization#
Realtime Authorization is required and enabled by default. To allow your users to listen to messages from topics, create a RLS (Row Level Security) policy:

```javascript
1CREATE POLICY "authenticated can receive broadcasts"2ON "realtime"."messages"3FOR SELECT4TO authenticated5USING ( true );
```

See theRealtime Authorizationdocs to learn how to set up more specific policies.


### Set up trigger function#
First, set up a trigger function that usesrealtime.broadcast_changesto insert an event whenever it is triggered. The event is set up to include data on the schema, table, operation, and field changes that triggered it.

For this example use case, we want to have a topic with the nametopic:<record id>to which we're going to broadcast events.

```javascript
1CREATE OR REPLACE FUNCTION public.your_table_changes()2RETURNS trigger3SECURITY DEFINER SET search_path = ''4AS $$5BEGIN6    PERFORM realtime.broadcast_changes(7	    'topic:' || NEW.id::text,   -- topic8		   TG_OP,                          -- event9		   TG_OP,                          -- operation10		   TG_TABLE_NAME,                  -- table11		   TG_TABLE_SCHEMA,                -- schema12		   NEW,                            -- new record13		   OLD                             -- old record14		);15    RETURN NULL;16END;17$$ LANGUAGE plpgsql;
```

Of note are the Postgres native trigger special variables used:

- TG_OP- the operation that triggered the function
- TG_TABLE_NAME- the table that caused the trigger
- TG_TABLE_SCHEMA- the schema of the table that caused the trigger invocation
- NEW- the record after the change
- OLD- the record before the change

You can read more about them in thisguide.


### Set up trigger#
Next, set up a trigger so the function runs whenever your target table has a change.

```javascript
1CREATE TRIGGER broadcast_changes_for_your_table_trigger2AFTER INSERT OR UPDATE OR DELETE ON public.your_table3FOR EACH ROW4EXECUTE FUNCTION your_table_changes ();
```

As you can see, it will be broadcasting all operations so our users will receive events when records are inserted, updated or deleted frompublic.your_table.


### Listen on client side#
Finally, client side will requires to be set up to listen to the topictopic:<record id>to receive the events.

```javascript
1const gameId = 'id'2await supabase.realtime.setAuth() // Needed for Realtime Authorization3const changes = supabase4  .channel(`topic:${gameId}`)5  .on('broadcast', { event: 'INSERT' }, (payload) => console.log(payload))6  .on('broadcast', { event: 'UPDATE' }, (payload) => console.log(payload))7  .on('broadcast', { event: 'DELETE' }, (payload) => console.log(payload))8  .subscribe()
```


### Broadcast replay#
This feature is currently in Public Alpha. If you have any issuessubmit a support ticket.


### How it works#
Broadcast Replay enablesprivatechannels to access messages that were sent earlier. Only messages published viaBroadcast From the Databaseare available for replay.

You can configure replay with the following options:

- since(Required): The epoch timestamp in milliseconds (e.g.,1697472000000), specifying the earliest point from which messages should be retrieved.
- limit(Optional): The number of messages to return. This must be a positive integer, with a maximum value of 25.

This is currently available only in the Supabase JavaScript client version 2.74.0 and later.

```javascript
1const config = {2  private: true,3  broadcast: {4    replay: {5      since: 1697472000000, // Unix timestamp in milliseconds6      limit: 107    }8  }9}10const channel = supabase.channel('main:room', { config })1112// Broadcast callback receives meta field13channel.on('broadcast', { event: 'position' }, (payload) => {14  if (payload?.meta?.replayed) {15    console.log('Replayed message: ', payload)16  } else {17    console.log('This is a new message', payload)18  }19  // ...20})21.subscribe()
```


### When to use Broadcast replay#
A few common use cases for Broadcast Replay include:

- Displaying the most recent messages from a chat room
- Loading the last events that happened during a sports event
- Ensuring users always see the latest events after a page reload or network interruption
- Highlighting the most recent sections that changed in a web page


================================================================================


# Realtime Concepts
Source: https://supabase.com/docs/guides/realtime/concepts

Realtime Concepts


### Concepts#
There are several concepts and terminology that is useful to understand how Realtime works.

- Channels: the foundation of Realtime. Think of them as rooms where clients can communicate and listen to events. Channels are identified by a topic name and if they are public or private.
- Topics: the name of the channel. They are used to identify the channel and are a string used to identify the channel.
- Events: the type of messages that can be sent and received.
- Payload: the actual data that is sent and received and that the user will act upon.
- Concurrent Connections: number of total channels subscribed for all clients.


### Channels#
Channels are the foundation of Realtime. Think of them as rooms where clients can communicate and listen to events. Channels are identified by a topic name and if they are public or private.

For private channels, you need to useRealtime Authorizationto control access to the channel and if they are able to send messages.
For public channels, any user can subscribe to the channel, send and receive messages.

You can set your project to use only private channels or both private and public channels in theRealtime Settings.

If you have a private channel and a public channel with the same topic name, Realtime sees them as unique channels and won't send messages between them.


### Database resources#
Realtime uses several database connections to perform several operations. As a user, you are able to tune some of them usingRealtime Settings.


### Database connections#
Realtime uses several database connections to perform various operations. You can configure some of these connections throughRealtime Settings.

The connections include:

- Migrations: Two temporary connections to run database migrations when needed
- Authorization: Configurable connection pool to check authorization policies on join that are always started.
- Broadcast from database: One connection to receive data from replication slot used to broadcast the changes to the clients that is always started.
- Postgres Changes: Multiple connection pools required. These pools are only started if you use Postgres Changes.Subscription management: To manage the subscribers to Postgres ChangesSubscription cleanup: To cleanup the subscribers to Postgres ChangesWAL pull: To pull the changes from the database

- Subscription management: To manage the subscribers to Postgres Changes
- Subscription cleanup: To cleanup the subscribers to Postgres Changes
- WAL pull: To pull the changes from the database

The number of connections varies based on your compute add-on size and configuration. The following table shows the default connection pool sizes for different compute add-on variants:

You can customizeAuthorization Pool Sizethrough theDatabase connection pool sizeparameter in your Realtime configuration. If not specified, the default values shown in the table will be used.


### Replication slots#
Realtime also uses, at maximum, 2 replication slots.

- Broadcast from database: To broadcast the changes from the database to the clients
- Postgres Changes: To listen to changes from the database


### Schema and tables#
Therealtimeschema creates the following tables:

- schema_migrations- To track the migrations that have been run on the database from Realtime
- subscription- Track the subscribers to Postgres Changes
- messages- Partitioned table per day that's used for Authorization and Broadcast from databaseAuthorization: To check the authorization policies on join by checking if a given user can read and write to this tableBroadcast from database: Replication slot tracks a publication to this table to broadcast the changes to the connected clients.The schema from the table is the following:1createtablerealtime.messages (2topictextnot null,-- The topic of the message3extensiontextnot null,-- The extension of the message (presence, broadcast)4payload jsonbnull,-- The payload of the message5eventtextnull,-- The event of the message6privatebooleannulldefaultfalse,-- If the message is going to use a private channel7updated_attimestamp without time zonenot nulldefaultnow(),-- The timestamp of the message8inserted_attimestamp without time zonenot nulldefaultnow(),-- The timestamp of the message9id uuidnot nulldefaultgen_random_uuid(),-- The id of the message10constraintmessages_pkeyprimary key(id, inserted_at))partitionbyRANGE(inserted_at);

- Authorization: To check the authorization policies on join by checking if a given user can read and write to this table
- Broadcast from database: Replication slot tracks a publication to this table to broadcast the changes to the connected clients.
- The schema from the table is the following:1createtablerealtime.messages (2topictextnot null,-- The topic of the message3extensiontextnot null,-- The extension of the message (presence, broadcast)4payload jsonbnull,-- The payload of the message5eventtextnull,-- The event of the message6privatebooleannulldefaultfalse,-- If the message is going to use a private channel7updated_attimestamp without time zonenot nulldefaultnow(),-- The timestamp of the message8inserted_attimestamp without time zonenot nulldefaultnow(),-- The timestamp of the message9id uuidnot nulldefaultgen_random_uuid(),-- The id of the message10constraintmessages_pkeyprimary key(id, inserted_at))partitionbyRANGE(inserted_at);

```javascript
1create table realtime.messages (2topic text not null, -- The topic of the message3extension text not null, -- The extension of the message (presence, broadcast)4payload jsonb null, -- The payload of the message5event text null, -- The event of the message6private boolean null default false, -- If the message is going to use a private channel7updated_at timestamp without time zone not null default now(), -- The timestamp of the message8inserted_at timestamp without time zone not null default now(), -- The timestamp of the message9id uuid not null default gen_random_uuid (), -- The id of the message10constraint messages_pkey primary key (id, inserted_at)) partition by RANGE (inserted_at);
```

Realtime has a cleanup process that will delete tables older than 3 days.


### Functions#
Realtime creates two functions on your database:

- realtime.send- Inserts an entry intorealtime.messagestable that will trigger the replication slot to broadcast the changes to the clients. It also captures errors to prevent the trigger from breaking.
- realtime.broadcast_changes- usesrealtime.sendto broadcast the changes with a format that is compatible with Postgres Changes


================================================================================


# Operational Error Codes
Source: https://supabase.com/docs/guides/realtime/error_codes

Operational Error Codes


### List of operational codes to help understand your deployment and usage.
List of operational codes to help understand your deployment and usage.

The number of channels you can create has reached its limit.

The rate of joins per second from your clients has reached the channel limits.

Database is initializing connection.

The number of connected clients has reached its limit.

Database had connection issues and connection was not able to be established.

Realtime was not able to connect to the tenant's database due to not having enough available connections.

Learn more:

- Connection management guide

Verify your database connection limits.

Error when trying to authorize the WebSocket connection.

Verify user information on connect.

Error when trying to connect to the WebSocket server.

Verify user information on connect.

Error executing a database transaction in tenant database.

Error when calling another realtime node.

Error when starting the Postgres CDC extension which is used for Postgres Changes.

Error when starting the Postgres CDC stream which is used for Postgres Changes.

The number of connections you have set for Realtime are not enough to handle your current use case.

Connection against Tenant database is still starting.

JWT exp claim value it's incorrect.

Scheduled task for realtime.message cleanup was unable to run.

JWT signature was not able to be validated.

Token received does not comply with the JWT format.

Check to see if we require to run migrations fails.

Error when running the migrations against the Tenant database that are required by Realtime.

Error when creating partitions for realtime.messages.

Error when pooling the replication slot.

Error when preparing the replication slot.

The configuration provided to Realtime on connect will not be able to provide you any Postgres Changes.

Verify your configuration on channel startup as you might not have your tables properly registered.

Realtime has been disabled for the tenant.

Realtime is a distributed application and this means that one the system is unable to communicate with one of the distributed nodes.

Realtime is currently restarting.

Postgres changes still waiting to be subscribed.

Maximum number of WAL senders reached in tenant database.

Learn more:

- Configuring max WAL senders

The replication slot is being used by another transaction.

Error on RLS policy used for authorization.

Error when starting the replication and listening of errors for database broadcasting.

Error when trying to delete a subscription for postgres changes.

Our framework to syncronize processes has failed to properly startup a connection to the database.

The table you are trying to listen to has spaces in its name which we are unable to support.

Change the table name to not have spaces in it.

The tenant you are trying to connect to does not exist.

Verify the tenant name you are trying to connect to exists in the realtime.tenants table.

RPC request within the Realtime server has timed out.

You are trying to use Realtime without a topic name set.

Error when trying to checkout a connection from the tenant pool.

Error when trying to check the processes on a remote node.

Unable to connect to Project database.

Realtime was not able to connect to the tenant's database.

Error when trying to create a counter to track rate limits for a tenant.

Error when trying to decrement a counter to track rate limits for a tenant.

Error when trying to delete subscriptions that are no longer being used.

Error when trying to delete a tenant.

An error were we are not handling correctly the response to be sent to the end user.

Error when trying to find a counter to track rate limits for a tenant.

Error when trying to increment a counter to track rate limits for a tenant.

Unable to LISTEN for notifications against the Tenant Database.

Payload sent in NOTIFY operation was JSON parsable.

Error when setting up Authorization Policies.

Error when trying to subscribe to Postgres changes.

Error when handling track presence for this socket.

Error when trying to update a counter to track rate limits for a tenant.

Unauthorized access to Realtime channel.

Unhandled message received by a Realtime process.

An unknown data type was processed by the Realtime system.

An error we are not handling correctly was triggered on a channel.

An error we are not handling correctly was triggered on a controller.

Presence event type not recognized by service.

Received a HTTP request with a body that was not able to be processed by the endpoint.


================================================================================


# Getting Started with Realtime
Source: https://supabase.com/docs/guides/realtime/getting_started

Getting Started with Realtime


### Learn how to build real-time applications with Supabase Realtime
Learn how to build real-time applications with Supabase Realtime


### Quick start#

### 1. Install the client library#
```javascript
1npm install @supabase/supabase-js
```


### 2. Initialize the client#
Get your project URL and key.


### Get API details#
Now that you've created some database tables, you are ready to insert data using the auto-generated API.

To do this, you need to get the Project URL and key fromthe projectConnectdialog.

Supabase is changing the way keys work to improve project security and developer experience. You canread the full announcement, but in the transition period, you can use both the currentanonandservice_rolekeys and the new publishable key with the formsb_publishable_xxxwhich will replace the older keys.

In most cases, you can get the correct key fromthe Project'sConnectdialog, but if you want a specific key, you can find all keys inthe API Keys section of a Project's Settings page:

- For legacy keys, copy theanonkey for client-side operations and theservice_rolekey for server-side operations from theLegacy API Keystab.
- For new keys, open theAPI Keystab, if you don't have a publishable key already, clickCreate new API Keys, and copy the value from thePublishable keysection.

Read the API keys docsfor a full explanation of all key types and their uses.

```javascript
1import {  } from '@supabase/supabase-js'23const  = ('https://<project>.supabase.co', '<anon_key or sb_publishable_key>')
```


### 3. Create your first Channel#
Channels are the foundation of Realtime. Think of them as rooms where clients can communicate. Each channel is identified by a topic name and if they are public or private.

```javascript
1// Create a channel with a descriptive topic name2const channel = supabase.channel('room:lobby:messages', {3  config: { private: true }, // Recommended for production4})
```


### 4. Set up authorization#
Since we're using a private channel, you need to create a basic RLS policy on therealtime.messagestable to allow authenticated users to connect. Row Level Security (RLS) policies control who can access your Realtime channels based on user authentication and custom rules:

```javascript
1-- Allow authenticated users to receive broadcasts2CREATE POLICY "authenticated_users_can_receive" ON realtime.messages3  FOR SELECT TO authenticated USING (true);45-- Allow authenticated users to send broadcasts6CREATE POLICY "authenticated_users_can_send" ON realtime.messages7  FOR INSERT TO authenticated WITH CHECK (true);
```


### 5. Send and receive messages#
There are three main ways to send messages with Realtime:


### 5.1 using client libraries#
Send and receive messages using the Supabase client:

```javascript
1// Listen for messages2channel3  .on('broadcast', { event: 'message_sent' }, (payload: { payload: any }) => {4    console.log('New message:', payload.payload)5  })6  .subscribe()78// Send a message9channel.send({10  type: 'broadcast',11  event: 'message_sent',12  payload: {13    text: 'Hello, world!',14    user: 'john_doe',15    timestamp: new Date().toISOString(),16  },17})
```


### 5.2 using HTTP/REST API#
Send messages via HTTP requests, perfect for server-side applications:

```javascript
1// Send message via REST API2const  = await (`https://<project>.supabase.co/rest/v1/rpc/broadcast`, {3  : 'POST',4  : {5    'Content-Type': 'application/json',6    : `Bearer <your-service-role-key>`,7    : '<your-service-role-key>',8  },9  : .({10    : 'room:lobby:messages',11    : 'message_sent',12    : {13      : 'Hello from server!',14      : 'system',15      : new ().(),16    },17    : true,18  }),19})
```


### 5.3 using database triggers#
Automatically broadcast database changes using triggers. Choose the approach that best fits your needs:

Usingrealtime.broadcast_changes(Best for mirroring database changes)

```javascript
1-- Create a trigger function for broadcasting database changes2CREATE OR REPLACE FUNCTION broadcast_message_changes()3RETURNS TRIGGER AS $$4BEGIN5  -- Broadcast to room-specific channel6  PERFORM realtime.broadcast_changes(7    'room:' || NEW.room_id::text || ':messages',8    TG_OP,9    TG_OP,10    TG_TABLE_NAME,11    TG_TABLE_SCHEMA,12    NEW,13    OLD14  );15  RETURN NULL;16END;17$$ LANGUAGE plpgsql SECURITY DEFINER;1819-- Apply trigger to your messages table20CREATE TRIGGER messages_broadcast_trigger21  AFTER INSERT OR UPDATE OR DELETE ON messages22  FOR EACH ROW EXECUTE FUNCTION broadcast_message_changes();
```

Usingrealtime.send(Best for custom notifications and filtered data)

```javascript
1-- Create a trigger function for custom notifications2CREATE OR REPLACE FUNCTION notify_message_activity()3RETURNS TRIGGER AS $$4BEGIN5  -- Send custom notification when new message is created6  IF TG_OP = 'INSERT' THEN7    PERFORM realtime.send(8      jsonb_build_object(9        'message_id', NEW.id,10        'user_id', NEW.user_id,11        'room_id', NEW.room_id,12        'created_at', NEW.created_at13      ),14      'message_created',15      'room:' || NEW.room_id::text || ':notifications',16      true  -- private channel17    );18  END IF;1920  RETURN NULL;21END;22$$ LANGUAGE plpgsql SECURITY DEFINER;2324-- Apply trigger to your messages table25CREATE TRIGGER messages_notification_trigger26  AFTER INSERT ON messages27  FOR EACH ROW EXECUTE FUNCTION notify_message_activity();
```

- realtime.broadcast_changessends the full database change with metadata
- realtime.sendallows you to send custom payloads and control exactly what data is broadcast


### Essential best practices#

### Use private channels#
Always use private channels for production applications to ensure proper security and authorization:

```javascript
1const channel = supabase.channel('room:123:messages', {2  config: { private: true },3})
```


### Follow naming conventions#
Channel Topics:Use the patternscope:id:entity

- room:123:messages- Messages in room 123
- game:456:moves- Game moves for game 456
- user:789:notifications- Notifications for user 789


### Clean up subscriptions#
Always unsubscribe when you are done with a channel to ensure you free up resources:

```javascript
1// React example2import { useEffect } from 'react'34useEffect(() => {5  const channel = supabase.channel('room:123:messages')67  return () => {8    supabase.removeChannel(channel)9  }10}, [])
```


### Choose the right feature#

### When to use Broadcast#
- Real-time messaging and notifications
- Custom events and game state
- Database change notifications (with triggers)
- High-frequency updates (e.g. Cursor tracking)
- Most use cases


### When to use Presence#
- User online/offline status
- Active user counters
- Use minimally due to computational overhead


### When to use Postgres Changes#
- Quick testing and development
- Low amount of connected users


### Next steps#
Now that you understand the basics, dive deeper into each feature:


### Core features#
- Broadcast- Learn about sending messages, database triggers, and REST API usage
- Presence- Implement user state tracking and online indicators
- Postgres Changes- Understanding database change listeners (consider migrating to Broadcast)


### Security & configuration#
- Authorization- Set up RLS policies for private channels
- Settings- Configure your Realtime instance for optimal performance


### Advanced topics#
- Architecture- Understand how Realtime works under the hood
- Benchmarks- Performance characteristics and scaling considerations
- Limits- Usage limits and best practices


### Integration guides#
- Realtime with Next.js- Build real-time Next.js applications
- User Presence- Implement user presence features
- Database Changes- Listen to database changes


### Framework examples#
- Flutter Integration- Build real-time Flutter applications

Ready to build something amazing? Start with theBroadcast guideto create your first real-time feature!


================================================================================


# Realtime Limits
Source: https://supabase.com/docs/guides/realtime/limits

Realtime Limits

Our cluster supports millions of concurrent connections and message throughput for production workloads.

Upgrade your plan to increase your limits. Without a spend cap, or on an Enterprise plan, some limits are still in place to protect budgets. All limits are configurable per project.Contact supportif you need your limits increased.


### Limits by plan#
Beyond the Free and Pro Plan you can customize your limits bycontacting support.


### Limit errors#
When you exceed a limit, errors will appear in the backend logs and client-side messages in the WebSocket connection.

- Logs: check theRealtime logsinside your project Dashboard.
- WebSocket errors: Use your browser's developer tools to find the WebSocket initiation request and view individual messages.

You can use theRealtime Inspectorto reproduce an error and share those connection details with Supabase support.

Some limits can cause a Channel join to be refused. Realtime will reply with one of the following WebSocket messages:


### too_many_channels#
Too many channels currently joined for a single connection.


### too_many_connections#
Too many total concurrent connections for a project.


### too_many_joins#
Too many Channel joins per second.


### tenant_events#
Connections will be disconnected if your project is generating too many messages per second.supabase-jswill reconnect automatically when the message throughput decreases below your plan limit. Aneventis a WebSocket message delivered to, or sent from a client.


### Postgres changes payload limit#
When this limit is reached, thenewandoldrecord payloads only include the fields with a value size of less than or equal to 64 bytes.


================================================================================


# Postgres Changes
Source: https://supabase.com/docs/guides/realtime/postgres-changes

Postgres Changes


### Listen to Postgres changes using Supabase Realtime.
Listen to Postgres changes using Supabase Realtime.

Let's explore how to use Realtime's Postgres Changes feature to listen to database events.


### Quick start#
In this example we'll set up a database table, secure it with Row Level Security, and subscribe to all changes using the Supabase client libraries.


### Set up a Supabase project with a 'todos' table
Create a new projectin the Supabase Dashboard.

After your project is ready, create a table in your Supabase database. You can do this with either the Table interface or theSQL Editor.

```javascript
1-- Create a table called "todos"2-- with a column to store tasks.3create table todos (4  id serial primary key,5  task text6);
```


### Allow anonymous access
In this example we'll turn onRow Level Securityfor this table and allow anonymous access. In production, be sure to secure your application with the appropriate permissions.

```javascript
1-- Turn on security2alter table "todos"3enable row level security;45-- Allow anonymous access6create policy "Allow anonymous access"7on todos8for select9to anon10using (true);
```


### Enable Postgres replication
Go to your project'sPublications settings, and undersupabase_realtime, toggle on the tables you want to listen to.

Alternatively, add tables to thesupabase_realtimepublication by running the given SQL:

```javascript
1alter publication supabase_realtime2add table your_table_name;
```


### Install the client
Install the Supabase JavaScript client.

```javascript
1npm install @supabase/supabase-js
```


### Create the client
This client will be used to listen to Postgres changes.

```javascript
1import {  } from '@supabase/supabase-js'23const  = (4  'https://<project>.supabase.co',5  '<sb_publishable_... or anon key>'6)
```


### Listen to changes by schema
Listen to changes on all tables in thepublicschema by setting theschemaproperty to 'public' and event name to*. The event name can be one of:

- INSERT
- UPDATE
- DELETE
- *

The channel name can be any string except 'realtime'.

```javascript
1const  = 2  .('schema-db-changes')3  .(4    'postgres_changes',5    {6      : '*',7      : 'public',8    },9    () => .()10  )11  .()
```


### Insert dummy data
Now we can add some data to our table which will trigger thechannelAevent handler.

```javascript
1insert into todos (task)2values3  ('Change!');
```


### Usage#
You can use the Supabase client libraries to subscribe to database changes.


### Listening to specific schemas#
Subscribe to specific schema events using theschemaparameter:

```javascript
1const changes = supabase2  .channel('schema-db-changes')3  .on(4    'postgres_changes',5    {6      schema: 'public', // Subscribes to the "public" schema in Postgres7      event: '*',       // Listen to all changes8    },9    (payload) => console.log(payload)10  )11  .subscribe()
```

The channel name can be any string except 'realtime'.


### Listening toINSERTevents#
Use theeventparameter to listen only to databaseINSERTs:

```javascript
1const changes = supabase2  .channel('schema-db-changes')3  .on(4    'postgres_changes',5    {6      event: 'INSERT', // Listen only to INSERTs7      schema: 'public',8    },9    (payload) => console.log(payload)10  )11  .subscribe()
```

The channel name can be any string except 'realtime'.


### Listening toUPDATEevents#
Use theeventparameter to listen only to databaseUPDATEs:

```javascript
1const changes = supabase2  .channel('schema-db-changes')3  .on(4    'postgres_changes',5    {6      event: 'UPDATE', // Listen only to UPDATEs7      schema: 'public',8    },9    (payload) => console.log(payload)10  )11  .subscribe()
```

The channel name can be any string except 'realtime'.


### Listening toDELETEevents#
Use theeventparameter to listen only to databaseDELETEs:

```javascript
1const changes = supabase2  .channel('schema-db-changes')3  .on(4    'postgres_changes',5    {6      event: 'DELETE', // Listen only to DELETEs7      schema: 'public',8    },9    (payload) => console.log(payload)10  )11  .subscribe()
```

The channel name can be any string except 'realtime'.


### Listening to specific tables#
Subscribe to specific table events using thetableparameter:

```javascript
1const changes = supabase2  .channel('table-db-changes')3  .on(4    'postgres_changes',5    {6      event: '*',7      schema: 'public',8      table: 'todos',9    },10    (payload) => console.log(payload)11  )12  .subscribe()
```

The channel name can be any string except 'realtime'.


### Listening to multiple changes#
To listen to different events and schema/tables/filters combinations with the same channel:

```javascript
1const channel = supabase2  .channel('db-changes')3  .on(4    'postgres_changes',5    {6      event: '*',7      schema: 'public',8      table: 'messages',9    },10    (payload) => console.log(payload)11  )12  .on(13    'postgres_changes',14    {15      event: 'INSERT',16      schema: 'public',17      table: 'users',18    },19    (payload) => console.log(payload)20  )21  .subscribe()
```


### Filtering for specific changes#
Use thefilterparameter for granular changes:

```javascript
1const changes = supabase2  .channel('table-filter-changes')3  .on(4    'postgres_changes',5    {6      event: 'INSERT',7      schema: 'public',8      table: 'todos',9      filter: 'id=eq.1',10    },11    (payload) => console.log(payload)12  )13  .subscribe()
```


### Available filters#
Realtime offers filters so you can specify the data your client receives at a more granular level.


### Equal to (eq)#
To listen to changes when a column's value in a table equals a client-specified value:

```javascript
1const channel = supabase2  .channel('changes')3  .on(4    'postgres_changes',5    {6      event: 'UPDATE',7      schema: 'public',8      table: 'messages',9      filter: 'body=eq.hey',10    },11    (payload) => console.log(payload)12  )13  .subscribe()
```

This filter uses Postgres's=filter.


### Not equal to (neq)#
To listen to changes when a column's value in a table does not equal a client-specified value:

```javascript
1const channel = supabase2  .channel('changes')3  .on(4    'postgres_changes',5    {6      event: 'INSERT',7      schema: 'public',8      table: 'messages',9      filter: 'body=neq.bye',10    },11    (payload) => console.log(payload)12  )13  .subscribe()
```

This filter uses Postgres's!=filter.


### Less than (lt)#
To listen to changes when a column's value in a table is less than a client-specified value:

```javascript
1const channel = supabase2  .channel('changes')3  .on(4    'postgres_changes',5    {6      event: 'INSERT',7      schema: 'public',8      table: 'profiles',9      filter: 'age=lt.65',10    },11    (payload) => console.log(payload)12  )13  .subscribe()
```

This filter uses Postgres's<filter, so it works for non-numeric types. Make sure to check the expected behavior of the compared data's type.


### Less than or equal to (lte)#
To listen to changes when a column's value in a table is less than or equal to a client-specified value:

```javascript
1const channel = supabase2  .channel('changes')3  .on(4    'postgres_changes',5    {6      event: 'UPDATE',7      schema: 'public',8      table: 'profiles',9      filter: 'age=lte.65',10    },11    (payload) => console.log(payload)12  )13  .subscribe()
```

This filter uses Postgres'<=filter, so it works for non-numeric types. Make sure to check the expected behavior of the compared data's type.


### Greater than (gt)#
To listen to changes when a column's value in a table is greater than a client-specified value:

```javascript
1const channel = supabase2  .channel('changes')3  .on(4    'postgres_changes',5    {6      event: 'INSERT',7      schema: 'public',8      table: 'products',9      filter: 'quantity=gt.10',10    },11    (payload) => console.log(payload)12  )13  .subscribe()
```

This filter uses Postgres's>filter, so it works for non-numeric types. Make sure to check the expected behavior of the compared data's type.


### Greater than or equal to (gte)#
To listen to changes when a column's value in a table is greater than or equal to a client-specified value:

```javascript
1const channel = supabase2  .channel('changes')3  .on(4    'postgres_changes',5    {6      event: 'INSERT',7      schema: 'public',8      table: 'products',9      filter: 'quantity=gte.10',10    },11    (payload) => console.log(payload)12  )13  .subscribe()
```

This filter uses Postgres's>=filter, so it works for non-numeric types. Make sure to check the expected behavior of the compared data's type.


### Contained in list (in)#
To listen to changes when a column's value in a table equals any client-specified values:

```javascript
1const channel = supabase2  .channel('changes')3  .on(4    'postgres_changes',5    {6      event: 'INSERT',7      schema: 'public',8      table: 'colors',9      filter: 'name=in.(red, blue, yellow)',10    },11    (payload) => console.log(payload)12  )13  .subscribe()
```

This filter uses Postgres's= ANY. Realtime allows a maximum of 100 values for this filter.


### Receivingoldrecords#
By default, onlynewrecord changes are sent but if you want to receive theoldrecord (previous values) whenever youUPDATEorDELETEa record, you can set thereplica identityof your table tofull:

```javascript
1alter table2  messages replica identity full;
```

RLS policies are not applied toDELETEstatements, because there is no way for Postgres to verify that a user has access to a deleted record. When RLS is enabled andreplica identityis set tofullon a table, theoldrecord contains only the primary key(s).


### Private schemas#
Postgres Changes works out of the box for tables in thepublicschema. You can listen to tables in your private schemas by granting tableSELECTpermissions to the database role found in your access token. You can run a query similar to the following:

```javascript
1grant select on "non_private_schema"."some_table" to authenticated;
```

We strongly encourage you to enable RLS and create policies for tables in private schemas. Otherwise, any role you grant access to will have unfettered read access to the table.


### Custom tokens#
You may choose to sign your own tokens to customize claims that can be checked in your RLS policies.

Your project JWT secret is found with yourProject API keysin your dashboard.

Do not expose theservice_roletoken on the client because the role is authorized to bypass row-level security.

To use your own JWT with Realtime make sure to set the token after instantiating the Supabase client and before connecting to a Channel.

```javascript
1const { createClient } = require('@supabase/supabase-js')23const supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_KEY, {})45// Set your custom JWT here6supabase.realtime.setAuth('your-custom-jwt')78const channel = supabase9  .channel('db-changes')10  .on(11    'postgres_changes',12    {13      event: '*',14      schema: 'public',15      table: 'messages',16      filter: 'body=eq.bye',17    },18    (payload) => console.log(payload)19  )20  .subscribe()
```


### Refreshed tokens#
You will need to refresh tokens on your own, but once generated, you can pass them to Realtime.

For example, if you're using thesupabase-jsv2client then you can pass your token like this:

```javascript
1// Client setup23supabase.realtime.setAuth('fresh-token')
```


### Limitations#

### Delete events are not filterable#
You can't filter Delete events when tracking Postgres Changes. This limitation is due to the way changes are pulled from Postgres.


### Spaces in table names#
Realtime currently does not work when table names contain spaces.


### Database instance and realtime performance#
Realtime systems usually require forethought because of their scaling dynamics. For thePostgres Changesfeature, every change event must be checked to see if the subscribed user has access. For instance, if you have 100 users subscribed to a table where you make a single insert, it will then trigger 100 "reads": one for each user.

There can be a database bottleneck which limits message throughput. If your database cannot authorize the changes rapidly enough, the changes will be delayed until you receive a timeout.

Database changes are processed on a single thread to maintain the change order. That means compute upgrades don't have a large effect on the performance of Postgres change subscriptions. You can estimate the expected maximum throughput for your database below.

If you are using Postgres Changes at scale, you should consider using separate "public" table without RLS and filters. Alternatively, you can use Realtime server-side only and then re-stream the changes to your clients using a Realtime Broadcast.

Enter your database settings to estimate the maximum throughput for your instance:

Don't forget to run your own benchmarks to make sure that the performance is acceptable for your use case.

We are making many improvements to Realtime's Postgres Changes. If you are uncertain about the performance of your use case, reach out usingSupport Formand we will be happy to help you. We have a team of engineers that can advise you on the best solution for your use-case.


================================================================================


# Presence
Source: https://supabase.com/docs/guides/realtime/presence

Presence


### Share state between users with Realtime Presence.
Share state between users with Realtime Presence.

Let's explore how to implement Realtime Presence to track state between multiple users.


### Usage#
You can use the Supabase client libraries to track Presence state between users.


### How Presence works#
Presence lets each connected client publish a small piece of state—called a “presence payload”—to a shared channel. Supabase stores each client’s payload under a unique presence key and keeps a merged view of all connected clients.

When any client subscribes, disconnects, or updates their presence payload, Supabase triggers one of three events:

- sync— the full presence state has been updated
- join— a new client has started tracking presence
- leave— a client has stopped tracking presence

The complete presence state returned bypresenceState()looks like this:

```javascript
1{2  "client_key_1": [{ "userId": 1, "typing": false }],3  "client_key_2": [{ "userId": 2, "typing": true }]4}
```


### Initialize the client#
Get the Project URL and key fromthe project'sConnectdialog.

Supabase is changing the way keys work to improve project security and developer experience. You canread the full announcement, but in the transition period, you can use both the currentanonandservice_rolekeys and the new publishable key with the formsb_publishable_xxxwhich will replace the older keys.

In most cases, you can get the correct key fromthe Project'sConnectdialog, but if you want a specific key, you can find all keys inthe API Keys section of a Project's Settings page:

- For legacy keys, copy theanonkey for client-side operations and theservice_rolekey for server-side operations from theLegacy API Keystab.
- For new keys, open theAPI Keystab, if you don't have a publishable key already, clickCreate new API Keys, and copy the value from thePublishable keysection.

```javascript
1import {  } from '@supabase/supabase-js'23const  = 'https://<project>.supabase.co'4const  = '<sb_publishable_... or anon key>'56const  = (, )
```


### Sync and track state#
Listen to thesync,join, andleaveevents triggered whenever any client joins or leaves the channel or changes their slice of state:

```javascript
1const  = .('room_01')234  .('presence', { : 'sync' }, () => {5    const  = .()6    .('sync', )7  })8  .('presence', { : 'join' }, ({ ,  }) => {9    .('join', , )10  })11  .('presence', { : 'leave' }, ({ ,  }) => {12    .('leave', , )13  })14  .()
```


### Sending state#
You can send state to all subscribers usingtrack():

```javascript
1const  = .('room_01')23const  = {4  : 'user-1',5  : new ().(),6}78.(async () => {9  if ( !== 'SUBSCRIBED') { return }1011  const  = await .()12  .()13})
```

A client will receive state from any other client that is subscribed to the same topic (in this caseroom_01). It will also automatically trigger its ownsyncandjoinevent handlers.


### Stop tracking#
You can stop tracking presence using theuntrack()method. This will trigger thesyncandleaveevent handlers.

```javascript
1const  = async () => {2  const  = await .()3  .()4}56()
```


### Presence options#
You can pass configuration options while initializing the Supabase Client.


### Presence key#
By default, Presence will generate a uniqueUUIDv1key on the server to track a client channel's state. If you prefer, you can provide a custom key when creating the channel. This key should be unique among clients.

```javascript
1import {  } from '@supabase/supabase-js'2const  = ('SUPABASE_URL', 'SUPABASE_PUBLISHABLE_KEY')34const  = .('test', {5  : {6    : {7      : 'userId-123',8    },9  },10})
```


================================================================================


# Realtime Pricing
Source: https://supabase.com/docs/guides/realtime/pricing

Realtime Pricing

You are charged for the number of Realtime messages and the number of Realtime peak connections.


### Messages#
$2.50per 1 million messages. You are only charged for usage exceeding your subscription
plan's quota.

For a detailed explanation of how charges are calculated, refer toManage Realtime Messages usage.


### Peak connections#
$10per 1,000 peak connections. You are only charged for usage exceeding your subscription
plan's quota.

For a detailed explanation of how charges are calculated, refer toManage Realtime Peak Connections usage.


================================================================================


# Realtime Protocol
Source: https://supabase.com/docs/guides/realtime/protocol

Realtime Protocol


### WebSocket connection setup#
To start the connection we use the WebSocket URL, which for:

- Supabase projects:wss://<PROJECT_REF>.supabase.co/realtime/v1/websocket?apikey=<API_KEY>
- self-hosted projects:wss://<HOST>:<PORT>/socket/websocket?apikey=<API_KEY>

As an example, usingwebsocat, you would run the following command in your terminal:

```javascript
1# With Supabase2websocat "wss://<PROJECT_REF>.supabase.co/realtime/v1/websocket?apikey=<API_KEY>"34# With self-hosted5websocat "wss://<HOST>:<PORT>/socket/websocket?apikey=<API_KEY>"
```

During this stage you can also set other URL params:

- vsn: sets the protocol version. Possible values are1.0.0and2.0.0. Defaults to1.0.0.
- log_level: sets the log level to be used by this connection to help you debug potential issues. This only affects server side logs.

After connecting aphx_joinevent must be sent to the server to join a channel. The next sections outline the different messages types and events that are supported.


### Protocol messages#
Messages can be serialized in different formats. The Realtime protocol supports two versions:1.0.0and2.0.0.


### 1.0.0#
Version 1.0.0 is extremely simple. It uses JSON as the serialization format for messages. The underlying WebSocket messages are all text frames.

Messages contain the following fields:

- event: The type of event being sent or received. Examplephx_join,postgres_changes,broadcast, etc.
- topic: The topic to which the message belongs. This is a string that identifies the channel or context of the message.
- payload: The data associated with the event. This can be any JSON-serializable data structure, such as an object or an array.
- ref: A unique reference ID for the message. This is useful to track replies to a specific message.
- join_ref: A unique reference ID to uniquely identify a joined topic for pushes, broadcasts, replies, etc.

Example:

```javascript
1{2  "topic": "realtime:presence-room",3  "event": "phx_join",4  "payload": {5    "config": {6      "broadcast": {7        "ack": false,8        "self": false9      },10      "presence": {11        "enabled": false12      },13      "private": false14    }15  },16  "ref": "1",17  "join_ref": "1"18}
```


### 2.0.0#
Version 2.0.0 uses text and binary WebSocket frames.


### Text frames#
Text frames are always JSON encoded, but unlike version 1.0.0, they use a JSON array where the element order must be exactly:

- join_ref
- ref
- topic
- event
- payload

Example:

```javascript
1[2  "1",3  "1",4  "realtime:presence-room",5  "phx_join",6  {7    "config": {8      "broadcast": {9        "ack": false,10        "self": false11      },12      "presence": {13        "enabled": false14      },15      "private": false16    }17  }18]
```


### Binary frames#
The two special message types have a well defined binary format where the first byte defines the type of message. Both are used to send and receive broadcast events. See theclientandserversent events for more details.


### User Broadcast Push#
```javascript
10                   1                   2                   32 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 13+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+4|  Type (0x03)  | Join Ref Size |   Ref Size    |  Topic Size   |5+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+6|User Event Size| Metadata Size | Payload Enc.  |  Join Ref ... |7+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+8|                      Ref (variable length)                    |9+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+10|                     Topic (variable length)                   |11+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+12|                  User Event (variable length)                 |13+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+14|                   Metadata (variable length)                  |15+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+16|                User Payload (variable length)                 |17+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

Field Descriptions:

- Type: 1 byte, value = 0x03
- Join Ref Size: 1 byte, size of join reference string (max 255)
- Ref Size: 1 byte, size of reference string (max 255)
- Topic Size: 1 byte, size of topic string (max 255)
- User Event Size: 1 byte, size of user event string (max 255)
- Metadata Size: 1 byte, size of metadata string (max 255)
- Payload Encoding: 1 byte (0 = binary, 1 = JSON)
- Join Ref: Variable length string
- Ref: Variable length string
- Topic: Variable length string
- User Event: Variable length string
- Metadata: Variable length JSON string
- User Payload: Variable length payload data


### User Broadcast#
```javascript
10                   1                   2                   32 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 13+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+4|  Type (0x04)  |  Topic Size   |User Event Size| Metadata Size |5+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+6| Payload Enc.  |          Topic (variable length)              |7+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+8|                  User Event (variable length)                 |9+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+10|                   Metadata (variable length)                  |11+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+12|                User Payload (variable length)                 |13+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

Field Descriptions:

- Type: 1 byte, value = 0x04
- Topic Size: 1 byte, size of topic string (max 255)
- User Event Size: 1 byte, size of user event string (max 255)
- Metadata Size: 1 byte, size of metadata JSON string (max 255)
- Payload Encoding: 1 byte (0 = binary, 1 = JSON)
- Topic: Variable length string
- User Event: Variable length string
- Metadata: Variable length JSON string
- User Payload: Variable length payload data


### Event types#
Messages for all events are encoded as text frames using JSON except with thebroadcastevent type which can happen on both text and binary frames.


### Client sent events#

### phx_join#
This is the initial message required to join a channel. The client sends this message to the server to join a specific topic and configure the features it wants to use, such as Postgres changes, Presence, and Broadcast. The payload of thephx_joinevent contains the configuration options for the channel.

```javascript
1{2  "config": {3    "broadcast": {4      "ack": boolean,5      "self": boolean,6      "replay" : {7        "since": integer,8        "limit": integer9      }10    },11    "presence": {12      "enabled": boolean,13      "key": string14    },15    "postgres_changes": [16      {17        "event": string,18        "schema": string,19        "table": string,20        "filter": string21      }22    ]23    "private": boolean24  },25  "access_token": string26}
```

- config:private: Whether the channel is privatebroadcast: Configuration options for broadcasting messagesack: Acknowledge broadcast messagesself: Include the sender in broadcast messagesreplay: Configuration options for broadcast replay (Optional)since: Replay messages since a specific timestamp in millisecondslimit: Limit the number of replayed messages (Optional)presence: Configuration options for presence trackingenabled: Whether presence tracking is enabled for this channelkey: Key to be used for presence tracking, if not specified or empty, a UUID will be generated and usedpostgres_changes: Array of configurations for Postgres changesevent: Database change event to listen to, acceptsINSERT,UPDATE,DELETE, or*to listen to all events.schema: Schema of the table to listen to, accepts*wildcard to listen to all schemastable: Table of the database to listen to, accepts*wildcard to listen to all tablesfilter: Filter to be used when pulling changes from database. Read more about filters in the usage docs forPostgres Changes
- access_token: Optional access token for authentication, if not provided, the server will use the API key.

- private: Whether the channel is private
- broadcast: Configuration options for broadcasting messagesack: Acknowledge broadcast messagesself: Include the sender in broadcast messagesreplay: Configuration options for broadcast replay (Optional)since: Replay messages since a specific timestamp in millisecondslimit: Limit the number of replayed messages (Optional)
- presence: Configuration options for presence trackingenabled: Whether presence tracking is enabled for this channelkey: Key to be used for presence tracking, if not specified or empty, a UUID will be generated and used
- postgres_changes: Array of configurations for Postgres changesevent: Database change event to listen to, acceptsINSERT,UPDATE,DELETE, or*to listen to all events.schema: Schema of the table to listen to, accepts*wildcard to listen to all schemastable: Table of the database to listen to, accepts*wildcard to listen to all tablesfilter: Filter to be used when pulling changes from database. Read more about filters in the usage docs forPostgres Changes

- ack: Acknowledge broadcast messages
- self: Include the sender in broadcast messages
- replay: Configuration options for broadcast replay (Optional)since: Replay messages since a specific timestamp in millisecondslimit: Limit the number of replayed messages (Optional)

- since: Replay messages since a specific timestamp in milliseconds
- limit: Limit the number of replayed messages (Optional)

- enabled: Whether presence tracking is enabled for this channel
- key: Key to be used for presence tracking, if not specified or empty, a UUID will be generated and used

- event: Database change event to listen to, acceptsINSERT,UPDATE,DELETE, or*to listen to all events.
- schema: Schema of the table to listen to, accepts*wildcard to listen to all schemas
- table: Table of the database to listen to, accepts*wildcard to listen to all tables
- filter: Filter to be used when pulling changes from database. Read more about filters in the usage docs forPostgres Changes

Example on protocol version2.0.0:

```javascript
1[2  "3",3  "5",4  "realtime:chat-room",5  "phx_join",6  {7    "config": {8      "broadcast": {9        "ack": false,10        "self": true,11        "replay": {12          "since": 1763407103911,13          "limit": 1014        }15      },16      "presence": {17        "key": "user_id-827",18        "enabled": true19      },20      "postgres_changes": [],21      "private": true22    }23  }24]
```


### phx_leave#
This message is sent by the client to leave a channel. It can be used to clean up resources or stop listening for events on that channel. Payload should be empty object.

Example on protocol version2.0.0:

```javascript
1["1", "3", "realtime:avatar-stack-demo", "phx_leave", {}]
```


### heartbeat#
The heartbeat message should be sent at least every 25 seconds to avoid a connection timeout. Payload should be an empty object.

For heartbeat, the topicphoenixis used as this special message is not connected to a specific channel.

Example on protocol version2.0.0:

```javascript
1[null, "26", "phoenix", "heartbeat", {}]
```


### access_token#
Used to setup a new token to be used by Realtime for authentication and to refresh the token to prevent a private channel from closing when the token expires.

```javascript
1{2   "access_token": string3}
```

- access_token: The new access token to be used for authentication. Either to change it or to refresh it.

Example on protocol version2.0.0:

```javascript
1[2  "10",3  "1",4  "realtime:chat-room",5  "access_token",6  {7    "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWUsImlhdCI6MTUxNjIzOTAyMn0.KMUFsIDTnFmyG3nMiGM6H9FNFUROf3wh7SmqJp-QV30"8  }9]
```


### broadcast (text frame)#
Used to send a broadcast event to all clients in a channel.

Thepayloadfield contains the event name and the data to broadcast.

```javascript
1{2   "event": string,3   "payload": json,4   "type": "broadcast"5}
```

- event: The name of the user event to broadcast.
- payload: The user data associated with the event, which can be any JSON-serializable data structure.
- type: The type of message, which must always bebroadcast.

Example on protocol version2.0.0:

```javascript
1[2  "10",3  "1",4  "realtime:chat-room",5  "broadcast",6  {7    "event": "user-event",8    "type": "broadcast",9    "payload": {10      "content": "Hello, World!",11      "createdAt": "2025-11-17T21:14:14Z",12      "id": "9b823349-71c0-465b-9a83-a63aa2a9ae6d",13      "username": "VCSHLD556nQD-B-vUTJJ3"14    }15  }16]
```


### broadcast (binary frame)#
See theUser Broadcast Pushsection for the binary frame structure.

This message is a streamlined version of the text frame broadcast event that also supports non-JSON payloads.
Below is the same example from the previous section, showing the binary frame structure with hexadecimal values for the header and plain text for the remaining fields:

- Join Ref:10
- Ref:1
- Topic:realtime:chat-room
- Payload encoding being JSON
- User Event:user-event
- Metadata is empty
- User Payload

```javascript
10x03                      // Type20x02                      // Join Ref Size30x01                      // Ref Size40x12                      // Topic Size50x0A                      // User Event Size60x00                      // Metadata Size70x01                      // Payload Encoding (1 = JSON)810                        // Actual Join Ref91                         // Actual Ref10realtime:chat-room        // Topic11user-event                // User Event12{                         // User Event Payload13  "content": "Hello, World!",14  "createdAt": "2025-11-17T21:14:14Z",15  "id": "9b823349-71c0-465b-9a83-a63aa2a9ae6d",16  "username": "VCSHLD556nQD-B-vUTJJ3"17}
```

The payload encoding is just a hint for the client to know if the payload should be treated as JSON or not.


### presence#
Used to send presence metadata after joining a channel. The payload contains the presence information to be tracked by the server.
This metadata is then sent back to all clients in the channel viapresence_stateandpresence_diffevents.

```javascript
1{2   "type": "presence",3   "event": "track",4   "payload": json5}
```

Example on protocol version2.0.0:

```javascript
1[2  "1",3  "5",4  "realtime:presence-room",5  "presence",6  {7    "type": "presence",8    "event": "track",9    "payload": {10      "name": "Alice",11      "color": "hsl(29, 100%, 70%)"12    }13  }14]
```


### Server sent events#

### phx_close#
This message is sent by the server to signal that the channel has been closed. Payload will be empty object.

Example on protocol version2.0.0:

```javascript
1["3", "3", "realtime:avatar-stack-demo", "phx_close", {}]
```


### phx_error#
This message is sent by the server when an unexpected error occurs in the channel. Payload will be an empty object

```javascript
1["3", "3", "realtime:avatar-stack-demo", "phx_error", {}]
```


### phx_reply#
The server sends these messages in response to client requests that require acknowledgment.

```javascript
1{2   "status": string,3   "response": any,4}
```

- status: The status of the response, can beokorerror.
- response: The response data, which can vary based on the event that was replied to

phx_joinhas a specific response structure outlined below.

Contains the status of the join request and any additional information requested in thephx_joinpayload.

```javascript
1{2   "postgres_changes": [3      {4         "id": number,5         "event": string,6         "schema": string,7         "table": string8      }9   ]10}
```

- postgres_changes: Array of Postgres changes that the client is subscribed to, each object contains:id: Unique identifier for the Postgres changes subscriptionevent: The type of event the client is subscribed to, such asINSERT,UPDATE,DELETE, or*schema: The schema of the table the client is subscribed totable: The table the client is subscribed to

- id: Unique identifier for the Postgres changes subscription
- event: The type of event the client is subscribed to, such asINSERT,UPDATE,DELETE, or*
- schema: The schema of the table the client is subscribed to
- table: The table the client is subscribed to

Example on protocol version2.0.0:

```javascript
1[2  "1",3  "1",4  "realtime:chat-room",5  "phx_reply",6  {7    "status": "ok",8    "response": {9      "postgres_changes": [10        {11          "id": 106243155,12          "event": "*",13          "schema": "public",14          "table": "test"15        }16      ]17    }18  }19]
```


### system#
The server sends system messages to inform clients about the status of their Realtime channel subscriptions.

```javascript
1{2   "message": string,3   "status": string,4   "extension": string,5   "channel": string6}
```

- message: A human-readable message describing the status of the subscription.
- status: The status of the subscription, can beok,error, ortimeout.
- extension: The extension that sent the message.
- channel: The channel to which the message belongs, such asrealtime:room1.

Example on protocol version2.0.0:

```javascript
1[2  "13",3  null,4  "realtime:chat-room",5  "system",6  {7    "message": "Subscribed to PostgreSQL",8    "status": "ok",9    "extension": "postgres_changes",10    "channel": "main"11  }12]
```


### broadcast (text frame)#
This is the structure of broadcast events received by all clients subscribed to a channel. Thepayloadfield contains the event name and data that was broadcasted.

```javascript
1{2  "event": string,3  "meta" : {4    "id" : uuid,5    "replayed" : boolean6  },7  "payload": json,8  "type": "broadcast"9}
```

- event: The name of the user event to broadcast.
- meta: Metadata about the broadcast message. Not always present.id: A unique identifier for the broadcast message in UUID format.replayed: A boolean indicating whether the message is a replayed message. Not always present
- payload: The user data associated with the event, which can be any JSON-serializable data structure.
- type: The type of message, which must always bebroadcastfor broadcast messages.

- id: A unique identifier for the broadcast message in UUID format.
- replayed: A boolean indicating whether the message is a replayed message. Not always present

Example on protocol version2.0.0:

```javascript
1[2  null,3  null,4  "realtime:chat-room",5  "broadcast",6  {7    "event": "message",8    "type": "broadcast",9    "meta": {10      "id": "006554ce-d22d-469c-877a-88bef47214a3"11    },12    "payload": {13      "id": "513edcc1-4cbc-4274-aa26-c195f7e8c090",14      "content": "oi",15      "username": "hpK9jN2iY-I2HioHWr5ml",16      "createdAt": "2025-11-18T22:44:29Z"17    }18  }19]
```


### broadcast (binary frame)#
See theUser Broadcastsection for the binary frame structure.

This message is a streamlined version of the text frame broadcast event that also supports non-JSON payloads.
Below is the same example from the previous section, showing the binary frame structure with hexadecimal values for the header and plain text for the remaining fields:

- Topic:realtime:chat-room
- Payload encoding being JSON
- Metadata:{"id":"006554ce-d22d-469c-877a-88bef47214a3"}
- User Event:message
- User Payload

```javascript
10x04                                          // Type20x12                                          // Topic Size30x07                                          // User Event Size40x2D                                          // Metadata Size50x01                                          // Payload Encoding (1 = JSON)6realtime:chat-room                            // Topic7message                                       // User Event8{"id":"006554ce-d22d-469c-877a-88bef47214a3"} // Metadata9{                                             // User Event Payload10  "id": "513edcc1-4cbc-4274-aa26-c195f7e8c090",11  "content": "oi",12  "username": "hpK9jN2iY-I2HioHWr5ml",13  "createdAt": "2025-11-18T22:44:29Z"14}
```

The metadata field is JSON encoded. The payload encoding is just a hint for the client to know if the payload should be treated as JSON or not.


### postgres_changes#
The server sends this message when a database change occurs in a subscribed schema and table. The payload contains the details of the change, including the schema, table, event type, and the new and old records.

```javascript
1{2   "ids": [3      number4   ],5   "data": {6      "schema": string,7      "table": string,8      "commit_timestamp": string,9      "type": "*" | "INSERT" | "UPDATE" | "DELETE",10      "columns": [11        {12          "name": string,13          "type": string14        }15      ]16      "record": {17         [key: string]: boolean | number | string | null18      },19      "old_record": {20         [key: string]: boolean | number | string | null21      },22      "errors": string | null23   }24}
```

- ids: An array of unique identifiers matching the subscription when joining the channel.
- data: An object containing the details of the change:schema: The schema of the table where the change occurred.table: The table where the change occurred.commit_timestamp: The timestamp when the change was committed to the database.type: The type of event that occurred, such asINSERT,UPDATE,DELETE, or*for all events.columns: An array of objects representing the columns of the table, each containing:name: The name of the column.type: The data type of the column.record: An object representing the new values after the change, with keys as column names and values as their corresponding values.old_record: An object representing the old values before the change, with keys as column names and values as their corresponding values.errors: Any errors that occurred during the change, if applicable.

- schema: The schema of the table where the change occurred.
- table: The table where the change occurred.
- commit_timestamp: The timestamp when the change was committed to the database.
- type: The type of event that occurred, such asINSERT,UPDATE,DELETE, or*for all events.
- columns: An array of objects representing the columns of the table, each containing:name: The name of the column.type: The data type of the column.
- record: An object representing the new values after the change, with keys as column names and values as their corresponding values.
- old_record: An object representing the old values before the change, with keys as column names and values as their corresponding values.
- errors: Any errors that occurred during the change, if applicable.

- name: The name of the column.
- type: The data type of the column.

```javascript
1[2  null,3  null,4  "realtime:chat-room",5  "postgres_changes",6  {7    "ids": [104868189],8    "data": {9      "schema": "public",10      "table": "test",11      "commit_timestamp": "2025-11-19T00:22:40.877Z",12      "type": "UPDATE",13      "columns": [14        {15          "name": "id",16          "type": "int8"17        },18        {19          "name": "created_at",20          "type": "timestamptz"21        },22        {23          "name": "text",24          "type": "text"25        }26      ],27      "record": {28        "id": 46,29        "text": "content",30        "created_at": "2025-11-03T09:32:55+00:00"31      },32      "old_record": {33        "id": 4634      },35      "errors": null36    }37  }38]
```


### presence_state#
After joining, the server sends apresence_statemessage to a client with presence information. The payload field contains keys, where each key represents a client and its value is a JSON object containing information about that client. The key is defined by the client when joining the channel. If not specified, a UUID is automatically generated.

```javascript
1{2   [key: string]: {3      metas: [4         {5            phx_ref: string,6            [key: string]: any7         }8      ]9   }10}
```

- key: The client key.
- metas: An array of metadata objects for the client, each containing:phx_ref: A unique reference ID for the metadata.Any other custom fields defined by the client, such asname.

- phx_ref: A unique reference ID for the metadata.
- Any other custom fields defined by the client, such asname.

Example on protocol version2.0.0:

```javascript
1[2  "4",3  null,4  "realtime:cursor-room",5  "presence_state",6  {7    "2wCojG1xWgxG2ZxwocvSX": {8      "metas": [9        {10          "phx_ref": "GHlA1fShRjMmZhnL",11          "color": "hsl(204, 100%, 70%)",12          "key": "2wCojG1xWgxG2ZxwocvSX"13        }14      ]15    },16    "6eorYR7andHiq-7tCkmxQ": {17      "metas": [18        {19          "phx_ref": "GHk99Q_ez6-GzaeG",20          "color": "hsl(7, 100%, 70%)",21          "key": "6eorYR7andHiq-7tCkmxQ"22        }23      ]24    },25    "FOeQUamq3OLOWAAZK8iH3": {26      "metas": [27        {28          "phx_ref": "GHk-wA8Z61GGzeoG",29          "color": "hsl(212, 100%, 70%)",30          "key": "FOeQUamq3OLOWAAZK8iH3"31        }32      ]33    }34  }35]
```


### presence_diff#
After a change to the presence state, such as a client joining or leaving, the server sends a presence_diff message to update the client's view of the presence state. The payload field contains two keys,joinsandleaves, which represent clients that have joined and left, respectively. Each key is either specified by the client when joining the channel or automatically generated as a UUID.

```javascript
1{2  "joins": {3    [key: string]: {4      metas: [5        {6          phx_ref: string,7          [key: string]: any8        }9      ]10    }11  },12  "leaves": {13    [key: string]: {14      metas: [15        {16          phx_ref: string,17          [key: string]: any18        }19      ]20    }21  }22}
```

- joins: An object containing metadata for clients that have joined the channel, with keys as UUIDs and values as metadata objects.
- leaves: An object containing metadata for clients that have left the channel, with keys as UUIDs and values as metadata objects.

Example on protocol version2.0.0:

```javascript
1[2  null,3  null,4  "realtime:cursor-room",5  "presence_diff",6  {7    "joins": {8      "XnAJXkZVEJuBYZcp9GCG5": {9        "metas": [10          {11            "phx_ref": "GHlE8VLvxuKGzQJN",12            "color": "hsl(60, 100%, 70%)",13            "user": "123"14          }15        ]16      }17    },18    "leaves": {19      "ouCsaiOdKZ9yauoy4x5pv": {20        "metas": [21          {22            "phx_ref": "GHlE8HyhSPAmZgdB",23            "color": "hsl(72, 100%, 70%)",24            "user": "456"25          }26        ]27      }28    }29  }30]
```


================================================================================


# Using Realtime Presence with Flutter
Source: https://supabase.com/docs/guides/realtime/realtime-user-presence

Using Realtime Presence with Flutter

Use Supabase Presence to display the currently online users on your Flutter application.

Displaying the list of currently online users is a common feature for real-time collaborative applications. Supabase Presence makes it easy to track users joining and leaving the session so that you can make a collaborative app.


================================================================================


# Using Realtime with Next.js
Source: https://supabase.com/docs/guides/realtime/realtime-with-nextjs

Using Realtime with Next.js

In this guide, we explore the best ways to receive real-time Postgres changes with your Next.js application.
We'll show both client and server side updates, and explore which option is best.


================================================================================


# Realtime Reports
Source: https://supabase.com/docs/guides/realtime/reports

Realtime Reports

Realtime reports give insights into how your application uses Supabase Realtime, including connections, broadcast and change events, execution times, and lag.

These reports help you:

- Monitor connection counts and message volumes against your plan's quotas
- Identify performance bottlenecks in RLS policies or database replication
- Troubleshoot errors and connection issues
- Plan capacity upgrades based on usage trends

Access Realtime reports fromProject Settings > Product Reports > Realtimein your project dashboard.


### Realtime reports overview#

### Connected Clients#
The Connected Clients report helps you monitor the total number of concurrent Realtime client connections to your project over time. This metric is essential for understanding your application's connection usage patterns and identifying when you're approaching your plan's connection limits.

The report displays the total number of connected Realtime clients, showing how connection counts fluctuate throughout the selected time period. Each client connection represents an active WebSocket connection to your Realtime service, which can subscribe to multiple channels for receiving real-time updates.


### Actions you can take#

### Broadcast Events#
The Broadcast Events report helps you monitor the volume of broadcast messages sent through your Realtime channels over time. This metric is essential for understanding your application's real-time messaging patterns and identifying when you're approaching your plan's message throughput limits.

The report displays the total number of broadcast events sent by clients, showing message volume throughout the selected time period. Broadcast events are low-latency messages sent between users using Realtime's pub/sub pattern, which can be sent from client libraries, REST APIs, or directly from your database. Each event represents a message broadcast to subscribers of a specific channel topic.


### Actions you can take#

### Presence Events#
The Presence Events report helps you monitor the volume of presence state updates sent through your Realtime channels over time. This metric is essential for understanding how your application tracks and synchronizes shared state between users, such as online status, user activity, or custom state information.

The report displays the total number of presence events sent by clients, showing state synchronization activity throughout the selected time period. Presence events occur when clientstrack,update, oruntracktheir presence state in a channel, triggeringsync,join, orleaveevents. Unlike broadcast messages, presence state is persisted in the channel so new joiners immediately receive the current state without waiting for other users to send updates.


### Actions you can take#

### Postgres Changes Events#
The Postgres Changes Events report helps you monitor the volume of database change events (INSERT, UPDATE, DELETE) sent to your Realtime clients over time. This metric is essential for understanding how your application processes database changes and identifying potential performance bottlenecks or scaling issues.

The report displays the total number of Postgres change events received by clients, showing database change activity throughout the selected time period. Postgres Changes use logical replication to stream database changes from the Write-Ahead Log (WAL) to subscribed clients. Each event represents a database change that has been broadcast to clients subscribed to the relevant schema and table. Note that Postgres Changes process changes on a single thread to maintain order, which can create bottlenecks at scale compared to Broadcast.


### Actions you can take#

### Rate of Channel Joins#
The Rate of Channel Joins report helps you monitor how quickly clients are joining Realtime channels over time. This metric is essential for understanding your application's channel subscription patterns and identifying when you're approaching your plan's channel join rate limits.

The report displays the rate of channel joins per second, showing how frequently clients subscribe to channels throughout the selected time period. A channel join occurs whenever a client subscribes to a channel topic to receive real-time updates. Each client connection can join multiple channels (up to 100 per connection for most plans), and the join rate measures how many of these subscriptions happen per second across your entire project.


### Actions you can take#

### Message Payload Size#
The Message Payload Size report helps you monitor the median size of message payloads sent through your Realtime channels over time. This metric is essential for understanding how message size impacts performance, latency, and bandwidth usage in your real-time application.

The report displays the median payload size in bytes, showing how message sizes fluctuate throughout the selected time period. Payload size directly affects message throughput and latency—larger payloads require more bandwidth and processing time, which can increase latency and reduce the number of messages your system can handle per second. Monitoring this metric helps you optimize your message structure and identify opportunities to reduce payload sizes for better performance.


### Actions you can take#

### Broadcast From Database Replication Lag#
The Broadcast from Database Replication Lag report helps you monitor the median time between when a message is committed to your database and when it's broadcast to Realtime clients. This metric is essential for understanding the latency introduced by the database replication process when using broadcast from database.

The report displays the median replication lag in milliseconds, showing the delay between database commit and broadcast throughout the selected time period. When you use broadcast from database (by inserting messages intorealtime.messages), Realtime reads changes from the Write-Ahead Log (WAL) using logical replication. The lag represents the time it takes for these changes to be processed and broadcast to subscribed clients. Higher lag values indicate delays in the replication pipeline, which can impact the real-time responsiveness of your application.


### Actions you can take#

### (Read) Private Channel Subscription RLS Execution Time#
The (Read) Private Channel Subscription RLS Execution Time report helps you monitor the median time it takes to execute Row Level Security (RLS) policies when users subscribe to private channels. This metric is essential for understanding how RLS policy complexity impacts channel join latency and overall connection performance.

The report displays the median RLS execution time in milliseconds, showing how long it takes to validate user permissions when subscribing to private channels throughout the selected time period. When a user joins a private channel, Realtime checks RLS policies on therealtime.messagestable to determine if the user has read access. This authorization check happens once per channel subscription and the result is cached for the duration of the connection. However, complex RLS policies with joins, function calls, or missing indexes can significantly increase this initial connection time.


### Actions you can take#

### (Write) Private Channel Subscription RLS Execution Time#
The (Write)Private Channel Subscription RLS Execution Time report helps you monitor the median time it takes to execute Row Level Security (RLS) policies when users publish messages to private channels. This metric is essential for understanding how RLS policy complexity impacts message publishing latency and overall broadcast performance.

The report displays the median RLS execution time in milliseconds, showing how long it takes to validate user permissions when publishing to private channels throughout the selected time period. When a user sends a broadcast message to a private channel, Realtime checks RLS policies on therealtime.messagestable to determine if the user has write (INSERT) access. This authorization check happens for the first message sent and then it's cached. Complex RLS policies with joins, function calls, or missing indexes can significantly increase first message publishing latency.


### Actions you can take#

### Total Requests#
The Total Requests report helps you monitor the overall volume of HTTP requests for Realtime over time. This metric is essential for understanding your application's usage patterns and identifying traffic trends or potential issues with API request handling.

The report displays the total number of HTTP requests made to the Realtime service which include the WebSocket upgrade requests and the REST API requests.


### Actions you can take#

### Response Errors#
The Response Errors report helps you monitor the number of failed HTTP requests to the Realtime service over time. This metric is essential for identifying issues with API requests, WebSocket upgrade failures, authentication problems, and other error conditions that may impact your application's real-time functionality.

The report displays the total number of response errors from the Realtime API, showing error frequency throughout the selected time period. These errors include HTTP error status codes (4xx client errors and 5xx server errors) from REST API requests, failed WebSocket upgrade requests, authorization failures, and other error responses. Monitoring error rates alongside total requests helps you identify patterns, correlate errors with specific events, and troubleshoot issues affecting your Realtime service availability.


### Actions you can take#

### Response Speed#
The Response Speed report helps you monitor the average response time for HTTP requests to the Realtime service over time. This metric is essential for understanding API performance, identifying latency issues, and ensuring your real-time features meet performance expectations.

The report displays the average response time in milliseconds, showing how quickly the Realtime service responds to HTTP requests throughout the selected time period. This includes response times for REST API requests such as broadcast messages, WebSocket upgrade requests, and other HTTP-based interactions. Higher response times can indicate performance bottlenecks, database load issues, or network problems that may impact the real-time responsiveness of your application.


### Actions you can take#

================================================================================


# Settings
Source: https://supabase.com/docs/guides/realtime/settings

Settings


### Realtime Settings that allow you to configure your Realtime usage.
Realtime Settings that allow you to configure your Realtime usage.


### Settings#
All changes made in this screen will disconnect all your connected clients to ensure Realtime starts with the appropriate settings and all changes are stored in Supabase middleware.

You can set the following settings using the Realtime Settings screen in your Dashboard:

- Enable Realtime service: Determines if the Realtime service is enabled or disabled for your project.
- Channel Restrictions: You can toggle this settings to set Realtime to allow public channels or set it to use only private channels withRealtime Authorization.
- Database connection pool size: Determines the number of connections used for Realtime Authorization RLS checking
- Max concurrent clients: Determines the maximum number of clients that can be connected
- Max events per second: Determines the maximum number of events per second that can be sent
- Max presence events per second: Determines the maximum number of presence events per second that can be sent
- Max payload size in KB: Determines the maximum number of payload size in KB that can be sent


================================================================================


# Subscribing to Database Changes
Source: https://supabase.com/docs/guides/realtime/subscribing-to-database-changes

Subscribing to Database Changes


### Listen to database changes in real-time from your website or application.
Listen to database changes in real-time from your website or application.

You can use Supabase to subscribe to real-time database changes. There are two options available:


### Using Broadcast#
To automatically send messages when a record is created, updated, or deleted, we can attach aPostgres triggerto any table. Supabase Realtime provides arealtime.broadcast_changes()function which we can use in conjunction with a trigger. This function will use a private channel and needs broadcast authorization RLS policies to be met.


### Broadcast authorization#
Realtime Authorizationis required for receiving Broadcast messages. This is an example of a policy that allows authenticated users to listen to messages from topics:

```javascript
1create policy "Authenticated users can receive broadcasts"2on "realtime"."messages"3for select4to authenticated5using ( true );
```


### Create a trigger function#
Let's create a function that we can call any time a record is created, updated, or deleted. This function will make use of some of Postgres's nativetrigger variables. For this example, we want to have a topic with the nametopic:<record id>to which we're going to broadcast events.

```javascript
1create or replace function public.your_table_changes()2returns trigger3security definer4language plpgsql5as $$6begin7  perform realtime.broadcast_changes(8    'topic:' || coalesce(NEW.id, OLD.id) ::text,       -- topic - the topic to which you're broadcasting where you can use the topic id to build the topic name9    TG_OP,                                             -- event - the event that triggered the function10    TG_OP,                                             -- operation - the operation that triggered the function11    TG_TABLE_NAME,                                     -- table - the table that caused the trigger12    TG_TABLE_SCHEMA,                                   -- schema - the schema of the table that caused the trigger13    NEW,                                               -- new record - the record after the change14    OLD                                                -- old record - the record before the change15  );16  return null;17end;18$$;
```


### Create a trigger#
Let's set up a trigger so the function is executed after any changes to the table.

```javascript
1create trigger handle_your_table_changes2after insert or update or delete3on public.your_table4for each row5execute function your_table_changes ();
```


### Listening on client side#
Finally, on the client side, listen to the topictopic:<record_id>to receive the events. Remember to set the channel as a private channel, sincerealtime.broadcast_changesuses Realtime Authorization.

```javascript
1const  = 'id'2await ..() // Needed for Realtime Authorization3const  = 4  .(`topic:${}`, {5    : { : true },6  })7  .('broadcast', { : 'INSERT' }, () => .())8  .('broadcast', { : 'UPDATE' }, () => .())9  .('broadcast', { : 'DELETE' }, () => .())10  .()
```


### Using Postgres Changes#
Postgres Changes are simple to use, but have somelimitationsas your application scales. We recommend using Broadcast for most use cases.


### Enable Postgres Changes#
You'll first need to create asupabase_realtimepublication and add your tables (that you want to subscribe to) to the publication:

```javascript
1begin;23-- remove the supabase_realtime publication4drop5  publication if exists supabase_realtime;67-- re-create the supabase_realtime publication with no tables8create publication supabase_realtime;910commit;1112-- add a table called 'messages' to the publication13-- (update this to match your tables)14alter15  publication supabase_realtime add table messages;
```


### Streaming inserts#
You can use theINSERTevent to stream all new rows.

```javascript
1const  = 2  .('schema-db-changes')3  .(4    'postgres_changes',5    {6      : 'INSERT',7      : 'public',8    },9    () => .()10  )11  .()
```


### Streaming updates#
You can use theUPDATEevent to stream all updated rows.

```javascript
1const  = 2  .('schema-db-changes')3  .(4    'postgres_changes',5    {6      : 'UPDATE',7      : 'public',8    },9    () => .()10  )11  .()
```


================================================================================


# Storage
Source: https://supabase.com/docs/guides/storage

Storage


### Use Supabase to store and serve files.
Use Supabase to store and serve files.

Supabase Storage is a robust, scalable solution for managing files of any size with fine-grained access controls and optimized delivery. Whether you're storing user-generated content, analytics data, or vector embeddings, Supabase Storage provides specialized bucket types to meet your specific needs.


### Key features#
- Multi Protocol- S3 compatible Storage, RESTful API, TUS resumable uploads
- Global CDN- Serve your assets with lightning-fast performance from over 285 cities worldwide
- Image Optimization- Resize, compress, and transform media files on the fly with built-in image processing
- Fine-grained Access Control- Manage file permissions with row-level security and custom policies
- Multiple Bucket Types- Specialized storage solutions for different use cases


### Storage bucket types#
Supabase Storage offers different bucket types optimized for specific use cases:


### Files buckets#
Store and serve traditional files including images, videos, documents, and general-purpose content. Ideal for user-generated content, media libraries, and asset management.

Use cases:Images, videos, documents, PDFs, archives

Features:

- Global CDN delivery
- Image optimization and transformation
- Row-level security integration
- Direct URL access for files

Learn more about Files Buckets


### Analytics buckets#
Purpose-built for storing and analyzing data in open table formats like Apache Iceberg. Perfect for time-series data, logs, and large-scale analytical workloads.

Use cases:Data lakes, analytics pipelines, ETL operations, historical data analysis

Features:

- Apache Iceberg table format support
- SQL-accessible via Postgres foreign tables
- Partitioned data organization
- Efficient data querying and transformation

Learn more about Analytics Buckets


### Vector buckets#
Specialized storage for vector embeddings and similarity search operations. Designed for AI and ML applications requiring semantic search capabilities.

Use cases:AI-powered search, semantic similarity matching, embedding storage, RAG systems

Features:

- Optimized vector indexing (HNSW, Flat)
- Multiple distance metrics (cosine, euclidean, L2)
- Metadata filtering for vectors
- Similarity search queries

Learn more about Vector Buckets


### Examples#
Check out all of the Storagetemplates and examplesin our GitHub repository.

Resumable Uploads with Uppy


### Resources#
Find the source code and documentation in the Supabase GitHub repository.

Supabase Storage API

OpenAPI Spec


================================================================================


# Iceberg Catalog
Source: https://supabase.com/docs/guides/storage/analytics/connecting-to-analytics-bucket

Iceberg Catalog

Expect rapid changes, limited features, and possible breaking updates.share feedbackas we refine the experience and expand access.

Analytics buckets require authentication with two distinct services:


### Architecture overview#
Iceberg REST Catalogserves as the metadata management system for your Iceberg tables. It enables Iceberg clients such as PyIceberg and Apache Spark to perform critical operations:

- Creating and managing tables and namespaces
- Tracking schemas and handling schema evolution
- Managing partitions and table snapshots
- Ensuring transactional consistency and isolation

The REST Catalog only stores metadata describing your data's structure, schema, and partitioning strategy—not the actual data itself.

S3-Compatible Storage Endpointhandles the actual data storage and retrieval. It's optimized for reading and writing large analytical datasets stored in Parquet format, separate from the metadata management layer.


### Authentication setup#
To connect to an analytics bucket, you need:


### 1. S3 credentials#
Create S3 credentials throughProject Settings > Storage. See theS3 Authentication Guidefor detailed instructions.

You'll obtain:

- Access Key ID
- Secret Access Key
- Region(e.g.,us-east-1)


### 2. Supabase service key#
Retrieve your Service Key fromProject Settings > API. This key authenticates requests to the Iceberg REST Catalog.


### 3. Project reference#
Your Supabase project reference is the subdomain in your project URL (e.g.,your-project-refinhttps://your-project-ref.supabase.co).


### Testing your connection#
You can verify your setup by making a direct request to the Iceberg REST Catalog. Provide your Service Key as a Bearer token:

```javascript
1curl \2  --request GET -sL \3  --url 'https://<your-project-ref>.supabase.co/storage/v1/iceberg/v1/config?warehouse=<bucket-name>' \4  --header 'Authorization: Bearer <your-service-key>'
```

A successful response returns the catalog configuration including warehouse location and settings.


### Next steps#
- Connect with PyIceberg
- Connect with Apache Spark
- Query with Postgres


================================================================================


# Creating Analytics Buckets
Source: https://supabase.com/docs/guides/storage/analytics/creating-analytics-buckets

Creating Analytics Buckets


### Set up your first analytics bucket using the SDK or dashboard.
Set up your first analytics bucket using the SDK or dashboard.

This feature is inPrivate Alpha. API stability and backward compatibility are not guaranteed at this stage. Request access through thisform.

Analytics buckets useApache Iceberg, an open-table format for efficient management of large analytical datasets. You can interact with analytics buckets using tools such asPyIceberg,Apache Spark, or any client supporting theIceberg REST Catalog API.


### Creating an Analytics bucket#
You can create an analytics bucket using either the Supabase SDK or the Supabase Dashboard.


### Using the Supabase SDK#
```javascript
1import {  } from '@supabase/supabase-js'23const  = ('https://your-project.supabase.co', 'your-service-key')45const { ,  } = await ...('analytics-data')67if () {8  .('Failed to create analytics bucket:', )9} else {10  .('Analytics bucket created:', )11}
```


### Using the Supabase Dashboard#

### Next steps#
Once you've created your analytics bucket, you can:

- Connect with Iceberg clientslike PyIceberg or Apache Spark
- Set up real-time replicationfrom your Postgres database
- Query data with Postgresusing the Iceberg Foreign Data Wrapper


================================================================================


# Apache Spark
Source: https://supabase.com/docs/guides/storage/analytics/examples/apache-spark

Apache Spark

Expect rapid changes, limited features, and possible breaking updates.Share feedbackas we refine the experience and expand access.

Apache Spark enables distributed analytical processing of large datasets stored in your analytics buckets. Use it for complex transformations, aggregations, and machine learning workflows.


### Installation#
First, ensure you have Spark installed. For Python-based workflows:

```javascript
1pip install pyspark
```

For detailed Spark setup instructions, see theApache Spark documentation.


### Basic setup#
Here's a complete example showing how to configure Spark with your Supabase analytics bucket:

```javascript
1from pyspark.sql import SparkSession23# Configuration - Update with your Supabase credentials4PROJECT_REF = "your-project-ref"5WAREHOUSE = "your-analytics-bucket-name"6SERVICE_KEY = "your-service-key"78# S3 credentials from Project Settings > Storage9S3_ACCESS_KEY = "your-access-key"10S3_SECRET_KEY = "your-secret-key"11S3_REGION = "us-east-1"1213# Construct Supabase endpoints14S3_ENDPOINT = f"https://{PROJECT_REF}.supabase.co/storage/v1/s3"15CATALOG_URI = f"https://{PROJECT_REF}.supabase.co/storage/v1/iceberg"1617# Initialize Spark session with Iceberg configuration18spark = SparkSession.builder \19    .master("local[*]") \20    .appName("SupabaseIceberg") \21    .config("spark.driver.host", "127.0.0.1") \22    .config("spark.driver.bindAddress", "127.0.0.1") \23    .config(24        'spark.jars.packages',25        'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1,org.apache.iceberg:iceberg-aws-bundle:1.6.1'26    ) \27    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \28    .config("spark.sql.catalog.supabase", "org.apache.iceberg.spark.SparkCatalog") \29    .config("spark.sql.catalog.supabase.type", "rest") \30    .config("spark.sql.catalog.supabase.uri", CATALOG_URI) \31    .config("spark.sql.catalog.supabase.warehouse", WAREHOUSE) \32    .config("spark.sql.catalog.supabase.token", SERVICE_KEY) \33    .config("spark.sql.catalog.supabase.s3.endpoint", S3_ENDPOINT) \34    .config("spark.sql.catalog.supabase.s3.path-style-access", "true") \35    .config("spark.sql.catalog.supabase.s3.access-key-id", S3_ACCESS_KEY) \36    .config("spark.sql.catalog.supabase.s3.secret-access-key", S3_SECRET_KEY) \37    .config("spark.sql.catalog.supabase.s3.remote-signing-enabled", "false") \38    .config("spark.sql.defaultCatalog", "supabase") \39    .getOrCreate()4041print("✓ Spark session initialized with Iceberg")
```


### Creating tables#
```javascript
1# Create a namespace for organization2spark.sql("CREATE NAMESPACE IF NOT EXISTS analytics")34# Create a new Iceberg table5spark.sql("""6    CREATE TABLE IF NOT EXISTS analytics.events (7        event_id BIGINT,8        user_id BIGINT,9        event_name STRING,10        event_timestamp TIMESTAMP,11        properties STRING12    )13    USING iceberg14""")1516print("✓ Created table: analytics.events")
```


### Writing data#
```javascript
1# Insert data into the table2spark.sql("""3    INSERT INTO analytics.events (event_id, user_id, event_name, event_timestamp, properties)4    VALUES5        (1, 101, 'login', TIMESTAMP '2024-01-15 10:30:00', '{"browser":"chrome"}'),6        (2, 102, 'view_product', TIMESTAMP '2024-01-15 10:35:00', '{"product_id":"123"}'),7        (3, 101, 'logout', TIMESTAMP '2024-01-15 10:40:00', '{}'),8        (4, 103, 'purchase', TIMESTAMP '2024-01-15 10:45:00', '{"amount":99.99}')9""")1011print("✓ Inserted 4 rows into analytics.events")
```


### Reading data#
```javascript
1# Read the entire table2result_df = spark.sql("SELECT * FROM analytics.events")3result_df.show(truncate=False)45# Apply filters6filtered_df = spark.sql("""7    SELECT event_id, user_id, event_name8    FROM analytics.events9    WHERE event_name = 'login'10""")11filtered_df.show()1213# Aggregations14summary_df = spark.sql("""15    SELECT16        event_name,17        COUNT(*) as event_count,18        COUNT(DISTINCT user_id) as unique_users19    FROM analytics.events20    GROUP BY event_name21    ORDER BY event_count DESC22""")23summary_df.show()
```


### Advanced operations#

### Working with dataframes#
```javascript
1# Read as DataFrame2events_df = spark.read.format("iceberg").load("analytics.events")34# Apply Spark transformations5from pyspark.sql.functions import count, col, year, month67# Monthly event counts8monthly_events = events_df \9    .withColumn("month", month(col("event_timestamp"))) \10    .withColumn("year", year(col("event_timestamp"))) \11    .groupBy("year", "month", "event_name") \12    .agg(count("event_id").alias("count")) \13    .orderBy("year", "month")1415monthly_events.show()
```


### Joining tables#
```javascript
1# Create another table2spark.sql("""3    CREATE TABLE IF NOT EXISTS analytics.users (4        user_id BIGINT,5        username STRING,6        email STRING7    )8    USING iceberg9""")1011spark.sql("""12    INSERT INTO analytics.users VALUES13        (101, 'alice', 'alice@example.com'),14        (102, 'bob', 'bob@example.com'),15        (103, 'charlie', 'charlie@example.com')16""")1718# Join events with users19joined_df = spark.sql("""20    SELECT21        e.event_id,22        e.event_name,23        u.username,24        u.email,25        e.event_timestamp26    FROM analytics.events e27    JOIN analytics.users u ON e.user_id = u.user_id28    ORDER BY e.event_timestamp29""")3031joined_df.show(truncate=False)
```


### Exporting results#
```javascript
1# Export to Parquet2spark.sql("""3    SELECT event_name, COUNT(*) as count4    FROM analytics.events5    GROUP BY event_name6""").write \7    .mode("overwrite") \8    .parquet("/tmp/event_summary.parquet")910# Export to CSV11spark.sql("""12    SELECT *13    FROM analytics.events14    WHERE event_timestamp > TIMESTAMP '2024-01-15 10:30:00'15""").write \16    .mode("overwrite") \17    .option("header", "true") \18    .csv("/tmp/recent_events.csv")1920print("✓ Results exported")
```


### Performance best practices#
- Partition tables- Partition large tables by date or region for faster queries
- Select columns- Only select columns you need to reduce I/O
- Use filters early- Apply WHERE clauses to reduce data processed
- Cache frequently accessed tables- Usespark.catalog.cacheTable()for tables accessed multiple times
- Cluster mode- Use cluster mode for production workloads instead of local mode


### Complete example: Data processing pipeline#
```javascript
1from pyspark.sql import SparkSession2from pyspark.sql.functions import col, year, month, count34# Setup (see Basic Setup section above)5spark = SparkSession.builder \6    .master("local[*]") \7    .appName("SupabaseAnalytics") \8    .config("spark.sql.defaultCatalog", "supabase") \9    # ... (add all config from Basic Setup)10    .getOrCreate()1112# Step 1: Read raw events13raw_events = spark.sql("SELECT * FROM analytics.events")1415# Step 2: Transform and aggregate16monthly_summary = raw_events \17    .withColumn("month", month(col("event_timestamp"))) \18    .withColumn("year", year(col("event_timestamp"))) \19    .groupBy("year", "month", "event_name") \20    .agg(count("event_id").alias("total_events"))2122# Step 3: Save results23monthly_summary.write \24    .mode("overwrite") \25    .option("path", "analytics.monthly_summary") \26    .saveAsTable("analytics.monthly_summary")2728print("✓ Pipeline completed")29monthly_summary.show()
```


### Next steps#
- Query with Postgres
- Connect with PyIceberg
- Explore with DuckDB


================================================================================


# DuckDB
Source: https://supabase.com/docs/guides/storage/analytics/examples/duckdb

DuckDB

Expect rapid changes, limited features, and possible breaking updates.Share feedbackas we refine the experience and expand access.

DuckDB is a high-performance SQL database system optimized for analytical workloads. It can directly query Iceberg tables stored in your analytics buckets, making it ideal for data exploration and complex analytical queries.


### Installation#
Install DuckDB and the Iceberg extension:

```javascript
1pip install duckdb duckdb-iceberg
```


### Connecting to Analytics buckets#
Here's a complete example of connecting to your Supabase analytics bucket and querying Iceberg tables:

```javascript
1import duckdb2import os34# Configuration5PROJECT_REF = "your-project-ref"6WAREHOUSE = "your-analytics-bucket-name"7SERVICE_KEY = "your-service-key"89# S3 credentials10S3_ACCESS_KEY = "your-access-key"11S3_SECRET_KEY = "your-secret-key"12S3_REGION = "us-east-1"1314# Construct endpoints15S3_ENDPOINT = f"https://{PROJECT_REF}.supabase.co/storage/v1/s3"16CATALOG_URI = f"https://{PROJECT_REF}.supabase.co/storage/v1/iceberg"1718# Initialize DuckDB connection19conn = duckdb.connect(":memory:")2021# Install and load the Iceberg extension22conn.install_extension("iceberg")23conn.load_extension("iceberg")2425# Configure Iceberg catalog with Supabase credentials26conn.execute(f"""27    CREATE SECRET (28        TYPE S3,29        KEY_ID '{S3_ACCESS_KEY}',30        SECRET '{S3_SECRET_KEY}',31        REGION '{S3_REGION}',32        ENDPOINT '{S3_ENDPOINT}',33        URL_STYLE 'virtual'34    );35""")3637# Configure the REST catalog38conn.execute(f"""39    ATTACH 'iceberg://{CATALOG_URI}' AS iceberg_catalog40    (41        TYPE ICEBERG_REST,42        WAREHOUSE '{WAREHOUSE}',43        TOKEN '{SERVICE_KEY}'44    );45""")4647# Query your Iceberg tables48result = conn.execute("""49    SELECT *50    FROM iceberg_catalog.default.events51    LIMIT 1052""").fetchall()5354for row in result:55    print(row)5657# Complex aggregation example58analytics = conn.execute("""59    SELECT60        event_name,61        COUNT(*) as event_count,62        COUNT(DISTINCT user_id) as unique_users63    FROM iceberg_catalog.default.events64    GROUP BY event_name65    ORDER BY event_count DESC66""").fetchdf()6768print(analytics)
```


### Key features with DuckDB#

### Efficient data exploration#
DuckDB's lazy evaluation means it only scans the data you need:

```javascript
1# This only reads the columns you select2events = conn.execute("""3    SELECT event_id, event_name, event_timestamp4    FROM iceberg_catalog.default.events5    WHERE event_timestamp > NOW() - INTERVAL '7 days'6""").fetchdf()
```


### Converting to Pandas#
Convert results to Pandas DataFrames for further analysis:

```javascript
1df = conn.execute("""2    SELECT *3    FROM iceberg_catalog.default.events4""").fetchdf()56# Use pandas for visualization or further processing7print(df.describe())
```


### Exporting results#
Save your analytical results to various formats:

```javascript
1# Export to Parquet2conn.execute("""3    COPY (4        SELECT * FROM iceberg_catalog.default.events5    ) TO 'results.parquet'6""")78# Export to CSV9conn.execute("""10    COPY (11        SELECT event_name, COUNT(*) as count12        FROM iceberg_catalog.default.events13        GROUP BY event_name14    ) TO 'summary.csv' (FORMAT CSV, HEADER true)15""")
```


### Best practices#
- Connection pooling- Reuse connections for multiple queries
- Partition pruning- Filter by partition columns to improve query performance
- Column selection- Only select columns you need to reduce I/O
- Limit results- Use LIMIT during exploration to avoid processing large datasets


### Next steps#
- Query with Postgres
- Connect with PyIceberg
- Analyze with Apache Spark


================================================================================


# PyIceberg
Source: https://supabase.com/docs/guides/storage/analytics/examples/pyiceberg

PyIceberg

Expect rapid changes, limited features, and possible breaking updates.Share feedbackas we refine the experience and expand access.

PyIceberg is a Python client for Apache Iceberg that enables programmatic interaction with Iceberg tables. Use it to create, read, update, and delete data in your analytics buckets.


### Installation#
```javascript
1pip install pyiceberg pyarrow
```


### Basic setup#
Here's a complete example showing how to connect to your Supabase analytics bucket and perform operations:

```javascript
1from pyiceberg.catalog import load_catalog2import pyarrow as pa3import datetime45# Configuration - Update with your Supabase credentials6PROJECT_REF = "your-project-ref"7WAREHOUSE = "your-analytics-bucket-name"8SERVICE_KEY = "your-service-key"910# S3 credentials from Project Settings > Storage11S3_ACCESS_KEY = "your-access-key"12S3_SECRET_KEY = "your-secret-key"13S3_REGION = "us-east-1"1415# Construct Supabase endpoints16S3_ENDPOINT = f"https://{PROJECT_REF}.supabase.co/storage/v1/s3"17CATALOG_URI = f"https://{PROJECT_REF}.supabase.co/storage/v1/iceberg"1819# Load the Iceberg REST Catalog20catalog = load_catalog(21    "supabase-analytics",22    type="rest",23    warehouse=WAREHOUSE,24    uri=CATALOG_URI,25    token=SERVICE_KEY,26    **{27        "py-io-impl": "pyiceberg.io.pyarrow.PyArrowFileIO",28        "s3.endpoint": S3_ENDPOINT,29        "s3.access-key-id": S3_ACCESS_KEY,30        "s3.secret-access-key": S3_SECRET_KEY,31        "s3.region": S3_REGION,32        "s3.force-virtual-addressing": False,33    },34)3536print("✓ Successfully connected to Iceberg catalog")
```


### Creating tables#
```javascript
1# Create a namespace for organization2catalog.create_namespace_if_not_exists("analytics")34# Define the schema for your Iceberg table5schema = pa.schema([6    pa.field("event_id", pa.int64()),7    pa.field("user_id", pa.int64()),8    pa.field("event_name", pa.string()),9    pa.field("event_timestamp", pa.timestamp("ms")),10    pa.field("properties", pa.string()),11])1213# Create the table14table = catalog.create_table_if_not_exists(15    ("analytics", "events"),16    schema=schema17)1819print("✓ Created table: analytics.events")
```


### Writing data#
```javascript
1import datetime23# Prepare your data4current_time = datetime.datetime.now()5data = pa.table({6    "event_id": [1, 2, 3, 4, 5],7    "user_id": [101, 102, 101, 103, 102],8    "event_name": ["login", "view_product", "logout", "purchase", "login"],9    "event_timestamp": [current_time] * 5,10    "properties": [11        '{"browser":"chrome"}',12        '{"product_id":"123"}',13        '{}',14        '{"amount":99.99}',15        '{"browser":"firefox"}'16    ],17})1819# Append data to the table20table.append(data)21print("✓ Appended 5 rows to analytics.events")
```


### Reading data#
```javascript
1# Scan the entire table2scan_result = table.scan().to_pandas()3print(f"Total rows: {len(scan_result)}")4print(scan_result.head())56# Query with filters7filtered = table.scan(8    filter="event_name = 'login'"9).to_pandas()10print(f"Login events: {len(filtered)}")1112# Select specific columns13selected = table.scan(14    selected_fields=["user_id", "event_name", "event_timestamp"]15).to_pandas()16print(selected.head())
```


### Advanced operations#

### Listing tables and namespaces#
```javascript
1# List all namespaces2namespaces = catalog.list_namespaces()3print("Namespaces:", namespaces)45# List tables in a namespace6tables = catalog.list_tables("analytics")7print("Tables in analytics:", tables)89# Get table metadata10table_metadata = catalog.load_table(("analytics", "events"))11print("Schema:", table_metadata.schema())12print("Partitions:", table_metadata.partitions())
```


### Handling errors#
```javascript
1try:2    # Attempt to load a table3    table = catalog.load_table(("analytics", "nonexistent"))4except Exception as e:5    print(f"Error loading table: {e}")67# Check if table exists before creating8namespace = "analytics"9table_name = "events"1011try:12    existing_table = catalog.load_table((namespace, table_name))13    print(f"Table already exists")14except Exception:15    print(f"Table does not exist, creating...")16    table = catalog.create_table((namespace, table_name), schema=schema)
```


### Performance tips#
- Batch writes- Insert data in batches rather than row-by-row for better performance
- Partition strategies- Use partitioning for large tables to improve query performance
- Schema evolution- PyIceberg supports schema changes without rewriting data
- Data format- Use Parquet for efficient columnar storage


### Complete example: ETL pipeline#
```javascript
1from pyiceberg.catalog import load_catalog2import pyarrow as pa3import pandas as pd45# Setup (see Basic Setup section above)6catalog = load_catalog(...)78# Step 1: Create analytics namespace9catalog.create_namespace_if_not_exists("warehouse")1011# Step 2: Define table schema12schema = pa.schema([13    pa.field("id", pa.int64()),14    pa.field("name", pa.string()),15    pa.field("created_at", pa.timestamp("ms")),16])1718# Step 3: Create table19table = catalog.create_table_if_not_exists(20    ("warehouse", "products"),21    schema=schema22)2324# Step 4: Load data from CSV or database25df = pd.read_csv("products.csv")26data = pa.Table.from_pandas(df)2728# Step 5: Write to analytics bucket29table.append(data)30print(f"✓ Loaded {len(data)} products to warehouse.products")3132# Step 6: Verify33result = table.scan().to_pandas()34print(result.describe())
```


### Next steps#
- Query with Postgres
- Analyze with Apache Spark
- Explore with DuckDB


================================================================================


# Analytics Buckets
Source: https://supabase.com/docs/guides/storage/analytics/introduction

Analytics Buckets


### Store large datasets for analytics and reporting.
Store large datasets for analytics and reporting.

Expect rapid changes, limited features, and possible breaking updates.share feedbackas we refine the experience and expand access.

Analytics buckets enable analytical workflows on large-scale datasets while keeping your primary database optimized for transactional operations.


### Why Analytics buckets?#
Postgres tables are purpose-built for transactional workloads with frequent inserts, updates, deletes, and low-latency queries. Analytical workloads have fundamentally different requirements:

- Processing large volumes of historical data
- Running complex queries and aggregations
- Minimizing storage costs
- Preventing analytical queries from impacting production traffic

Analytics buckets address these requirements usingApache Iceberg, an open-table format specifically designed for efficient management of large analytical datasets.


### Ideal use cases#
Analytics buckets are perfect for:

- Data warehousing and business intelligence- Build scalable data warehouses for BI tools
- Historical data archiving- Retain large volumes of historical data cost-effectively
- Periodically refreshed analytics- Maintain near real-time analytical views
- Complex analytical queries- Execute sophisticated aggregations and joins over large datasets

By separating transactional and analytical workloads, Supabase lets you build scalable analytics pipelines without compromising your primary Postgres performance.


================================================================================


# Analytics Buckets Limits
Source: https://supabase.com/docs/guides/storage/analytics/limits

Analytics Buckets Limits

Expect rapid changes, limited features, and possible breaking updates.share feedbackas we refine the experience and expand access.

The following default limits are applied when this feature is in the alpha stage, they can be adjusted on a case-by-case basis:


================================================================================


# Analytics Buckets Pricing
Source: https://supabase.com/docs/guides/storage/analytics/pricing

Analytics Buckets Pricing

Expect rapid changes, limited features, and possible breaking updates.share feedbackas we refine the experience and expand access.

Analytics buckets arefreeto use during the alpha phase. You will still be charged for the underlying egress.


================================================================================


# Query with PostgreSQL
Source: https://supabase.com/docs/guides/storage/analytics/query-with-postgres

Query with PostgreSQL


### Query analytics bucket data directly from PostgreSQL using SQL.
Query analytics bucket data directly from PostgreSQL using SQL.

Once your data flows into an analytics bucket—either via theReplication Pipelineor custom pipelines—you can query it directly from Postgres using standard SQL.

This is made possible by theIceberg Foreign Data Wrapper, which creates a bridge between your Postgres database and Iceberg tables.


### Setup overview#
You have two options to enable querying:


### Installing via Dashboard UI#
The dashboard provides the easiest setup experience:


### Querying your data#
Once the foreign data wrapper is installed, you can query your Iceberg tables using standard SQL:

```javascript
1select *2from schema_name.table_name3limit 100;
```


### Common query examples#
Get the latest events:

```javascript
1select event_id, event_name, event_timestamp2from analytics.events3order by event_timestamp desc4limit 1000;
```

Join with transactional data:

```javascript
1SELECT2  e.event_id,3  e.event_name,4  u.user_email5FROM analytics.events e6JOIN public.users u ON e.user_id = u.id7WHERE e.event_timestamp > NOW() - INTERVAL '7 days'8LIMIT 100;
```


### Manual installation#
For advanced use cases, you can manually install and configure the Iceberg Foreign Data Wrapper. See theIceberg Foreign Data Wrapper documentationfor detailed instructions.


================================================================================


# Realtime Data Sync to Analytics Buckets
Source: https://supabase.com/docs/guides/storage/analytics/replication

Realtime Data Sync to Analytics Buckets


### Replicate your PostgreSQL data to analytics buckets in real-time.
Replicate your PostgreSQL data to analytics buckets in real-time.

Expect rapid changes, limited features, and possible breaking updates.Share feedbackas the experience is refined and access is expanded.

By combining replication powered bySupabase ETLwithAnalytics Buckets, you can build an end-to-end data warehouse solution that automatically syncs changes from your Postgres database to Iceberg tables.

This guide provides a quickstart for replicating to Analytics Buckets. For complete replication configuration including other destinations, see theReplication Setup Guide.


### How it works#
The replication pipeline captures changes (INSERT, UPDATE, DELETE) from your Postgres database in real-time using Postgres logical replication and writes them to your analytics bucket. This allows you to maintain an always-up-to-date data warehouse without impacting your production workloads.


### Setup steps#

### Step 1: Create an Analytics bucket#
First, create a new analytics bucket to store your replicated data:


### Step 2: Create a publication#
A publication defines which tables and change types will be replicated. Create one using SQL in the Supabase SQL Editor:

```javascript
1-- Create publication for tables you want to replicate2CREATE PUBLICATION pub_warehouse3  FOR TABLE users, orders, products;
```

This publication will track all changes (INSERT, UPDATE, DELETE) for the specified tables. For advanced publication options like column filtering and row predicates, see theReplication Setup Guide.


### Step 3: Create the replication pipeline#
Now set up the pipeline to sync data to your analytics bucket:


### Monitoring your pipeline#
Once started, you can monitor the pipeline status directly in theDatabase > Replicationsection:

- Status- Shows if the pipeline is running, paused, or encountered errors
- Sync Progress- View the number of records replicated
- Logs- Check detailed logs for troubleshooting

Deleted tables are automatically recreated by the pipeline. To permanently delete a table, pause the pipeline first or remove it from the publication before deleting. See theFAQfor details.


### Next steps#
Once data is flowing to your analytics bucket, you can:

- Query with SQL via Postgres
- Connect with PyIceberg
- Analyze with Apache Spark

For detailed replication configuration and advanced topics:

- Replication Setup Guide- Complete replication configuration including BigQuery and other destinations
- Replication Monitoring Guide- Monitor replication pipeline status and health
- Replication FAQ- Common questions about replication


================================================================================


# Creating Buckets
Source: https://supabase.com/docs/guides/storage/buckets/creating-buckets

Creating Buckets

You can create a bucket using the Supabase Dashboard. Since storage is interoperable with your Postgres database, you can also use SQL or our client libraries.
Here we create a bucket called "avatars":

```javascript
1// Use the JS library to create a bucket.23const { ,  } = await ..('avatars', {4  : true, // default: false5})
```

Reference.


### Restricting uploads#
When creating a bucket you can add additional configurations to restrict the type or size of files you want this bucket to contain.

For example, imagine you want to allow your users to upload only images to theavatarsbucket and the size must not be greater than 1MB. You can achieve the following by providingallowedMimeTypesandmaxFileSize:

```javascript
1// Use the JS library to create a bucket.23const { ,  } = await ..('avatars', {4  : true,5  : ['image/*'],6  : '1MB',7})
```

If an upload request doesn't meet the above restrictions it will be rejected. SeeFile Limitsfor more information.


================================================================================


# Storage Buckets
Source: https://supabase.com/docs/guides/storage/buckets/fundamentals

Storage Buckets

Buckets allow you to keep your files organized and determines theAccess Modelfor your assets.Upload restrictionslike max file size and allowed content types are also defined at the bucket level.


### Access model#
There are 2 access models for buckets,publicandprivatebuckets.


### Private buckets#
When a bucket is set toPrivateall operations are subject to access control viaRLS policies. This also applies when downloading assets. Buckets are private by default.

The only ways to download assets within a private bucket is to:

- Use thedownload methodby providing a authorization header containing your user's JWT. The RLS policy you create on thestorage.objectstable will use this user to determine if they have access.
- Create a signed URL with thecreateSignedUrlmethodthat can be accessed for a limited time.


### Example use cases:#
- Uploading users' sensitive documents
- Securing private assets by using RLS to set up fine-grain access controls


### Public buckets#
When a bucket is designated as 'Public,' it effectively bypasses access controls for both retrieving and serving files within the bucket. This means that anyone who possesses the asset URL can readily access the file.

Access control is still enforced for other types of operations including uploading, deleting, moving, and copying.


### Example use cases:#
- User profile pictures
- User public media
- Blog post content

Public buckets are more performant than private buckets since they arecached differently.


================================================================================


# Storage CDN
Source: https://supabase.com/docs/guides/storage/cdn/fundamentals

Storage CDN

All assets uploaded to Supabase Storage are cached on a Content Delivery Network (CDN) to improve the latency for users all around the world. CDNs are a geographically distributed set of servers ornodeswhich cache content from anorigin server. For Supabase Storage, the origin is the storage server running in thesame region as your project. Aside from performance, CDNs also help with security and availability by mitigating Distributed Denial of Service (DDoS) and other application attacks.


### Example#
Let's walk through an example of how a CDN helps with performance.

A new bucket is created for a Supabase project launched in Singapore. All requests to the Supabase Storage API are routed to the CDN first.

A user from the United States requests an object and is routed to the U.S. CDN. At this point, that CDN node does not have the object in its cache and pings the origin server in Singapore.

Another user, also in the United States, requests the same object and is served directly from the CDN cache in the United States instead of routing the request back to Singapore.

Note that CDNs might still evict your object from their cache if it has not been requested for a while from a specific region. For example, if no user from United States requests your object, it will be removed from the CDN cache even if we set a very long cache control duration.

The cache status of a particular request is sent in thecf-cache-statusheader. A cache status ofMISSindicates that the CDN node did not have the object in its cache and had to ping the origin to get it. A cache status ofHITindicates that the object was sent directly from the CDN.


### Public vs private buckets#
Objects in public buckets do not require any authorization to access objects. This leads to a better cache hit rate compared to private buckets.

For private buckets, permissions for accessing each object is checked on a per user level. For example, if two different users access the same object in a private bucket from the same region, it results in a cache miss for both the users since they might have different security policies attached to them.
On the other hand, if two different users access the same object in a public bucket from the same region, it results in a cache hit for the second user.


================================================================================


# Cache Metrics
Source: https://supabase.com/docs/guides/storage/cdn/metrics

Cache Metrics

Cache hits can be determined via themetadata.response.headers.cf_cache_statuskey in ourLogs Explorer. Any value that corresponds to eitherHIT,STALE,REVALIDATED, orUPDATINGis categorized as a cache hit.
The following example query will show the top cache misses from theedge_logs:

```javascript
1select2  r.path as path,3  r.search as search,4  count(id) as count5from6  edge_logs as f7  cross join unnest(f.metadata) as m8  cross join unnest(m.request) as r9  cross join unnest(m.response) as res10  cross join unnest(res.headers) as h11where12  starts_with(r.path, '/storage/v1/object')13  and r.method = 'GET'14  and h.cf_cache_status in ('MISS', 'NONE/UNKNOWN', 'EXPIRED', 'BYPASS', 'DYNAMIC')15group by path, search16order by count desc17limit 50;
```

Try outthis queryin the Logs Explorer.

Your cache hit ratio over time can then be determined using the following query:

```javascript
1select2  timestamp_trunc(timestamp, hour) as timestamp,3  countif(h.cf_cache_status in ('HIT', 'STALE', 'REVALIDATED', 'UPDATING')) / count(f.id) as ratio4from5  edge_logs as f6  cross join unnest(f.metadata) as m7  cross join unnest(m.request) as r8  cross join unnest(m.response) as res9  cross join unnest(res.headers) as h10where starts_with(r.path, '/storage/v1/object') and r.method = 'GET'11group by timestamp12order by timestamp desc;
```

Try outthis queryin the Logs Explorer.


================================================================================


# Smart CDN
Source: https://supabase.com/docs/guides/storage/cdn/smart-cdn

Smart CDN

With Smart CDN caching enabled, the asset metadata in your database is synchronized to the edge. This automatically revalidates the cache when the asset is changed or deleted.

Moreover, the Smart CDN achieves a greater cache hit rate by shielding the origin server from asset requests that remain unchanged, even when different query strings are used in the URL.

Smart CDN caching is automatically enabled forPro Plan and above.


### Cache duration#
When Smart CDN is enabled, the asset is cached on the CDN for as long as possible. You can still control how long assets are stored in the browser using thecacheControloption when uploading a file. Smart CDN caching works with all types of storage operations including signed URLs.

When a file is updated or deleted, the CDN cache is automatically invalidated to reflect the change (including transformed images). It can takeup to 60 secondsfor the CDN cache to be invalidated as the asset metadata has to propagate across all the data-centers around the globe.

When an asset is invalidated at the CDN level, browsers may not update its cache. This is where cache eviction comes into play.


### Cache eviction#
Even when an asset is marked as invalidated at the CDN level, browsers may not refresh their cache for that asset.

If you have assets that undergo frequent updates, it is advisable to upload the new asset to a different path. This approach ensures that you always have the most up-to-date asset accessible.

If you anticipate that your asset might be deleted, it's advisable to set a shorter browser Time-to-Live (TTL) value using thecacheControloption. The default TTL is typically set to 1 hour, which is generally a reasonable default value.


### Bypassing cache#
If you need to ensure assets refresh directly from the origin server and bypass the cache, you can achieve this by adding a unique query string to the URL.

For instance, you can use a URL like/storage/v1/object/sign/profile-pictures/cat.jpg?version=1with a long browser cache (e.g., 1 year). To update the picture, increment the version query parameter in the URL, like/storage/v1/object/sign/profile-pictures/cat.jpg?version=2. The CDN will recognize it as a new object and fetch the updated version from the origin.


================================================================================


# Error Codes
Source: https://supabase.com/docs/guides/storage/debugging/error-codes

Error Codes


### Learn about the Storage error codes and how to resolve them
Learn about the Storage error codes and how to resolve them


### Handling Storage errors in SDKs#
When using the Supabase SDKs, storage errors provide detailed information to help you debug issues:

```javascript
1try {2  const { data, error } = await supabase.storage.from('avatars').download('avatar.png')34  if (error) {5    // Access error details6    console.log('Error code:', error.error)7    console.log('Error message:', error.message)8    console.log('HTTP status:', error.status)9    console.log('Status code:', error.statusCode)10  }11} catch (err) {12  console.error('Unexpected error:', err)13}
```


### Storage error codes#
We are transitioning to a new error code system. For backwards compatibility you'll still be able to see the old error codes

Error codes in Storage are returned as part of the response body. They are useful for debugging and understanding what went wrong with your request.
The error codes are returned in the following format:

```javascript
1{2  "code": "error_code",3  "message": "error_message"4}
```

Here is the full list of error codes and their descriptions:


### Legacy error codes#
As we are transitioning to a new error code system, you might still see the following error format:

```javascript
1{2  "httpStatusCode": 400,3  "code": "error_code",4  "message": "error_message"5}
```

Here's a list of the most common error codes and their potential resolutions:


### 404not_found#
Indicates that the resource is not found or you don't have the correct permission to access itResolution:

- Add a RLS policy to grant permission to the resource. See ourAccess Control docsfor more information.
- Ensure you include the userAuthorizationheader
- Verify the object exists


### 409already_exists#
Indicates that the resource already exists.Resolution:

- Use theupsertfunctionality in order to overwrite the file. Find out morehere.


### 403unauthorized#
You don't have permission to action this requestResolution:

- Add RLS policy to grant permission. See ourAccess Control docsfor more information.
- Ensure you include the userAuthorizationheader


### 429too many requests#
This problem typically arises when a large number of clients are concurrently interacting with the Storage service, and the pooler has reached itsmax_clientslimit.

Resolution:

- Increase the max_clients limits of the pooler.
- Upgrade to a bigger project compute instancehere.


### 544database_timeout#
This problem arises when a high number of clients are concurrently using the Storage service, and Postgres doesn't have enough available connections to efficiently handle requests to Storage.

Resolution:

- Increase the pool_size limits of the pooler.
- Upgrade to a bigger project compute instancehere.


### 500internal_server_error#
This issue occurs where there is a unhandled error.Resolution:

- File a support ticket to Storage teamhere


================================================================================


# Logs
Source: https://supabase.com/docs/guides/storage/debugging/logs

Logs

TheStorage Logsprovide a convenient way to examine all incoming request logs to your Storage service. You can filter by time and keyword searches.

For more advanced filtering needs, use theLogs Explorerto query the Storage logs dataset directly. The Logs Explorer is separate from the SQL Editor and uses a subset of the BigQuery SQL syntax rather than traditional SQL.

For more details on filtering the log tables, seeAdvanced Log Filtering


### Example Storage queries for the Logs Explorer#

### Filter by status 5XX error#
```javascript
1select2  id,3  storage_logs.timestamp,4  event_message,5  r.statusCode,6  e.message as errorMessage,7  e.raw as rawError8from9  storage_logs10  cross join unnest(metadata) as m11  cross join unnest(m.res) as r12  cross join unnest(m.error) as e13where r.statusCode >= 50014order by timestamp desc15limit 100;
```


### Filter by status 4XX error#
```javascript
1select2  id,3  storage_logs.timestamp,4  event_message,5  r.statusCode,6  e.message as errorMessage,7  e.raw as rawError8from9  storage_logs10  cross join unnest(metadata) as m11  cross join unnest(m.res) as r12  cross join unnest(m.error) as e13where r.statusCode >= 400 and r.statusCode < 50014order by timestamp desc15limit 100;
```


### Filter by method#
```javascript
1select id, storage_logs.timestamp, event_message, r.method2from3  storage_logs4  cross join unnest(metadata) as m5  cross join unnest(m.req) as r6where r.method in ("POST")7order by timestamp desc8limit 100;
```


### Filter by IP address#
```javascript
1select id, storage_logs.timestamp, event_message, r.remoteAddress2from3  storage_logs4  cross join unnest(metadata) as m5  cross join unnest(m.req) as r6where r.remoteAddress in ("IP_ADDRESS")7order by timestamp desc8limit 100;
```


================================================================================


# Copy Objects
Source: https://supabase.com/docs/guides/storage/management/copy-move-objects

Copy Objects


### Learn how to copy and move objects
Learn how to copy and move objects


### Copy objects#
You can copy objects between buckets or within the same bucket. Currently only objects up to 5 GB can be copied using the API.

When making a copy of an object, the owner of the new object will be the user who initiated the copy operation.


### Copying objects within the same bucket#
To copy an object within the same bucket, use thecopymethod.

```javascript
1await ..('avatars').('public/avatar1.png', 'private/avatar2.png')
```


### Copying objects across buckets#
To copy an object across buckets, use thecopymethod and specify the destination bucket.

```javascript
1await ..('avatars').('public/avatar1.png', 'private/avatar2.png', {2  : 'avatars2',3})
```


### Move objects#
You can move objects between buckets or within the same bucket. Currently only objects up to 5GB can be moved using the API.

When moving an object, the owner of the new object will be the user who initiated the move operation. Once the object is moved, the original object will no longer exist.


### Moving objects within the same bucket#
To move an object within the same bucket, you can use themovemethod.

```javascript
1const { ,  } = await .2  .('avatars')3  .('public/avatar1.png', 'private/avatar2.png')
```


### Moving objects across buckets#
To move an object across buckets, use themovemethod and specify the destination bucket.

```javascript
1await ..('avatars').('public/avatar1.png', 'private/avatar2.png', {2  : 'avatars2',3})
```


### Permissions#
Tocopyobjects, users needselectpermission on the source object andinsertpermission on the destination object. If the user also hasupdatepermission, the copy can be performed as an upsert, which will overwrite the destination object if it already exists.

Tomoveobjects, users needselectandupdatepermissions on the object.

For example:

```javascript
1create policy "User can select their own objects (in any buckets)"2on storage.objects3for select4to authenticated5using (6    owner_id = (select auth.uid())7);89create policy "User can insert in their own folders (in any buckets)"10on storage.objects11for insert12to authenticated13with check (14    (storage.folder(name))[1] = (select auth.uid())15);1617create policy "User can update their own objects (in any buckets)"18on storage.objects19for update20to authenticated21using (22    owner_id = (select auth.uid())23);
```


================================================================================


# Delete Objects
Source: https://supabase.com/docs/guides/storage/management/delete-objects

Delete Objects


### Learn about deleting objects
Learn about deleting objects

When you delete one or more objects from a bucket, the files are permanently removed and not recoverable. You can delete a single object or multiple objects at once.

Deleting objects should always be done via theStorage APIand NOT via aSQL query. Deleting objects via a SQL query will not remove the object from the bucket and will result in the object being orphaned.


### Delete objects#
To delete one or more objects, use theremovemethod.

```javascript
1await ..('bucket').(['object-path-2', 'folder/avatar2.png'])
```

When deleting objects, there is a limit of 1000 objects at a time using theremovemethod.


### RLS#
To delete an object, the user must have thedeletepermission on the object. For example:

```javascript
1create policy "User can delete their own objects"2on storage.objects3for delete4TO authenticated5USING (6    owner = (select auth.uid()::text)7);
```


================================================================================


# Pricing
Source: https://supabase.com/docs/guides/storage/pricing

Pricing

You are charged for the total size of all assets in your buckets.

$0.00002919per GB-Hr ($0.021per GB per month). You are only
charged for usage exceeding your subscription plan's quota.

For a detailed explanation of how charges are calculated, refer toManage Storage size usage.

If you useStorage Image Transformations, additional charges apply.


================================================================================


# Storage Optimizations
Source: https://supabase.com/docs/guides/storage/production/scaling

Storage Optimizations


### Scaling Storage
Scaling Storage

Here are some optimizations that you can consider to improve performance and reduce costs as you start scaling Storage.


### Egress#
If your project has high egress, these optimizations can help reducing it.


### Resize images#
Images typically make up most of your egress. By keeping them as small as possible, you can cut down on egress and boost your application's performance. You can take advantage of ourImage Transformationservice to optimize any image on the fly.


### Set a high cache-control value#
Using the browser cache can effectively lower your egress since the asset remains stored in the user's browser after the initial download. Setting a highcache-controlvalue ensures the asset stays in the user's browser for an extended period, decreasing the need to download it from the server repeatedly. Read morehere


### Limit the upload size#
You have the option to set a maximum upload size for your bucket. Doing this can prevent users from uploading and then downloading excessively large files. You can control the maximum file size by configuring this option at thebucket level.


### Smart CDN#
By leveraging ourSmart CDN, you can achieve a higher cache hit rate and therefore lower your egress cached, as we charge less for cached egress (seeegress pricing).


### Optimize listing objects#
Once you have a substantial number of objects, you might observe that thesupabase.storage.list()method starts to slow down. This occurs because the endpoint is quite generic and attempts to retrieve both folders and objects in a single query. While this approach is very useful for building features like the Storage viewer on the Supabase dashboard, it can impact performance with a large number of objects.

If your application doesn't need the entire hierarchy computed you can speed up drastically the query execution for listing your objects by creating a Postgres function as following:

```javascript
1create or replace function list_objects(2    bucketid text,3    prefix text,4    limits int default 100,5    offsets int default 06) returns table (7    name text,8    id uuid,9    updated_at timestamptz,10    created_at timestamptz,11    last_accessed_at timestamptz,12    metadata jsonb13) as $$14begin15    return query SELECT16        objects.name,17        objects.id,18        objects.updated_at,19        objects.created_at,20        objects.last_accessed_at,21        objects.metadata22    FROM storage.objects23    WHERE objects.name like prefix || '%'24    AND bucket_id = bucketid25    ORDER BY name ASC26    LIMIT limits27    OFFSET offsets;28end;29$$ language plpgsql stable;
```

You can then use the your Postgres function as following:

Using SQL:

```javascript
1select * from list_objects('bucket_id', '', 100, 0);
```

Using the SDK:

```javascript
1const { data, error } = await supabase.rpc('list_objects', {2  bucketid: 'yourbucket',3  prefix: '',4  limit: 100,5  offset: 0,6})
```


### Optimizing RLS#
When creating RLS policies against the storage tables you can add indexes to the interested columns to speed up the lookup


================================================================================


# Storage Quickstart
Source: https://supabase.com/docs/guides/storage/quickstart

Storage Quickstart


### Learn how to use Supabase to store and serve files.
Learn how to use Supabase to store and serve files.

This guide shows the basic functionality of Supabase Storage. Find a fullexample application on GitHub.


### Concepts#
Supabase Storage consists of Files, Folders, and Buckets.


### Files#
Files can be any sort of media file. This includes images, GIFs, and videos. It is best practice to store files outside of your database because of their sizes. For security, HTML files are returned as plain text.


### Folders#
Folders are a way to organize your files (just like on your computer). There is no right or wrong way to organize your files. You can store them in whichever folder structure suits your project.


### Buckets#
Buckets are distinct containers for files and folders. You can think of them like "super folders". Generally you would create distinct buckets for different Security and Access Rules. For example, you might keep all video files in a "video" bucket, and profile pictures in an "avatar" bucket.

File, Folder, and Bucket namesmust followAWS object key naming guidelinesand avoid use of any other characters.


### Create a bucket#
You can create a bucket using the Supabase Dashboard. Since the storage is interoperable with your Postgres database, you can also use SQL or our client libraries. Here we create a bucket called "avatars":


### Upload a file#
You can upload a file from the Dashboard, or within a browser using our JS libraries.


### Download a file#
You can download a file from the Dashboard, or within a browser using our JS libraries.


### Add security rules#
To restrict access to your files you can use either the Dashboard or SQL.


================================================================================


# S3 Authentication
Source: https://supabase.com/docs/guides/storage/s3/authentication

S3 Authentication


### Learn about authenticating with Supabase Storage S3.
Learn about authenticating with Supabase Storage S3.

You have two options to authenticate with Supabase Storage S3:

- Using the generated S3 access keys from yourproject settings(Intended exclusively for server-side use)
- Using a Session Token, which will allow you to authenticate with a user JWT token and provide limited access via Row Level Security (RLS).


### S3 access keys#
S3 access keys provide full access to all S3 operations across all buckets and bypass RLS policies. These are meant to be used only on the server.

To authenticate with S3, generate a pair of credentials (Access Key ID and Secret Access Key), copy the endpoint and region from theproject settings page.

This is all the information you need to connect to Supabase Storage using any S3-compatible service.

For optimal performance when uploading large files you should always use the direct storage hostname. This provides several performance enhancements that will greatly improve performance when uploading large files.

Instead ofhttps://project-id.supabase.cousehttps://project-id.storage.supabase.co

```javascript
1import { S3Client } from '@aws-sdk/client-s3';23const client = new S3Client({4  forcePathStyle: true,5  region: 'project_region',6  endpoint: 'https://project_ref.storage.supabase.co/storage/v1/s3',7  credentials: {8    accessKeyId: 'your_access_key_id',9    secretAccessKey: 'your_secret_access_key',10  }11})
```


### Session token#
You can authenticate to Supabase S3 with a user JWT token to provide limited access via RLS to all S3 operations. This is useful when you want initialize the S3 client on the server scoped to a specific user, or use the S3 client directly from the client side.

All S3 operations performed with the Session Token are scoped to the authenticated user. RLS policies on the Storage Schema are respected.

To authenticate with S3 using a Session Token, use the following credentials:

- access_key_id:project_ref
- secret_access_key:anonKey
- session_token:valid jwt token

For example, using theaws-sdklibrary:

```javascript
1import { S3Client } from '@aws-sdk/client-s3'23const {4  data: { session },5} = await supabase.auth.getSession()67const client = new S3Client({8  forcePathStyle: true,9  region: 'project_region',10  endpoint: 'https://project_ref.storage.supabase.co/storage/v1/s3',11  credentials: {12    accessKeyId: 'project_ref',13    secretAccessKey: 'anonKey',14    sessionToken: session.access_token,15  },16})
```


================================================================================


# S3 Compatibility
Source: https://supabase.com/docs/guides/storage/s3/compatibility

S3 Compatibility


### Learn about the compatibility of Supabase Storage with S3.
Learn about the compatibility of Supabase Storage with S3.

Supabase Storage is compatible with the S3 protocol. You can use any S3 client to interact with your Storage objects.

Storage supportsstandard,resumableandS3 uploadsand all these protocols are interoperable. You can upload a file with the S3 protocol and list it with the REST API or upload with Resumable uploads and list with S3.

Storage supports presigning a URL using query parameters. Specifically, Supabase Storage expects requests to be made usingAWS Signature Version 4. To enable this feature, enable the S3 connection via S3 protocol in the Settings page for Supabase Storage.

The S3 protocol is currently in Public Alpha. If you encounter any issues or have feature requests,contact us.

S3 versioning is not supported.Supabase Storage does not enable S3's versioning capabilities for buckets. Deleted objects are permanently removed and cannot be restored.


### Implemented endpoints#
The most commonly used endpoints are implemented, and more will be added. Implemented S3 endpoints are marked with ✅ in the following tables.


### Bucket operations#

### Object operations#

================================================================================


# Custom Roles
Source: https://supabase.com/docs/guides/storage/schema/custom-roles

Custom Roles


### Learn about using custom roles with storage schema
Learn about using custom roles with storage schema

In this guide, you will learn how to create and use custom roles with Storage to manage role-based access to objects and buckets. The same approach can be used to use custom roles with any other Supabase service.

Supabase Storage uses the same role-based access control system as any other Supabase service using RLS (Row Level Security).


### Create a custom role#
Let's create a custom rolemanagerto provide full read access to a specific bucket. For a more advanced setup, see theRBAC Guide.

```javascript
1create role 'manager';23-- Important to grant the role to the authenticator and anon role4grant manager to authenticator;5grant anon to manager;
```


### Create a policy#
Let's create a policy that gives full read permissions to all objects in the bucketteamsfor themanagerrole.

```javascript
1create policy "Manager can view all files in the bucket 'teams'"2on storage.objects3for select4to manager5using (6 bucket_id = 'teams'7);
```


### Test the policy#
To impersonate themanagerrole, you will need a valid JWT token with themanagerrole.
You can quickly create one using thejsonwebtokenlibrary in Node.js.

Signing a new JWT requires yourJWT_SECRET. You must store this secret securely. Never expose it in frontend code, and do not check it into version control.

```javascript
1const  = ('jsonwebtoken')23const  = 'your-jwt-secret' // You can find this in your Supabase project settings under API. Store this securely.4const  = '' // the user id that we want to give the manager role56const  = .({ : 'manager', :  }, , {7  : '1h',8})
```

Now you can use this token to access the Storage API.

```javascript
1const { StorageClient } = require('@supabase/storage-js')23const PROJECT_URL = 'https://your-project-id.supabase.co/storage/v1'45const storage = new StorageClient(PROJECT_URL, {6  authorization: `Bearer ${token}`,7})89await storage.from('teams').list()
```


================================================================================


# The Storage Schema
Source: https://supabase.com/docs/guides/storage/schema/design

The Storage Schema


### Learn about the storage schema
Learn about the storage schema

Storage uses Postgres to store metadata regarding your buckets and objects. Users can use RLS (Row-Level Security) policies for access control. This data is stored in a dedicated schema within your project calledstorage.

When working with SQL, it's crucial to consider all records in Storage tables as read-only. All operations, including uploading, copying, moving, and deleting, shouldexclusively go through the API.

This is important because the storage schema only stores the metadata and the actual objects are stored in a provider like S3. Deleting the metadata doesn't remove the object in the underlying storage provider. This results in your object being inaccessible, but you'll still be billed for it.

Here is the schema that represents the Storage service:

You have the option to query this table directly to retrieve information about your files in Storage without the need to go through our API.


### Modifying the schema#
We strongly recommend refraining from making any alterations to thestorageschema and treating it as read-only. This approach is important because any modifications to the schema on your end could potentially clash with our future updates, leading to downtime.

However, we encourage you to add custom indexes as they can significantly improve the performance of the RLS policies you create for enforcing access control.


================================================================================


# Storage Helper Functions
Source: https://supabase.com/docs/guides/storage/schema/helper-functions

Storage Helper Functions


### Learn the storage schema
Learn the storage schema

Supabase Storage provides SQL helper functions which you can use to write RLS policies.


### storage.filename()#
Returns the name of a file. For example, if your file is stored inpublic/subfolder/avatar.pngit would return:'avatar.png'

Usage

This example demonstrates how you would allow any user to download a file calledfavicon.ico:

```javascript
1create policy "Allow public downloads"2on storage.objects3for select4to public5using (6  storage.filename(name) = 'favicon.ico'7);
```


### storage.foldername()#
Returns an array path, with all of the subfolders that a file belongs to. For example, if your file is stored inpublic/subfolder/avatar.pngit would return:[ 'public', 'subfolder' ]

Usage

This example demonstrates how you would allow authenticated users to upload files to a folder calledprivate:

```javascript
1create policy "Allow authenticated uploads"2on storage.objects3for insert4to authenticated5with check (6  (storage.foldername(name))[1] = 'private'7);
```


### storage.extension()#
Returns the extension of a file. For example, if your file is stored inpublic/subfolder/avatar.pngit would return:'png'

Usage

This example demonstrates how you would allow restrict uploads to only PNG files inside a bucket calledcats:

```javascript
1create policy "Only allow PNG uploads"2on storage.objects3for insert4to authenticated5with check (6  bucket_id = 'cats' and storage.extension(name) = 'png'7);
```


================================================================================


# Storage Access Control
Source: https://supabase.com/docs/guides/storage/security/access-control

Storage Access Control

Supabase Storage is designed to work perfectly with PostgresRow Level Security(RLS).

You can use RLS to createSecurity Access Policiesthat are incredibly powerful and flexible, allowing you to restrict access based on your business needs.


### Access policies#
By default Storage does not allow any uploads to buckets without RLS policies. You selectively allow certain operations by creating RLS policies on thestorage.objectstable.

You can find the documentation for the storage schemahere, and to simplify the process of crafting your policies, you can utilize thesehelper functions.

The RLS policies required for different operations are documentedhere

For example, the only RLS policy required foruploadingobjects is to grant theINSERTpermission to thestorage.objectstable.

To allow overwriting files using theupsertfunctionality you will need to additionally grantSELECTandUPDATEpermissions.


### Policy examples#
An easy way to get started would be to create RLS policies forSELECT,INSERT,UPDATE,DELETEoperations and restrict the policies to meet your security requirements. For example, one can start with the followingINSERTpolicy:

```javascript
1create policy "policy_name"2ON storage.objects3for insert with check (4  true5);
```

and modify it to only allow authenticated users to upload assets to a specific bucket by changing it to:

```javascript
1create policy "policy_name"2on storage.objects for insert to authenticated with check (3    -- restrict bucket4    bucket_id = 'my_bucket_id'5);
```

This example demonstrates how you would allow authenticated users to upload files to a folder calledprivateinsidemy_bucket_id:

```javascript
1create policy "Allow authenticated uploads"2on storage.objects3for insert4to authenticated5with check (6  bucket_id = 'my_bucket_id' and7  (storage.foldername(name))[1] = 'private'8);
```

This example demonstrates how you would allow authenticated users to upload files to a folder called with theirusers.idinsidemy_bucket_id:

```javascript
1create policy "Allow authenticated uploads"2on storage.objects3for insert4to authenticated5with check (6  bucket_id = 'my_bucket_id' and7  (storage.foldername(name))[1] = (select auth.jwt()->>'sub')8);
```

Allow a user to access a file that was previously uploaded by the same user:

```javascript
1create policy "Individual user Access"2on storage.objects for select3to authenticated4using ( (select auth.jwt()->>'sub') = owner_id );
```


### Bypassing access controls#
If you exclusively use Storage from trusted clients, such as your own servers, and need to bypass the RLS policies, you can use theservice keyin theAuthorizationheader. Service keys entirely bypass RLS policies, granting you unrestricted access to all Storage APIs.

Remember you should not share the service key publicly.


================================================================================


# Ownership
Source: https://supabase.com/docs/guides/storage/security/ownership

Ownership

When creating new buckets or objects in Supabase Storage, an owner is automatically assigned to the bucket or object. The owner is the user who created the resource and the value is derived from thesubclaim in the JWT.
We store theownerin theowner_idcolumn.

When using theservice_keyto create a resource, the owner will not be set and the resource will be owned by anyone. This is also the case when you are creating Storage resources via the Dashboard.

The Storage schema has 2 fields to represent ownership:ownerandowner_id.owneris deprecated and will be removed. Useowner_idinstead.


### Access control#
By itself, the ownership of a resource does not provide any access control. However, you can enforce the ownership by implementing access control against storage resources scoped to their owner.

For example, you can implement a policy where only the owner of an object can delete it. To do this, check theowner_idfield of the object and compare it with thesubclaim of the JWT:

```javascript
1create policy "User can delete their own objects"2on storage.objects3for delete4to authenticated5using (6    owner_id = (select auth.uid()::text)7);
```

The use of RLS policies is just one way to enforce access control. You can also implement access control in your server code by following the same pattern.


================================================================================


# Bandwidth & Storage Egress
Source: https://supabase.com/docs/guides/storage/serving/bandwidth

Bandwidth & Storage Egress


### Bandwidth & Storage Egress
Bandwidth & Storage Egress


### Bandwidth & Storage egress#
Free Plan Organizations in Supabase have a limit of 10 GB of bandwidth (5 GB cached + 5 GB uncached). This limit is calculated by the sum of all the data transferred from the Supabase servers to the client. This includes all the data transferred from the database, storage, and functions.


### Checking Storage egress requests in Logs Explorer#
We have a template query that you can use to get the number of requests for each object inLogs Explorer.

```javascript
1select2  request.method as http_verb,3  request.path as filepath,4  (responseHeaders.cf_cache_status = 'HIT') as cached,5  count(*) as num_requests6from7  edge_logs8  cross join unnest(metadata) as metadata9  cross join unnest(metadata.request) as request10  cross join unnest(metadata.response) as response11  cross join unnest(response.headers) as responseHeaders12where13  (path like '%storage/v1/object/%' or path like '%storage/v1/render/%')14  and request.method = 'GET'15group by 1, 2, 316order by num_requests desc17limit 100;
```

Example of the output:

```javascript
1[2  {3    "filepath": "/storage/v1/object/sign/large%20bucket/20230902_200037.gif",4    "http_verb": "GET",5    "cached": true,6    "num_requests": 1007  },8  {9    "filepath": "/storage/v1/object/public/demob/Sports/volleyball.png",10    "http_verb": "GET",11    "cached": false,12    "num_requests": 16813  }14]
```


### Calculating egress#
If you already know the size of those files, you can calculate the egress by multiplying the number of requests by the size of the file.
You can also get the size of the file with the following cURL:

```javascript
1curl -s -w "%{size_download}\n" -o /dev/null "https://my_project.supabase.co/storage/v1/object/large%20bucket/20230902_200037.gif"
```

This will return the size of the file in bytes.
For this example, let's say that20230902_200037.gifhas a file size of 3 megabytes andvolleyball.pnghas a file size of 570 kilobytes.

Now, we have to sum all the egress for all the files to get the total egress:

```javascript
1100 * 3MB = 300MB2168 * 570KB = 95.76MB3Total Egress = 395.76MB
```

You can see that these values can get quite large, so it's important to keep track of the egress and optimize the files.


### Optimizing egress#
See ourscaling tips for egress.


================================================================================


# Serving assets from Storage
Source: https://supabase.com/docs/guides/storage/serving/downloads

Serving assets from Storage


### Serving assets from Storage
Serving assets from Storage


### Public buckets#
As mentioned in theBuckets Fundamentalsall files uploaded in a public bucket are publicly accessible and benefit a high CDN cache HIT ratio.

You can access them by using this conventional URL:

```javascript
1https://[project_id].supabase.co/storage/v1/object/public/[bucket]/[asset-name]
```

You can also use the Supabase SDKgetPublicUrlto generate this URL for you

```javascript
1const {  } = ..('bucket').('filePath.jpg')23.(.)
```


### Downloading#
If you want the browser to start an automatic download of the asset instead of trying serving it, you can add the?downloadquery string parameter.

By default it will use the asset name to save the file on disk. You can optionally pass a custom name to thedownloadparameter as following:?download=customname.jpg


### Programmatic downloads with query parameters#
When using the SDK'sdownload()method, you can pass additional query parameters to customize the download behavior:

```javascript
1import { createClient } from '@supabase/supabase-js'2const supabase = createClient('your_project_url', 'your_supabase_api_key')34// ---cut---5// Download with custom filename6const { data, error } = await supabase.storage.from('avatars').download('avatar1.png', {7  download: 'my-custom-name.png',8})
```


### Private buckets#
Assets stored in a non-public bucket are considered private and are not accessible via a public URL like the public buckets.

You can access them only by:

- Signing a time limited URL on the Server, for example with Edge Functions.
- with a GET request the URLhttps://[project_id].supabase.co/storage/v1/object/authenticated/[bucket]/[asset-name]and the user Authorization header


### Signing URLs#
You can sign a time-limited URL that you can share to your users by invoking thecreateSignedUrlmethod on the SDK.

```javascript
1const { ,  } = await .2  .('bucket')3  .('private-document.pdf', 3600)45if () {6  .(.)7}
```


================================================================================


# Storage Image Transformations
Source: https://supabase.com/docs/guides/storage/serving/image-transformations

Storage Image Transformations


### Transform images with Storage
Transform images with Storage

Supabase Storage offers the functionality to optimize and resize images on the fly. Any image stored in your buckets can be transformed and optimized for fast delivery.

Image Resizing is currently enabled forPro Plan and above.


### Get a public URL for a transformed image#
Our client libraries methods likegetPublicUrlandcreateSignedUrlsupport thetransformoption. This returns the URL that serves the transformed image.

```javascript
1..('bucket').('image.jpg', {2  : {3    : 500,4    : 600,5  },6})
```

An example URL could look like this:

```javascript
1https://project_id.supabase.co/storage/v1/render/image/public/bucket/image.jpg?width=500&height=600`
```


### Signing URLs with transformation options#
To share a transformed image in a private bucket for a fixed amount of time, provide the transform option when you create the signed URL:

```javascript
1..('bucket').('image.jpg', 60000, {2  : {3    : 200,4    : 200,5  },6})
```

The transformation options are embedded into the token attached to the URL — they cannot be changed once signed.


### Downloading images#
To download a transformed image, pass thetransformoption to thedownloadfunction.

```javascript
1..('bucket').('image.jpg', {2  : {3    : 800,4    : 300,5  },6})
```


### Automatic image optimization (WebP)#
When using the image transformation API, Storage will automatically find the best format supported by the client and return that to the client, without any code change. For instance, if you use Chrome when viewing a JPEG image and using transformation options, you'll see that images are automatically optimized aswebpimages.

As a result, this will lower the egress that you send to your users and your application will load much faster.

We currently only support WebP. AVIF support will come in the near future.

Disabling automatic optimization:

In case you'd like to return the original format of the image andopt-outfrom the automatic image optimization detection, you can pass theformat=originparameter when requesting a transformed image, this is also supported in the JavaScript SDK starting from v2.2.0

```javascript
1await ..('bucket').('image.jpeg', {2  : {3    : 200,4    : 200,5    : 'origin',6  },7})
```


### Next.js loader#
You can use Supabase Image Transformation to optimize your Next.js images using a customLoader.

To get started, create asupabase-image-loader.jsfile in your Next.js project which exports a default function:

```javascript
1const projectId = '' // your supabase project id23export default function supabaseLoader({ src, width, quality }) {4  return `https://${projectId}.supabase.co/storage/v1/render/image/public/${src}?width=${width}&quality=${quality || 75}`5}
```

In yournext.config.jsfile add the following configuration to instruct Next.js to use our custom loader

```javascript
1. = {2  : {3    : 'custom',4    : './supabase-image-loader.js',5  },6}
```

At this point you are ready to use theImagecomponent provided by Next.js

```javascript
1import Image from 'next/image'23const MyImage = (props) => {4  return <Image src="bucket/image.png" alt="Picture of the author" width={500} height={500} />5}
```


### Transformation options#
We currently support a few transformation options focusing on optimizing, resizing, and cropping images.


### Optimizing#
You can set the quality of the returned image by passing a value from 20 to 100 (with 100 being the highest quality) to thequalityparameter. This parameter defaults to 80.

Example:

```javascript
1..('bucket').('image.jpg', {2  : {3    : 50,4  },5})
```


### Resizing#
You can usewidthandheightparameters to resize an image to a specific dimension. If only one parameter is specified, the image will be resized and cropped, maintaining the aspect ratio.


### Modes#
You can use different resizing modes to fit your needs, each of them uses a different approach to resize the image:

Use theresizeparameter with one of the following values:

- cover: resizes the image while keeping the aspect ratio to fill a given size and crops projecting parts. (default)
- contain: resizes the image while keeping the aspect ratio to fit a given size.
- fill: resizes the image without keeping the aspect ratio.

cover: resizes the image while keeping the aspect ratio to fill a given size and crops projecting parts. (default)

contain: resizes the image while keeping the aspect ratio to fit a given size.

fill: resizes the image without keeping the aspect ratio.

Example:

```javascript
1..('bucket').('image.jpg', {2  : {3    : 800,4    : 300,5    : 'contain', // 'cover' | 'fill'6  },7})
```


### Limits#
- Width and height must be an integer value between 1-2500.
- The image size cannot exceed 25MB.
- The image resolution cannot exceed 50MP.


### Supported image formats#
$5per 1,000 origin images. You are only charged for usage exceeding your subscription
plan's quota.

The count resets at the start of each billing cycle.

For a detailed breakdown of how charges are calculated, refer toManage Storage Image Transformations usage.


### Self hosting#
Our solution to image resizing and optimization can be self-hosted as with any other Supabase product. Under the hood we useimgproxy


### imgproxy configuration:#
Deploy an imgproxy container with the following configuration:

```javascript
1imgproxy:2  image: darthsim/imgproxy3  environment:4    - IMGPROXY_ENABLE_WEBP_DETECTION=true5    - IMGPROXY_JPEG_PROGRESSIVE=true
```

Note: make sure that this service can only be reachable within an internal network and not exposed to the public internet


### Storage API configuration:#
Onceimgproxyis deployed we need to configure a couple of environment variables in your self-hostedstorage-apiservice as follows:

```javascript
1ENABLE_IMAGE_TRANSFORMATION=true2IMGPROXY_URL=yourinternalimgproxyurl.internal.com
```


================================================================================


# Limits
Source: https://supabase.com/docs/guides/storage/uploads/file-limits

Limits


### Learn how to increase Supabase file limits.
Learn how to increase Supabase file limits.


### Global file size#
You can set the maximum file size across all your buckets by setting theGlobal file size limitvalue in yourStorage Settings. For Free projects, the limit can't exceed 50 MB. On the Pro Plan and up, you can set this value to up to 500 GB. If you need more than 500 GB,contact us.

This option is a global limit, which applies to all your buckets.

Additionally, you can specify the maximum file size on a perbucket levelbut it can't be higher than this global limit. As a good practice, the global limit should be set to the highest possible file size that your application accepts, with smaller per-bucket limits set as needed.


### Per bucket restrictions#
You can have different restrictions on a per bucket level such as restricting the file types (e.g.pdf,images,videos) or the maximum file size, which should be lower than the global limit. To apply these limits on a bucket level seeCreating Buckets.


### File name restrictions#
File names can only include the following characters:

- Alphanumeric:A-Z,a-z,0-9
- Punctuation:_(underscore),-(hyphen),.(dot),'(apostrophe),,(comma)
- Special characters:!,*,&,$,@,=,;,:,+,?,(,)
- Whitespace


================================================================================


# Resumable Uploads
Source: https://supabase.com/docs/guides/storage/uploads/resumable-uploads

Resumable Uploads


### Learn how to upload files to Supabase Storage.
Learn how to upload files to Supabase Storage.

The resumable upload method is recommended when:

- Uploading large files that may exceed 6MB in size
- Network stability is a concern
- You want to have progress events for your uploads

Supabase Storage implements theTUS protocolto enable resumable uploads. TUS stands for The Upload Server and is an open protocol for supporting resumable uploads. The protocol allows the upload process to be resumed from where it left off in case of interruptions. This method can be implemented using thetus-js-clientlibrary, or other client-side libraries likeUppythat support the TUS protocol.

For optimal performance when uploading large files you should always use the direct storage hostname. This provides several performance enhancements that will greatly improve performance when uploading large files.

Instead ofhttps://project-id.supabase.cousehttps://project-id.storage.supabase.co

Here's an example of how to upload a file usingtus-js-client:

```javascript
1const tus = require('tus-js-client')23const projectId = ''45async function uploadFile(bucketName, fileName, file) {6    const { data: { session } } = await supabase.auth.getSession()78    return new Promise((resolve, reject) => {9        var upload = new tus.Upload(file, {10            // Supabase TUS endpoint (with direct storage hostname)11            endpoint: `https://${projectId}.storage.supabase.co/storage/v1/upload/resumable`,12            retryDelays: [0, 3000, 5000, 10000, 20000],13            headers: {14                authorization: `Bearer ${session.access_token}`,15                'x-upsert': 'true', // optionally set upsert to true to overwrite existing files16            },17            uploadDataDuringCreation: true,18            removeFingerprintOnSuccess: true, // Important if you want to allow re-uploading the same file https://github.com/tus/tus-js-client/blob/main/docs/api.md#removefingerprintonsuccess19            metadata: {20                bucketName: bucketName,21                objectName: fileName,22                contentType: 'image/png',23                cacheControl: 3600,24                metadata: JSON.stringify({ // custom metadata passed to the user_metadata column25                   yourCustomMetadata: true,26                }),27            },28            chunkSize: 6 * 1024 * 1024, // NOTE: it must be set to 6MB (for now) do not change it29            onError: function (error) {30                console.log('Failed because: ' + error)31                reject(error)32            },33            onProgress: function (bytesUploaded, bytesTotal) {34                var percentage = ((bytesUploaded / bytesTotal) * 100).toFixed(2)35                console.log(bytesUploaded, bytesTotal, percentage + '%')36            },37            onSuccess: function () {38                console.log('Download %s from %s', upload.file.name, upload.url)39                resolve()40            },41        })424344        // Check if there are any previous uploads to continue.45        return upload.findPreviousUploads().then(function (previousUploads) {46            // Found previous uploads so we select the first one.47            if (previousUploads.length) {48                upload.resumeFromPreviousUpload(previousUploads[0])49            }5051            // Start the upload52            upload.start()53        })54    })55}
```


### Upload URL#
When uploading using the resumable upload endpoint, the storage server creates a unique URL for each upload, even for multiple uploads to the same path. All chunks will be uploaded to this URL using thePATCHmethod.

This unique upload URL will be valid forup to 24 hours. If the upload is not completed within 24 hours, the URL will expire and you'll need to start the upload again. TUS client libraries typically create a new URL if the previous one expires.


### Concurrency#
When two or more clients upload to the same upload URL only one of them will succeed. The other clients will receive a409 Conflicterror. Only 1 client can upload to the same upload URL at a time which prevents data corruption.

When two or more clients upload a file to the same path using different upload URLs, the first client to complete the upload will succeed and the other clients will receive a409 Conflicterror.

If you provide thex-upsertheader the last client to complete the upload will succeed instead.


### Uppy example#
You can check afull example using Uppy.

Uppy has integrations with different frameworks:

- React
- Svelte
- Vue
- Angular


### Presigned uploads#
Resumable uploads also supports using signed upload tokens to created time-limited URLs that you can share to your users by invoking thecreateSignedUploadUrlmethod on the SDK and including the returned token in thex-signatureheader of the resumable upload.

```javascript
1// Create a signed upload URL2const { data } = await supabase.storage.from('bucket_name').createSignedUploadUrl('file_path', {3  upsert: true, // Optional: allow overwriting existing files4})56// Use the signed URL token in resumable upload headers7// Include data.token in the x-signature header
```

See thisfull example using Uppy with signed URLsfor more context.


### Overwriting files#
When uploading a file to a path that already exists, the default behavior is to return a400 Asset Already Existserror.
If you want to overwrite a file on a specific path you can set thex-upsertheader totrue.

We do advise against overwriting files when possible, as the CDN will take some time to propagate the changes to all the edge nodes leading to stale content.
Uploading a file to a new path is the recommended way to avoid propagation delays and stale content.

To learn more, see theCDNguide.


================================================================================


# S3 Uploads
Source: https://supabase.com/docs/guides/storage/uploads/s3-uploads

S3 Uploads


### Learn how to upload files to Supabase Storage using S3.
Learn how to upload files to Supabase Storage using S3.

You can use the S3 protocol to upload files to Supabase Storage. To get started with S3, see theS3 setup guide.

The S3 protocol supports file upload using:

- A single request
- Multiple requests via Multipart Upload


### Single request uploads#
ThePutObjectaction uploads the file in a single request. This matches the behavior of the Supabase SDKStandard Upload.

UsePutObjectto upload smaller files, where retrying the entire upload won't be an issue. The maximum file size on paid plans is 500 GB.

For example, using JavaScript and theaws-sdkclient:

```javascript
1import { S3Client, PutObjectCommand } from '@aws-sdk/client-s3'23const s3Client = new S3Client({...})45const file = fs.createReadStream('path/to/file')67const uploadCommand = new PutObjectCommand({8  Bucket: 'bucket-name',9  Key: 'path/to/file',10  Body: file,11  ContentType: 'image/jpeg',12})1314await s3Client.send(uploadCommand)
```


### Multipart uploads#
Multipart Uploads split the file into smaller parts and upload them in parallel, maximizing the upload speed on a fast network. When uploading large files, this allows you to retry the upload of individual parts in case of network issues.

This method is preferable overResumable Uploadfor server-side uploads, when you want to maximize upload speed at the cost of resumability. The maximum file size on paid plans is 500 GB.


### Upload a file in parts#
Use theUploadclass from an S3 client to upload a file in parts. For example, using JavaScript:

```javascript
1import { S3Client } from '@aws-sdk/client-s3'2import { Upload } from '@aws-sdk/lib-storage'34const s3Client = new S3Client({...})56const file = fs.createReadStream('path/to/very-large-file')78const upload = new Upload(s3Client, {9  Bucket: 'bucket-name',10  Key: 'path/to/file',11  ContentType: 'image/jpeg',12  Body: file,13})1415await uploader.done()
```


### Aborting multipart uploads#
All multipart uploads are automatically aborted after 24 hours. To abort a multipart upload before that, you can use theAbortMultipartUploadaction.


================================================================================


# Standard Uploads
Source: https://supabase.com/docs/guides/storage/uploads/standard-uploads

Standard Uploads


### Learn how to upload files to Supabase Storage.
Learn how to upload files to Supabase Storage.


### Uploading#
The standard file upload method is ideal for small files that are not larger than 6MB.

It uses the traditionalmultipart/form-dataformat and is simple to implement using the supabase-js SDK. Here's an example of how to upload a file using the standard upload method:

Though you can upload up to 5GB files using the standard upload method, we recommend usingTUS Resumable Uploadfor uploading files greater than 6MB in size for better reliability.

```javascript
1import {  } from '@supabase/supabase-js'23// Create Supabase client4const  = ('your_project_url', 'your_supabase_api_key')56// Upload file using standard upload7async function () {8  const { ,  } = await ..('bucket_name').('file_path', )9  if () {10    // Handle error11  } else {12    // Handle success13  }14}
```


### Overwriting files#
When uploading a file to a path that already exists, the default behavior is to return a400 Asset Already Existserror.
If you want to overwrite a file on a specific path you can set theupsertoptions totrueor using thex-upsertheader.

```javascript
1// Create Supabase client2const  = ('your_project_url', 'your_supabase_api_key')34await ..('bucket_name').('file_path', , {5  : true,6})
```

We do advise against overwriting files when possible, as our Content Delivery Network will take sometime to propagate the changes to all the edge nodes leading to stale content.
Uploading a file to a new path is the recommended way to avoid propagation delays and stale content.


### Content type#
By default, Storage will assume the content type of an asset from the file extension. If you want to specify the content type for your asset, pass thecontentTypeoption during upload.

```javascript
1// Create Supabase client2const  = ('your_project_url', 'your_supabase_api_key')34await ..('bucket_name').('file_path', , {5  : 'image/jpeg',6})
```


### Concurrency#
When two or more clients upload a file to the same path, the first client to complete the upload will succeed and the other clients will receive a400 Asset Already Existserror.
If you provide thex-upsertheader the last client to complete the upload will succeed instead.


================================================================================


# Creating Vector Buckets
Source: https://supabase.com/docs/guides/storage/vector/creating-vector-buckets

Creating Vector Buckets


### Set up vector buckets and indexes using the dashboard or JavaScript SDK.
Set up vector buckets and indexes using the dashboard or JavaScript SDK.

Expect rapid changes, limited features, and possible breaking updates.Share feedbackas we refine the experience and expand access.

Vector buckets organize your vector data into logical units. Within each bucket, you create indexes that define how vectors are stored and searched based on their dimensions and distance metrics.


### Creating a Vector bucket#
You can create vector buckets using either the Supabase Dashboard or the SDK.


### Using the Supabase Dashboard#
Your vector bucket is now ready. The next step is to create indexes within it.


### Using the SDK#
```javascript
1import {  } from '@supabase/supabase-js'23const  = ('https://your-project.supabase.co', 'your-service-key')45// Create a vector bucket6await ...('embeddings')78.('✓ Vector bucket created: embeddings')
```


### Creating indexes#
Indexes organize vectors within a bucket with consistent dimensions and distance metrics. For comprehensive index management documentation, seeWorking with Vector Indexes.


### Quick start: Creating an index via Dashboard#

### Quick start: Creating an index via SDK#
```javascript
1const bucket = supabase.storage.vectors.from('embeddings')23// Create an index4await bucket.createIndex({5  indexName: 'documents-openai',6  dataType: 'float32',7  dimension: 1536,8  distanceMetric: 'cosine',9})1011console.log('✓ Index created: documents-openai')
```


### Key details#
- Dimensionmust match your embedding model (e.g., 1536 for OpenAI)
- Distance metric(cosine,euclidean, orl2) is immutable after creation
- Maximum indexes per bucket: 10
- Maximum batch size: 500 vectors per operation

For detailed information on distance metrics, embedding dimensions, managing multiple indexes, and advanced index operations, seeWorking with Vector Indexes.


### Next steps#
After creating your bucket and indexes, you can:

- Store vectors
- Query vectors with similarity search
- Explore vector bucket limits


================================================================================


# Vector Buckets
Source: https://supabase.com/docs/guides/storage/vector/introduction

Vector Buckets


### Store, index, and query vector embeddings at scale with similarity search.
Store, index, and query vector embeddings at scale with similarity search.

Expect rapid changes, limited features, and possible breaking updates.Share feedbackas we refine the experience and expand access.

Vector buckets enable efficient storage and similarity search of vector embeddings. Built on S3-compatible storage, they provide high-performance semantic search capabilities for AI and machine learning applications.


### What are Vector buckets?#
Vector buckets are specialized storage containers optimized for vector data. Unlike traditional databases optimized for transactional queries, vector buckets use specialized indexing and distance metrics to perform fast similarity searches across millions of embeddings.

Each vector bucket contains:

- Indexes- Organized collections of vectors with consistent dimensions and distance metrics
- Vectors- Embeddings with associated metadata for filtering and enrichment
- Metadata- Additional context about vectors (text, tags, IDs, etc.)


### Key features#
- Similarity Search- Find semantically similar vectors using cosine, euclidean, or L2 distance metrics
- Metadata Filtering- Filter results by associated metadata before/after similarity search
- Batch Operations- Insert, update, and query up to 500 vectors per request
- Scalable Storage- Store millions of vectors in a single index
- S3 Native- Built on proven S3 infrastructure for reliability and durability


### Ideal use cases#
Vector buckets excel at:

- Semantic Search- Find documents or images similar to a query
- Recommendation Systems- Suggest products, content, or connections based on embeddings
- Clustering & Anomaly Detection- Group similar items or identify outliers
- Image Search- Retrieve visually similar images from large catalogs
- RAG (Retrieval-Augmented Generation)- Find relevant context for LLM queries
- Personalization- Recommend tailored content based on user embeddings


### Comparison to pgvector#
Vector buckets share similarities to pgvector and matches the developer experience of using pgvector as much as possible, but Vector buckets and anyForeign Data Wrappers (FDW)they use only support one similarity search algorithm, the<===>distance operator.

This makes Vector buckets ideal for:

- Large-scale data storage
- Backend processing workflows
- Applications where speed is less critical

And pgvector is ideal for:

- Fast prototyping and small data volumes
- Applications requiring quick response times
- User-facing features closer to the front end


### How Vector buckets work#
The system automatically handles indexing and optimization, making searches fast and reliable even with millions of vectors.


### Next steps#
Get started by learning how tocreate vector bucketsor dive intostoring vectors.


================================================================================


# Vector Bucket Limits
Source: https://supabase.com/docs/guides/storage/vector/limits

Vector Bucket Limits


### Understanding capacity, limits, and billing for vector buckets.
Understanding capacity, limits, and billing for vector buckets.

Expect rapid changes, limited features, and possible breaking updates.Share feedbackas we refine the experience and expand access.

Vector buckets have default limits during the alpha phase. These limits are designed to ensure fair resource allocation and can be adjusted on a case-by-case basis for production workloads.


### Storage limits#

================================================================================


# Querying Vectors
Source: https://supabase.com/docs/guides/storage/vector/querying-vectors

Querying Vectors


### Perform similarity search and retrieve vectors using JavaScript SDK or PostgreSQL.
Perform similarity search and retrieve vectors using JavaScript SDK or PostgreSQL.

Expect rapid changes, limited features, and possible breaking updates.Share feedbackas we refine the experience and expand access.

Vector similarity search finds vectors most similar to a query vector using distance metrics. You can query vectors using the JavaScript SDK or directly from Postgres using SQL.

Vector buckets and anyForeign Data Wrappers (FDW)they use only support one similarity search algorithm, the<===>distance operator.


### Basic similarity search#
```javascript
1import { createClient } from '@supabase/supabase-js'23const supabase = createClient('https://your-project.supabase.co', 'your-service-key')45const index = supabase.storage.vectors.from('embeddings').index('documents-openai')67// Query with a vector embedding8const { data, error } = await index.queryVectors({9  queryVector: {10    float32: [0.1, 0.2, 0.3 /* ... embedding of 1536 dimensions ... */],11  },12  topK: 5,13  returnDistance: true,14  returnMetadata: true,15})1617if (error) {18  console.error('Query failed:', error)19} else {20  // Results are ranked by similarity (lowest distance = most similar)21  data.vectors.forEach((result, rank) => {22    console.log(`${rank + 1}. ${result.metadata?.title}`)23    console.log(`   Similarity score: ${result.distance.toFixed(4)}`)24  })25}
```


### Semantic search#
Find documents similar to a query by embedding the query text:

```javascript
1import { createClient } from '@supabase/supabase-js'2import OpenAI from 'openai'34const supabase = createClient(...)5const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })67async function semanticSearch(query, topK = 5) {8  // Embed the query9  const queryEmbedding = await openai.embeddings.create({10    model: 'text-embedding-3-small',11    input: query12  })1314  const queryVector = queryEmbedding.data[0].embedding1516  // Search for similar vectors17  const { data, error } = await supabase.storage.vectors18    .from('embeddings')19    .index('documents-openai')20    .queryVectors({21      queryVector: { float32: queryVector },22      topK,23      returnDistance: true,24      returnMetadata: true25    })2627  if (error) {28    throw error29  }3031  return data.vectors.map((result) => ({32    id: result.key,33    title: result.metadata?.title,34    similarity: 1 - result.distance, // Convert distance to similarity (0-1)35    metadata: result.metadata36  }))37}3839// Usage40const results = await semanticSearch('How do I use vector search?')41results.forEach((result) => {42  console.log(`${result.title} (${(result.similarity * 100).toFixed(1)}% similar)`)43})
```


### Filtered similarity search#
```javascript
1const index = supabase.storage.vectors2  .from('embeddings')3  .index('documents-openai')45// Search with metadata filter6const { data } = await index.queryVectors({7  queryVector: { float32: [...embedding...] },8  topK: 10,9  filter: {10    // Filter by metadata fields11    category: 'electronics',12    in_stock: true,13    price: { $lte: 500 } // Less than or equal to 50014  },15  returnDistance: true,16  returnMetadata: true17})
```


### Retrieving specific vectors#
```javascript
1const index = supabase.storage.vectors.from('embeddings').index('documents-openai')23const { data, error } = await index.getVectors({4  keys: ['doc-1', 'doc-2', 'doc-3'],5  returnData: true,6  returnMetadata: true,7})89if (!error) {10  data.vectors.forEach((vector) => {11    console.log(`${vector.key}: ${vector.metadata?.title}`)12  })13}
```


### Listing vectors#
```javascript
1const index = supabase.storage.vectors.from('embeddings').index('documents-openai')23let nextToken = undefined4let pageCount = 056do {7  const { data, error } = await index.listVectors({8    maxResults: 100,9    nextToken,10    returnData: false, // Don't return embeddings for faster response11    returnMetadata: true,12  })1314  if (error) break1516  pageCount++17  console.log(`Page ${pageCount}: ${data.vectors.length} vectors`)1819  data.vectors.forEach((vector) => {20    console.log(`  - ${vector.key}: ${vector.metadata?.title}`)21  })2223  nextToken = data.nextToken24} while (nextToken)
```


### Hybrid search: Vectors + relational data#
Combine similarity search with SQL filtering and joins:

```javascript
1async function hybridSearch(queryVector, filters) {2  const index = supabase.storage.vectors.from('embeddings').index('documents-openai')34  // Get similar vectors with filters5  const { data: vectorResults } = await index.queryVectors({6    queryVector: { float32: queryVector },7    topK: 100,8    filter: filters,9    returnDistance: true,10    returnMetadata: true,11  })1213  // Get additional details from relational database14  const { data: details } = await supabase15    .from('documents')16    .select('*')17    .in(18      'id',19      vectorResults.vectors.map((v) => v.metadata?.doc_id)20    )2122  // Merge results23  return vectorResults.vectors.map((vector) => {24    const detail = details?.find((d) => d.id === vector.metadata?.doc_id)25    return {26      ...vector,27      ...detail,28    }29  })30}
```


### Real-world examples#

### RAG (retrieval-augmented generation)#
```javascript
1import OpenAI from 'openai'2import { createClient } from '@supabase/supabase-js'34async function retrieveContextForLLM(userQuery) {5  const supabase = createClient(...)6  const openai = new OpenAI()78  // 1. Embed the user query9  const queryEmbedding = await openai.embeddings.create({10    model: 'text-embedding-3-small',11    input: userQuery12  })1314  // 2. Retrieve relevant documents15  const { data: vectorResults } = await supabase.storage.vectors16    .from('embeddings')17    .index('documents-openai')18    .queryVectors({19      queryVector: { float32: queryEmbedding.data[0].embedding },20      topK: 5,21      returnMetadata: true22    })2324  // 3. Use vectors to augment LLM prompt25  const context = vectorResults.vectors26    .map(v => v.metadata?.content || '')27    .join('\n\n')2829  const response = await openai.chat.completions.create({30    model: 'gpt-4',31    messages: [32      {33        role: 'system',34        content: `Use the following context to answer the user's question:\n\n${context}`35      },36      {37        role: 'user',38        content: userQuery39      }40    ]41  })4243  return response.choices[0].message.content44}
```


### Product recommendations#
```javascript
1async function recommendProducts(userEmbedding, topK = 5) {2  const supabase = createClient(...)34  // Find similar products5  const { data } = await supabase.storage.vectors6    .from('embeddings')7    .index('products-openai')8    .queryVectors({9      queryVector: { float32: userEmbedding },10      topK,11      filter: {12        in_stock: true13      },14      returnMetadata: true15    })1617  return data.vectors.map((result) => ({18    id: result.metadata?.product_id,19    name: result.metadata?.name,20    price: result.metadata?.price,21    similarity: 1 - result.distance22  }))23}
```


### Filtering before similarity search#
```javascript
1// Use metadata filters to reduce search scope2const { data } = await index.queryVectors({3  queryVector,4  topK: 100,5  filter: {6    category: 'electronics', // Pre-filter by category7  },8})
```


### Next steps#
- Store vectors
- Work with vector indexes
- Explore vector bucket limits


================================================================================


# Storing Vectors
Source: https://supabase.com/docs/guides/storage/vector/storing-vectors

Storing Vectors


### Insert and update vector embeddings with metadata using the JavaScript SDK or Postgres.
Insert and update vector embeddings with metadata using the JavaScript SDK or Postgres.

Expect rapid changes, limited features, and possible breaking updates.Share feedbackas we refine the experience and expand access.

Once you've created a bucket and index, you can start storing vectors. Vectors can include optional metadata for filtering and enrichment during queries.


### Basic vector insertion#
```javascript
1import {  } from '@supabase/supabase-js'23const  = ('https://your-project.supabase.co', 'your-service-key')45// Get bucket and index6const  = ...('embeddings')7const  = .('documents-openai')89// Insert vectors10const {  } = await .({11  : [12    {13      : 'doc-1',14      : {15        : [0.1, 0.2, 0.3 /* ... rest of embedding ... */],16      },17      : {18        : 'Getting Started with Vector Buckets',19        : 'documentation',20      },21    },22    {23      : 'doc-2',24      : {25        : [0.4, 0.5, 0.6 /* ... rest of embedding ... */],26      },27      : {28        : 'Advanced Vector Search',29        : 'blog',30      },31    },32  ],33})3435if () {36  .('Error storing vectors:', )37} else {38  .('✓ Vectors stored successfully')39}
```


### Storing vectors from Embeddings API#
Generate embeddings using an LLM API and store them directly:

```javascript
1import {  } from '@supabase/supabase-js'2import  from 'openai'34const  = ('https://your-project.supabase.co', 'your-service-key')56const  = new ({7  : ..,8})910// Documents to embed and store11const  = [12  { : '1', : 'How to Train Your AI', : 'Guide for training models...' },13  { : '2', : 'Vector Search Best Practices', : 'Tips for semantic search...' },14  {15    : '3',16    : 'Building RAG Systems',17    : 'Implementing retrieval-augmented generation...',18  },19]2021// Generate embeddings22const  = await ..({23  : 'text-embedding-3-small',24  : .(() => .),25})2627// Prepare vectors for storage28const  = .((, ) => ({29  : .,30  : {31    : .[].,32  },33  : {34    : .,35    : 'knowledge_base',36    : new ().(),37  },38}))3940// Store vectors in batches (max 500 per request)41const  = ...('embeddings')42const  = .('documents-openai')4344for (let  = 0;  < .;  += 500) {45  const  = .(,  + 500)46  const {  } = await .({ :  })4748  if () {49    .(`Error storing batch ${ / 500 + 1}:`, )50  } else {51    .(`✓ Stored batch ${ / 500 + 1} (${.} vectors)`)52  }53}
```


### Updating vectors#
```javascript
1const index = bucket.index('documents-openai')23// Update a vector (same key)4const { error } = await index.putVectors({5  vectors: [6    {7      key: 'doc-1',8      data: {9        float32: [0.15, 0.25, 0.35 /* ... updated embedding ... */],10      },11      metadata: {12        title: 'Getting Started with Vector Buckets - Updated',13        updated_at: new Date().toISOString(),14      },15    },16  ],17})1819if (!error) {20  console.log('✓ Vector updated successfully')21}
```


### Deleting vectors#
```javascript
1const index = bucket.index('documents-openai')23// Delete specific vectors4const { error } = await index.deleteVectors({5  keys: ['doc-1', 'doc-2'],6})78if (!error) {9  console.log('✓ Vectors deleted successfully')10}
```


### Metadata best practices#
Metadata makes vectors more useful by enabling filtering and context:

```javascript
1const vectors = [2  {3    key: 'product-001',4    data: { float32: [...] },5    metadata: {6      product_id: 'prod-001',7      category: 'electronics',8      price: 299.99,9      in_stock: true,10      tags: ['laptop', 'portable'],11      description: 'High-performance ultrabook'12    }13  },14  {15    key: 'product-002',16    data: { float32: [...] },17    metadata: {18      product_id: 'prod-002',19      category: 'electronics',20      price: 99.99,21      in_stock: true,22      tags: ['headphones', 'wireless'],23      description: 'Noise-cancelling wireless headphones'24    }25  }26]2728const { error } = await index.putVectors({ vectors })
```


### Metadata field guidelines#
- Keep it lightweight- Metadata is returned with query results, so large values increase response size
- Use consistent types- Store the same field with consistent data types across vectors
- Index key fields- Mark fields you'll filter by to improve query performance
- Avoid nested objects- While supported, flat structures are easier to filter


### Batch processing large datasets#
For storing large numbers of vectors efficiently:

```javascript
1import { createClient } from '@supabase/supabase-js'2import fs from 'fs'34const supabase = createClient(...)5const index = supabase.storage.vectors6  .from('embeddings')7  .index('documents-openai')89// Read embeddings from file10const embeddingsFile = fs.readFileSync('embeddings.jsonl', 'utf-8')11const lines = embeddingsFile.split('\n').filter(line => line.trim())1213const vectors = lines.map((line, idx) => {14  const { key, embedding, metadata } = JSON.parse(line)15  return {16    key,17    data: { float32: embedding },18    metadata19  }20})2122// Process in batches23const BATCH_SIZE = 50024let processed = 02526for (let i = 0; i < vectors.length; i += BATCH_SIZE) {27  const batch = vectors.slice(i, i + BATCH_SIZE)2829  try {30    const { error } = await index.putVectors({ vectors: batch })3132    if (error) throw error3334    processed += batch.length35    console.log(`Progress: ${processed}/${vectors.length}`)36  } catch (error) {37    console.error(`Batch failed at offset ${i}:`, error)38    // Optionally implement retry logic39  }40}4142console.log('✓ All vectors stored successfully')
```


### Performance optimization#

### Batch operations#
Always use batch operations for better performance:

```javascript
1// ❌ Inefficient - Multiple requests2for (const vector of vectors) {3  await index.putVectors({ vectors: [vector] })4}56// ✅ Efficient - Single batch operation7await index.putVectors({ vectors })
```


### Metadata considerations#
Keep metadata concise:

```javascript
1// ❌ Large metadata2metadata: {3  full_document_text: 'Very long document content...',4  detailed_analysis: { /* large object */ }5}67// ✅ Lean metadata8metadata: {9  doc_id: 'doc-123',10  category: 'news',11  summary: 'Brief summary'12}
```


### Next steps#
- Query vectors with similarity search
- Work with vector indexes
- Explore vector bucket limits


================================================================================


# Working with Vector Indexes
Source: https://supabase.com/docs/guides/storage/vector/working-with-indexes

Working with Vector Indexes


### Create, manage, and optimize vector indexes for efficient similarity search.
Create, manage, and optimize vector indexes for efficient similarity search.

Expect rapid changes, limited features, and possible breaking updates.Share feedbackas we refine the experience and expand access.

Vector indexes organize embeddings within a bucket with consistent dimensions and distance metrics. Each index defines how similarity searches are performed across your vectors.


### Understanding vector indexes#
An index specifies:

- Index Name- Unique identifier within the bucket
- Dimension- Size of vector embeddings (e.g., 1536 for OpenAI)
- Distance Metric- Similarity calculation method (cosine, euclidean, or L2)
- Data Type- Vector format (currentlyfloat32)

Think of an index as a table in a traditional database. It has a schema (dimension) and a query strategy (distance metric).


### Creating indexes#

### Via Dashboard#

### Via SDK#
```javascript
1import {  } from '@supabase/supabase-js'23const  = ('https://your-project.supabase.co', 'your-service-key')45const  = ...('embeddings')67// Create an index8const { ,  } = await .({9  : 'documents-openai',10  : 'float32',11  : 1536,12  : 'cosine',13})1415if () {16  .('Error creating index:', )17} else {18  .('Index created:', )19}
```


### Choosing the right metric#
Most modern embedding models work best withcosinedistance:

- OpenAI(text-embedding-3-small, text-embedding-3-large): Cosine
- Cohere(embed-english-v3.0): Cosine
- Hugging Face(sentence-transformers): Cosine
- Google(text-embedding-004): Cosine
- Llama 2embeddings: Cosine or L2

Tip: Check your embedding model's documentation for the recommended distance metric.

Important: Creating an index with incorrect dimensions will cause insert and query operations to fail.


### Managing multiple indexes#
Create multiple indexes for different use cases or embedding models:

```javascript
1const bucket = supabase.storage.vectors.from('embeddings')23// Index for OpenAI embeddings4await bucket.createIndex({5  indexName: 'documents-openai',6  dimension: 1536,7  distanceMetric: 'cosine',8  dataType: 'float32',9})1011// Index for Cohere embeddings12await bucket.createIndex({13  indexName: 'documents-cohere',14  dimension: 1024,15  distanceMetric: 'cosine',16  dataType: 'float32',17})1819// Index for different use case20await bucket.createIndex({21  indexName: 'images-openai',22  dimension: 1536,23  distanceMetric: 'cosine',24  dataType: 'float32',25})2627// List all indexes28const { data: indexes } = await bucket.listIndexes()29console.log('All indexes:', indexes)
```


### Use cases for multiple indexes#
- Different embedding models- Store vectors from OpenAI, Cohere, and local models separately
- Different domains- Maintain separate indexes for documents, images, products, etc.
- A/B testing- Compare different embedding models side-by-side
- Multi-language- Keep language-specific embeddings separate


### Listing and inspecting indexes#

### List all indexes in a bucket#
```javascript
1const bucket = supabase.storage.vectors.from('embeddings')23const { data: indexes, error } = await bucket.listIndexes()45if (!error) {6  indexes?.forEach((index) => {7    console.log(`Index: ${index.name}`)8    console.log(`  Dimension: ${index.dimension}`)9    console.log(`  Distance: ${index.distanceMetric}`)10  })11}
```


### Get index details#
```javascript
1const { data: indexDetails, error } = await bucket.getIndex('documents-openai')23if (!error && indexDetails) {4  console.log(`Index: ${indexDetails.name}`)5  console.log(`Created at: ${indexDetails.createdAt}`)6  console.log(`Dimension: ${indexDetails.dimension}`)7  console.log(`Distance metric: ${indexDetails.distanceMetric}`)8}
```


### Deleting indexes#
Delete an index to free storage space:

```javascript
1const bucket = supabase.storage.vectors.from('embeddings')23const { error } = await bucket.deleteIndex('documents-openai')45if (error) {6  console.error('Error deleting index:', error)7} else {8  console.log('Index deleted successfully')9}
```


### Before deleting an index#
Warning: Deleting an index is permanent and cannot be undone.

- Backup important data- Export vectors before deletion if needed
- Update applications- Ensure no code references the deleted index
- Check dependencies- Verify no active queries use the index
- Plan the deletion- Do this during low-traffic periods


### Immutable properties#
Once created, these propertiescannot be changed:

- Dimension- Must create new index with different dimension
- Distance metric- Cannot change after creation
- Data type- Currently onlyfloat32supported


### Optimizing index performance#
```javascript
1// Good - Appropriate batch size2const batch = vectors.slice(0, 250)3await index.putVectors({ vectors: batch })45// Good - Filter metadata before query6const { data } = await index.queryVectors({7  queryVector,8  topK: 5,9  filter: { category: 'electronics' },10})1112// Avoid - Single vector inserts13for (const vector of vectors) {14  await index.putVectors({ vectors: [vector] })15}1617// Avoid - Returning unnecessary data18const { data } = await index.queryVectors({19  queryVector,20  topK: 1000, // Too many results21  returnData: true, // Include large embeddings22})
```


### Next steps#
- Store vectors in indexes
- Query vectors for similarity search
- Understand vector bucket limits
- Create vector buckets


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/admin-api ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/analytics-buckets ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-createuser ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-deleteuser ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-generatelink ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-getuserbyid ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-inviteuserbyemail ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-listusers ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-mfa-deletefactor ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-mfa-listfactors-admin ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-oauth-admin ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-oauth-approveauthorization ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-oauth-createclient ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-oauth-deleteclient ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-oauth-denyauthorization ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-oauth-getauthorizationdetails ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-oauth-getclient ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-oauth-listclients ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-oauth-listgrants ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-oauth-regenerateclientsecret ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-oauth-revokegrant ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-oauth-server ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-oauth-updateclient ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-signout ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-admin-updateuserbyid ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-api ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-exchangecodeforsession ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-getclaims ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-getsession ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-getuser ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-getuseridentities ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-initialize ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-linkidentity ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-mfa-api ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-mfa-challenge ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-mfa-challengeandverify ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-mfa-enroll ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-mfa-getauthenticatorassurancelevel ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-mfa-listfactors ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-mfa-unenroll ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-mfa-verify ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-onauthstatechange ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-reauthentication ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-refreshsession ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-resend ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-resetpasswordforemail ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-setsession ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-signinanonymously ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-signinwithidtoken ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-signinwithoauth ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-signinwithotp ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-signinwithpassword ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-signinwithsso ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-signinwithweb3 ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-signout ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-signup ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-startautorefresh ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-stopautorefresh ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-unlinkidentity ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-updateuser ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/auth-verifyotp ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/broadcastmessage ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/containedby ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/contains ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/db-abortsignal ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/db-csv ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/db-modifiers-select ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/db-overrideTypes ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/db-returns ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/delete ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/eq ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/explain ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/file-buckets ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/filter ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/functions-cors-headers ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/functions-invoke ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/functions-setauth ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/getchannels ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/gt ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/gte ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/ilike ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/in ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/initializing ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/insert ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/installing ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/introduction ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/is ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/like ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/limit ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/lt ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/lte ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/match ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/maybesingle ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/neq ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/not ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/or ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/order ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/overlaps ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/range ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/rangeadjacent ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/rangegt ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/rangegte ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/rangelt ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/rangelte ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/realtime-setauth ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/removeallchannels ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/removechannel ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/rpc ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/select ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/single ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-createbucket ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-deletebucket ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-emptybucket ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-copy ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-createsigneduploadurl ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-createsignedurl ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-createsignedurls ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-download ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-exists ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-getpublicurl ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-info ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-list ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-move ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-remove ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-tobase64 ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-update ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-upload ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-from-uploadtosignedurl ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-getbucket ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-listbuckets ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storage-updatebucket ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storageanalytics-createbucket ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storageanalytics-deletebucket ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storageanalytics-from ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storageanalytics-listbuckets ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storageclient-from ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storagevectors-createbucket ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storagevectors-deletebucket ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storagevectors-from ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storagevectors-getbucket ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/storagevectors-listbuckets ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/subscribe ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/textsearch ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/typescript-support ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/update ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/upsert ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/using-filters ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/using-modifiers ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/vector-buckets ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/vectorbucket-createindex ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/vectorbucket-deleteindex ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/vectorbucket-getindex ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/vectorbucket-index ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/vectorbucket-listindexes ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/vectorindex-deletevectors ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/vectorindex-getvectors ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/vectorindex-listvectors ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/vectorindex-putvectors ---


================================================================================


--- COULD NOT PARSE: https://supabase.com/docs/reference/javascript/vectorindex-queryvectors ---


================================================================================
